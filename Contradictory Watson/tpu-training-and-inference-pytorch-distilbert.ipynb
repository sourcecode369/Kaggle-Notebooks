{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Check TPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.0.0.2:8470']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
    "   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "   tpu = None\n",
    "if tpu:\n",
    "   tf.config.experimental_connect_to_cluster(tpu)\n",
    "   tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "   strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "   strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-googletrans.git\r\n",
      "  Cloning https://github.com/ssut/py-googletrans.git to /tmp/pip-req-build-m7cm_wyb\r\n",
      "  Running command git clone -q https://github.com/ssut/py-googletrans.git /tmp/pip-req-build-m7cm_wyb\r\n",
      "Collecting httpx==0.13.3\r\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 55 kB 1.6 MB/s \r\n",
      "\u001b[?25hCollecting rfc3986<2,>=1.3\r\n",
      "  Downloading rfc3986-1.4.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Collecting httpcore==0.9.*\r\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 42 kB 883 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: idna==2.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.0.0) (2.9)\r\n",
      "Collecting sniffio\r\n",
      "  Downloading sniffio-1.1.0-py3-none-any.whl (4.5 kB)\r\n",
      "Collecting hstspreload\r\n",
      "  Downloading hstspreload-2020.7.29-py3-none-any.whl (926 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 926 kB 14.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.0.0) (2020.6.20)\r\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.0.0) (3.0.4)\r\n",
      "Collecting h2==3.*\r\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.6 MB/s \r\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\r\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \r\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0\r\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting hpack<4,>=3.0\r\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\r\n",
      "Building wheels for collected packages: googletrans\r\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=16448 sha256=73541f2cc40e603cfb4425e0ff51395f03d0311cf3d84be414313437b45055b1\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-snpk8z8d/wheels/eb/82/a2/f07ad36dbee6290000d9057df7e3c81a973db81913efd3b252\r\n",
      "Successfully built googletrans\r\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h2, sniffio, h11, httpcore, hstspreload, httpx, googletrans\r\n",
      "Successfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.7.29 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.1.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  5115  100  5115    0     0  27648      0 --:--:-- --:--:-- --:--:-- 27648\r\n",
      "Updating... This may take around 2 minutes.\r\n",
      "Updating TPU runtime to pytorch-nightly ...\r\n",
      "Found existing installation: torch 1.5.0\r\n",
      "Uninstalling torch-1.5.0:\r\n",
      "  Successfully uninstalled torch-1.5.0\r\n",
      "Found existing installation: torchvision 0.6.0a0+35d732a\r\n",
      "Uninstalling torchvision-0.6.0a0+35d732a:\r\n",
      "Done updating TPU runtime\r\n",
      "  Successfully uninstalled torchvision-0.6.0a0+35d732a\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/108.8 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/124.6 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/2.4 MiB.                                      \r\n",
      "Processing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (1.18.5)\r\n",
      "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0+2f840b1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0+2f840b1 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: torch\r\n",
      "Successfully installed torch-1.7.0a0+2f840b1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.6+c4f8873\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (7.2.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.18.5)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.7.0a0+2f840b1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly) (0.18.2)\r\n",
      "Installing collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.8.0a0+40333c5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  libgfortran4 libopenblas-base\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libgfortran4 libomp5 libopenblas-base libopenblas-dev\r\n",
      "0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\r\n",
      "Need to get 8550 kB of archives.\r\n",
      "After this operation, 97.6 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\r\n",
      "Fetched 8550 kB in 0s (40.6 MB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libgfortran4:amd64.\r\n",
      "(Reading database ... 107745 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\r\n",
      "Unpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\r\n",
      "Selecting previously unselected package libopenblas-base:amd64.\r\n",
      "Preparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\r\n",
      "Unpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\r\n",
      "Selecting previously unselected package libopenblas-dev:amd64.\r\n",
      "Preparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\r\n",
      "Unpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (5.0.1-1) ...\r\n",
      "Setting up libomp5:amd64 (5.0.1-1) ...\r\n",
      "Setting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\r\n",
      "Setting up libopenblas-base:amd64 (0.2.20+ds-4) ...\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\r\n",
      "Setting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-googletrans.git\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH: 2f840b1662b487d5551d7230f8eb4d57645cfff5\n",
      "XLA: c4f8873d791e36e9819c102bac0e309d88b6ca8b\n",
      "CPU times: user 1.54 s, sys: 213 ms, total: 1.75 s\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%autosave 60\n",
    "\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\"\n",
    "os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from googletrans import Translator\n",
    "from dask import bag, diagnostics\n",
    "\n",
    "import transformers\n",
    "from transformers import (AdamW, \n",
    "                          DistilBertTokenizer, \n",
    "                          DistilBertModel, \n",
    "                          DistilBertTokenizerFast,                          \n",
    "                          get_cosine_schedule_with_warmup)\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.serialization as xser\n",
    "import torch_xla.version as xv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('PYTORCH:', xv.__torch_gitrev__)\n",
    "print('XLA:', xv.__xla_gitrev__)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\n",
    "test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\n",
    "sample_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Text Augmentation\n",
    "\n",
    "##### References - [JohnM's - Agmenting Data with Translations Kernel](https://www.kaggle.com/jpmiller/augmenting-data-with-translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 51.2s\n",
      "[########################################] | 100% Completed |  2min 40.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17370, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(words):\n",
    "    translator = Translator()\n",
    "    decoded = translator.translate(words, dest='en').text\n",
    "    return decoded\n",
    "\n",
    "other_langs = train.loc[train.lang_abv != \"en\"].copy()\n",
    "\n",
    "#TODO: use a dask dataframe instead of bags\n",
    "premise_bag = bag.from_sequence(other_langs.premise.tolist()).map(translate)\n",
    "hypo_bag =  bag.from_sequence(other_langs.hypothesis.tolist()).map(translate)\n",
    "with diagnostics.ProgressBar():\n",
    "    premises = premise_bag.compute()\n",
    "    hypos = hypo_bag.compute()\n",
    "    \n",
    "    \n",
    "other_langs[['premise', 'hypothesis']] = list(zip(premises, hypos))\n",
    "train = train.append(other_langs)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Dataset Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, df, ids, mask):\n",
    "        self.df = df\n",
    "        self.ids = ids\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):   \n",
    "        ids = self.ids[index]\n",
    "        mask = self.mask[index]\n",
    "        targets = self.df.iloc[index].label\n",
    "        return {\n",
    "            'ids':torch.tensor(ids),\n",
    "            'mask':torch.tensor(mask),\n",
    "            'targets':targets\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBERT(nn.Module):\n",
    "    def __init__(self, num_labels, multisample):\n",
    "        super(DistillBERT, self).__init__()\n",
    "        output_hidden_states = True\n",
    "        self.num_labels = num_labels\n",
    "        self.multisample= multisample\n",
    "        self.distillbert = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\", \n",
    "                                                           output_hidden_states=output_hidden_states,\n",
    "                                                           num_labels=1)\n",
    "        self.layer_norm = nn.LayerNorm(768*2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)        \n",
    "        self.classifier = nn.Linear(768*2, self.num_labels)\n",
    "    \n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None):\n",
    "        outputs = self.distillbert(input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   head_mask=head_mask,\n",
    "                                   inputs_embeds=inputs_embeds)\n",
    "        average_pool = torch.mean(outputs[0], 1)\n",
    "        max_pool, _ = torch.max(outputs[0], 1)\n",
    "        concatenate_layer = torch.cat((average_pool, max_pool), 1)\n",
    "        normalization = self.layer_norm(concatenate_layer)\n",
    "        if self.multisample:\n",
    "            # Multisample Dropout\n",
    "            logits = torch.mean(\n",
    "                torch.stack(\n",
    "                    [self.classifier(self.dropout(normalization)) for _ in range(5)],\n",
    "                    dim=0,\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            logits = self.dropout(normalization)\n",
    "            logits = self.classifier(logits)       \n",
    "        outputs = F.log_softmax(logits, dim=1)\n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Metrics Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Optimizer Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(model):\n",
    "    # Differential Learning Rate\n",
    "    def is_backbone(name):\n",
    "        return \"distillbert\" in name\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "       {'params': [param for name, param in model.named_parameters() if is_backbone(name)], 'lr': LR},\n",
    "       {'params': [param for name, param in model.named_parameters() if not is_backbone(name)], 'lr': 1e-3} \n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n",
    "    )\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Loss Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return nn.NLLLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n",
    "    # Train\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1],\n",
    "        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n",
    "    )\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data_time.update(time.time()-end)\n",
    "        ids = data[\"ids\"]\n",
    "        mask = data[\"mask\"]\n",
    "        targets = data[\"targets\"]\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids = ids,\n",
    "            attention_mask = mask\n",
    "        )\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        acc1= accuracy(outputs, targets, topk=(1,))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        top1.update(acc1[0].item(), ids.size(0))\n",
    "        scheduler.step()\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % 30 == 0:\n",
    "            progress.display(i)\n",
    "    del loss\n",
    "    del outputs\n",
    "    del ids\n",
    "    del mask\n",
    "    del targets\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop_fn(validation_loader, model, device):\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    learning_rate = AverageMeter('LR',':2.8f')\n",
    "    progress = ProgressMeter(\n",
    "        len(validation_loader),\n",
    "        [batch_time, losses, top1],\n",
    "        prefix='[xla:{}]Validation: '.format(xm.get_ordinal()))\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            ids = data[\"ids\"]\n",
    "            mask = data[\"mask\"]\n",
    "            targets = data[\"targets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            outputs = model(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask\n",
    "            )\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            acc1= accuracy(outputs, targets, topk=(1,))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            top1.update(acc1[0].item(), ids.size(0))\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "    del loss\n",
    "    del outputs\n",
    "    del ids\n",
    "    del mask\n",
    "    del targets\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "##### References - [Xhlulu's - Jigsaw TPU: DistilBERT with Huggingface and Keras Kernel](https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(df, fast_tokenizer):\n",
    "    fast_tokenizer.enable_truncation(max_length=MAX_LEN)\n",
    "    fast_tokenizer.enable_padding(max_length=MAX_LEN)\n",
    "    \n",
    "    text = list(zip(df.premise, df.hypothesis))\n",
    "    encoded = fast_tokenizer.encode_batch(\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    all_ids = []\n",
    "    all_masks = []\n",
    "    all_ids.extend([enc.ids for enc in encoded])\n",
    "    all_masks.extend([enc.attention_mask for enc in encoded])\n",
    "    \n",
    "    return np.array(all_ids), np.array(all_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714fae57be2a496a9dc15b6f9209f2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f314f19d7e4a77bbda4abc8edb6969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=541808922.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8947199fc141679f2c920ace26486d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 40\n",
    "MAX_LEN = 80\n",
    "# Scale learning rate to 8 TPU's\n",
    "LR = 2e-5 * xm.xrt_world_size() \n",
    "METRICS_DEBUG = True\n",
    "\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(DistillBERT(num_labels=3, multisample=False))\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "\n",
    "# Train Validation Split\n",
    "mask = np.random.rand(len(train)) < 0.95\n",
    "train_df = train[mask]\n",
    "valid_df = train[~mask]\n",
    "\n",
    "train_ids, train_mask = fast_encode(train_df, fast_tokenizer)\n",
    "valid_ids, valid_mask = fast_encode(valid_df, fast_tokenizer)\n",
    "\n",
    "train_dataset = DatasetRetriever(df=train_df, ids=train_ids, mask=train_mask)\n",
    "valid_dataset = DatasetRetriever(df=valid_df, ids=valid_ids, mask=valid_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run():\n",
    "    xm.master_print('Starting Run ...')\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    xm.master_print('Train Loader Created.')\n",
    "    \n",
    "    valid_sampler = DistributedSampler(\n",
    "        valid_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        sampler=valid_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    xm.master_print('Valid Loader Created.')\n",
    "    \n",
    "    num_train_steps = int(len(train_df) / TRAIN_BATCH_SIZE / xm.xrt_world_size())\n",
    "    device = xm.xla_device()\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    xm.master_print('Done Model Loading.')\n",
    "    optimizer = get_model_optimizer(model)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = num_train_steps * EPOCHS\n",
    "    )\n",
    "    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
    "        xm.master_print('Parallel Loader Created. Training ...')\n",
    "        train_loop_fn(para_loader.per_device_loader(device),\n",
    "                      model,  \n",
    "                      optimizer, \n",
    "                      device, \n",
    "                      scheduler, \n",
    "                      epoch\n",
    "                     )\n",
    "        \n",
    "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "            \n",
    "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
    "        xm.master_print('Parallel Loader Created. Validating ...')\n",
    "        eval_loop_fn(para_loader.per_device_loader(device), \n",
    "                     model,  \n",
    "                     device\n",
    "                    )\n",
    "        \n",
    "        # Serialized and Memory Reduced Model Saving\n",
    "        if epoch == EPOCHS-1:\n",
    "            xm.master_print('Saving Model ..')\n",
    "            xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n",
    "            xm.master_print('Model Saved.')\n",
    "            \n",
    "    if METRICS_DEBUG:\n",
    "      xm.master_print(met.metrics_report(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run ...\n",
      "Train Loader Created.\n",
      "Valid Loader Created.\n",
      "Done Model Loading.\n",
      "Num Train Steps= 129, XRT World Size= 8.\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [0][  0/129]\tTime  9.543 ( 9.543)\tData  0.151 ( 0.151)\tLoss 1.1094e+00 (1.1094e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:5]Train:  Epoch: [0][  0/129]\tTime  7.946 ( 7.946)\tData  0.065 ( 0.065)\tLoss 1.3750e+00 (1.3750e+00)\tAcc@1   6.25 (  6.25)\n",
      "[xla:4]Train:  Epoch: [0][  0/129]\tTime  6.287 ( 6.287)\tData  0.071 ( 0.071)\tLoss 1.3516e+00 (1.3516e+00)\tAcc@1  12.50 ( 12.50)\n",
      "[xla:2]Train:  Epoch: [0][  0/129]\tTime  2.152 ( 2.152)\tData  0.057 ( 0.057)\tLoss 1.1719e+00 (1.1719e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:6]Train:  Epoch: [0][  0/129]\tTime  3.719 ( 3.719)\tData  0.053 ( 0.053)\tLoss 1.1250e+00 (1.1250e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [0][  0/129]\tTime  0.566 ( 0.566)\tData  0.057 ( 0.057)\tLoss 1.4844e+00 (1.4844e+00)\tAcc@1  18.75 ( 18.75)\n",
      "[xla:3]Train:  Epoch: [0][  0/129]\tTime  0.327 ( 0.327)\tData  0.068 ( 0.068)\tLoss 1.3672e+00 (1.3672e+00)\tAcc@1   6.25 (  6.25)\n",
      "[xla:7]Train:  Epoch: [0][  0/129]\tTime  0.325 ( 0.325)\tData  0.053 ( 0.053)\tLoss 1.2188e+00 (1.2188e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:1]Train:  Epoch: [0][ 30/129]\tTime  0.164 ( 0.990)\tData  0.011 ( 0.773)\tLoss 1.1875e+00 (1.2145e+00)\tAcc@1  37.50 ( 32.06)\n",
      "[xla:2]Train:  Epoch: [0][ 30/129]\tTime  0.216 ( 1.042)\tData  0.010 ( 0.773)\tLoss 1.1328e+00 (1.1676e+00)\tAcc@1  43.75 ( 33.87)\n",
      "[xla:3]Train:  Epoch: [0][ 30/129]\tTime  0.201 ( 0.934)\tData  0.010 ( 0.725)\tLoss 1.0781e+00 (1.1912e+00)\tAcc@1  25.00 ( 33.87)\n",
      "[xla:4]Train:  Epoch: [0][ 30/129]\tTime  0.192 ( 1.176)\tData  0.009 ( 0.774)\tLoss 1.1094e+00 (1.1479e+00)\tAcc@1  31.25 ( 37.31)\n",
      "[xla:0]Train:  Epoch: [0][ 30/129]\tTime  0.198 ( 1.282)\tData  0.008 ( 0.777)\tLoss 1.2109e+00 (1.2271e+00)\tAcc@1  37.50 ( 29.84)\n",
      "[xla:7]Train:  Epoch: [0][ 30/129]\tTime  0.199 ( 0.882)\tData  0.010 ( 0.672)\tLoss 1.2500e+00 (1.1599e+00)\tAcc@1  37.50 ( 35.29)\n",
      "[xla:5]Train:  Epoch: [0][ 30/129]\tTime  0.213 ( 1.230)\tData  0.010 ( 0.778)\tLoss 9.7656e-01 (1.1889e+00)\tAcc@1  56.25 ( 35.48)\n",
      "[xla:6]Train:  Epoch: [0][ 30/129]\tTime  0.224 ( 1.093)\tData  0.011 ( 0.776)\tLoss 1.1250e+00 (1.1695e+00)\tAcc@1  31.25 ( 36.90)\n",
      "[xla:4]Train:  Epoch: [0][ 60/129]\tTime  0.184 ( 0.702)\tData  0.010 ( 0.400)\tLoss 1.0703e+00 (1.1466e+00)\tAcc@1  50.00 ( 37.41)\n",
      "[xla:3]Train:  Epoch: [0][ 60/129]\tTime  0.207 ( 0.579)\tData  0.020 ( 0.376)\tLoss 1.1250e+00 (1.1732e+00)\tAcc@1  43.75 ( 33.81)\n",
      "[xla:0]Train:  Epoch: [0][ 60/129]\tTime  0.207 ( 0.756)\tData  0.011 ( 0.402)\tLoss 1.1172e+00 (1.1995e+00)\tAcc@1  31.25 ( 32.89)\n",
      "[xla:1]Train:  Epoch: [0][ 60/129]\tTime  0.198 ( 0.608)\tData  0.009 ( 0.400)\tLoss 1.1719e+00 (1.1707e+00)\tAcc@1  31.25 ( 34.32)\n",
      "[xla:6]Train:  Epoch: [0][ 60/129]\tTime  0.184 ( 0.660)\tData  0.008 ( 0.402)\tLoss 1.1719e+00 (1.1520e+00)\tAcc@1  37.50 ( 37.91)\n",
      "[xla:2]Train:  Epoch: [0][ 60/129]\tTime  0.220 ( 0.635)\tData  0.013 ( 0.400)\tLoss 1.1094e+00 (1.1473e+00)\tAcc@1  37.50 ( 35.35)\n",
      "[xla:5]Train:  Epoch: [0][ 60/129]\tTime  0.228 ( 0.730)\tData  0.014 ( 0.403)\tLoss 1.1719e+00 (1.1780e+00)\tAcc@1  18.75 ( 33.81)\n",
      "[xla:7]Train:  Epoch: [0][ 60/129]\tTime  0.195 ( 0.553)\tData  0.008 ( 0.349)\tLoss 1.0234e+00 (1.1504e+00)\tAcc@1  43.75 ( 36.07)\n",
      "[xla:7]Train:  Epoch: [0][ 90/129]\tTime  0.173 ( 0.440)\tData  0.011 ( 0.239)\tLoss 1.2031e+00 (1.1440e+00)\tAcc@1  37.50 ( 37.02)\n",
      "[xla:5]Train:  Epoch: [0][ 90/129]\tTime  0.179 ( 0.559)\tData  0.011 ( 0.275)\tLoss 9.3359e-01 (1.1696e+00)\tAcc@1  62.50 ( 33.65)\n",
      "[xla:6]Train:  Epoch: [0][ 90/129]\tTime  0.199 ( 0.512)\tData  0.011 ( 0.274)\tLoss 1.0078e+00 (1.1475e+00)\tAcc@1  56.25 ( 36.96)\n",
      "[xla:4]Train:  Epoch: [0][ 90/129]\tTime  0.192 ( 0.540)\tData  0.011 ( 0.273)\tLoss 1.3047e+00 (1.1470e+00)\tAcc@1  18.75 ( 35.86)\n",
      "[xla:0]Train:  Epoch: [0][ 90/129]\tTime  0.185 ( 0.576)\tData  0.011 ( 0.274)\tLoss 1.3125e+00 (1.1806e+00)\tAcc@1  12.50 ( 33.93)\n",
      "[xla:2]Train:  Epoch: [0][ 90/129]\tTime  0.202 ( 0.495)\tData  0.011 ( 0.273)\tLoss 1.0703e+00 (1.1509e+00)\tAcc@1  50.00 ( 34.82)\n",
      "[xla:1]Train:  Epoch: [0][ 90/129]\tTime  0.222 ( 0.478)\tData  0.011 ( 0.273)\tLoss 1.0938e+00 (1.1550e+00)\tAcc@1  31.25 ( 34.62)\n",
      "[xla:3]Train:  Epoch: [0][ 90/129]\tTime  0.235 ( 0.458)\tData  0.015 ( 0.256)\tLoss 1.1562e+00 (1.1507e+00)\tAcc@1  37.50 ( 35.58)\n",
      "[xla:2]Train:  Epoch: [0][120/129]\tTime  0.268 ( 0.425)\tData  0.011 ( 0.209)\tLoss 1.0156e+00 (1.1367e+00)\tAcc@1  37.50 ( 36.06)\n",
      "[xla:0]Train:  Epoch: [0][120/129]\tTime  0.260 ( 0.486)\tData  0.011 ( 0.209)\tLoss 1.1797e+00 (1.1674e+00)\tAcc@1  25.00 ( 35.28)\n",
      "[xla:4]Train:  Epoch: [0][120/129]\tTime  0.261 ( 0.459)\tData  0.012 ( 0.209)\tLoss 1.1016e+00 (1.1362e+00)\tAcc@1  37.50 ( 37.04)\n",
      "[xla:1]Train:  Epoch: [0][120/129]\tTime  0.266 ( 0.412)\tData  0.010 ( 0.208)\tLoss 1.1172e+00 (1.1447e+00)\tAcc@1  43.75 ( 35.07)\n",
      "[xla:6]Train:  Epoch: [0][120/129]\tTime  0.284 ( 0.438)\tData  0.010 ( 0.210)\tLoss 9.8828e-01 (1.1427e+00)\tAcc@1  43.75 ( 36.57)\n",
      "[xla:5]Train:  Epoch: [0][120/129]\tTime  0.281 ( 0.473)\tData  0.022 ( 0.210)\tLoss 1.0547e+00 (1.1597e+00)\tAcc@1  37.50 ( 34.30)\n",
      "[xla:7]Train:  Epoch: [0][120/129]\tTime  0.265 ( 0.384)\tData  0.016 ( 0.183)\tLoss 1.1797e+00 (1.1373e+00)\tAcc@1  37.50 ( 37.45)\n",
      "[xla:3]Train:  Epoch: [0][120/129]\tTime  0.279 ( 0.397)\tData  0.012 ( 0.196)\tLoss 1.0781e+00 (1.1399e+00)\tAcc@1  25.00 ( 36.68)\n",
      "Finished training epoch 0\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:3]Validation: [0/6]\tTime  4.934 ( 4.934)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:5]Validation: [0/6]\tTime  4.937 ( 4.937)\tLoss 9.2188e-01 (9.2188e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Validation: [0/6]\tTime  4.975 ( 4.975)\tLoss 1.1250e+00 (1.1250e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Validation: [0/6]\tTime  4.894 ( 4.894)\tLoss 1.0938e+00 (1.0938e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Validation: [0/6]\tTime  5.330 ( 5.330)\tLoss 1.1719e+00 (1.1719e+00)\tAcc@1  25.00 ( 25.00)\n",
      "[xla:4]Validation: [0/6]\tTime  5.112 ( 5.112)\tLoss 1.1328e+00 (1.1328e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:0]Validation: [0/6]\tTime  4.864 ( 4.864)\tLoss 1.1172e+00 (1.1172e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:1]Validation: [0/6]\tTime  5.047 ( 5.047)\tLoss 1.0547e+00 (1.0547e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [1][  0/129]\tTime  0.151 ( 0.151)\tData  0.035 ( 0.035)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:3]Train:  Epoch: [1][  0/129]\tTime  0.264 ( 0.264)\tData  0.093 ( 0.093)\tLoss 1.0703e+00 (1.0703e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:0]Train:  Epoch: [1][  0/129]\tTime  0.167 ( 0.167)\tData  0.029 ( 0.029)\tLoss 1.1484e+00 (1.1484e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:4]Train:  Epoch: [1][  0/129]\tTime  0.214 ( 0.214)\tData  0.032 ( 0.032)\tLoss 1.1484e+00 (1.1484e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:7]Train:  Epoch: [1][  0/129]\tTime  0.219 ( 0.219)\tData  0.035 ( 0.035)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [1][  0/129]\tTime  0.215 ( 0.215)\tData  0.091 ( 0.091)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:6]Train:  Epoch: [1][  0/129]\tTime  0.237 ( 0.237)\tData  0.049 ( 0.049)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [1][  0/129]\tTime  0.236 ( 0.236)\tData  0.049 ( 0.049)\tLoss 1.1953e+00 (1.1953e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:1]Train:  Epoch: [1][ 30/129]\tTime  0.177 ( 0.215)\tData  0.013 ( 0.016)\tLoss 9.3359e-01 (1.1502e+00)\tAcc@1  56.25 ( 35.69)\n",
      "[xla:2]Train:  Epoch: [1][ 30/129]\tTime  0.192 ( 0.203)\tData  0.011 ( 0.015)\tLoss 1.1406e+00 (1.1027e+00)\tAcc@1  18.75 ( 37.51)\n",
      "[xla:3]Train:  Epoch: [1][ 30/129]\tTime  0.172 ( 0.215)\tData  0.008 ( 0.018)\tLoss 1.1719e+00 (1.1210e+00)\tAcc@1  37.50 ( 37.30)\n",
      "[xla:0]Train:  Epoch: [1][ 30/129]\tTime  0.201 ( 0.209)\tData  0.013 ( 0.014)\tLoss 1.1172e+00 (1.1542e+00)\tAcc@1  43.75 ( 35.28)\n",
      "[xla:7]Train:  Epoch: [1][ 30/129]\tTime  0.193 ( 0.204)\tData  0.011 ( 0.017)\tLoss 1.3828e+00 (1.1067e+00)\tAcc@1  18.75 ( 40.93)\n",
      "[xla:6]Train:  Epoch: [1][ 30/129]\tTime  0.180 ( 0.204)\tData  0.008 ( 0.015)\tLoss 1.2734e+00 (1.1220e+00)\tAcc@1  31.25 ( 36.69)\n",
      "[xla:4]Train:  Epoch: [1][ 30/129]\tTime  0.211 ( 0.204)\tData  0.014 ( 0.014)\tLoss 1.1406e+00 (1.0835e+00)\tAcc@1  31.25 ( 41.74)\n",
      "[xla:5]Train:  Epoch: [1][ 30/129]\tTime  0.217 ( 0.203)\tData  0.011 ( 0.016)\tLoss 8.3203e-01 (1.1133e+00)\tAcc@1  75.00 ( 38.10)\n",
      "[xla:7]Train:  Epoch: [1][ 60/129]\tTime  0.159 ( 0.206)\tData  0.010 ( 0.016)\tLoss 9.4922e-01 (1.1002e+00)\tAcc@1  62.50 ( 40.68)\n",
      "[xla:2]Train:  Epoch: [1][ 60/129]\tTime  0.196 ( 0.206)\tData  0.014 ( 0.015)\tLoss 9.7266e-01 (1.0932e+00)\tAcc@1  43.75 ( 39.25)\n",
      "[xla:0]Train:  Epoch: [1][ 60/129]\tTime  0.211 ( 0.209)\tData  0.015 ( 0.015)\tLoss 1.1328e+00 (1.1287e+00)\tAcc@1  31.25 ( 37.50)\n",
      "[xla:1]Train:  Epoch: [1][ 60/129]\tTime  0.225 ( 0.212)\tData  0.021 ( 0.016)\tLoss 1.2656e+00 (1.1181e+00)\tAcc@1  18.75 ( 37.91)\n",
      "[xla:3]Train:  Epoch: [1][ 60/129]\tTime  0.191 ( 0.212)\tData  0.011 ( 0.016)\tLoss 1.0703e+00 (1.1032e+00)\tAcc@1  50.00 ( 40.06)\n",
      "[xla:5]Train:  Epoch: [1][ 60/129]\tTime  0.192 ( 0.206)\tData  0.009 ( 0.016)\tLoss 9.5703e-01 (1.1188e+00)\tAcc@1  50.00 ( 38.63)\n",
      "[xla:6]Train:  Epoch: [1][ 60/129]\tTime  0.247 ( 0.206)\tData  0.015 ( 0.015)\tLoss 1.1328e+00 (1.1096e+00)\tAcc@1  43.75 ( 37.81)\n",
      "[xla:4]Train:  Epoch: [1][ 60/129]\tTime  0.191 ( 0.206)\tData  0.009 ( 0.015)\tLoss 1.0625e+00 (1.0921e+00)\tAcc@1  37.50 ( 39.86)\n",
      "[xla:3]Train:  Epoch: [1][ 90/129]\tTime  0.173 ( 0.213)\tData  0.010 ( 0.016)\tLoss 1.1250e+00 (1.0853e+00)\tAcc@1  50.00 ( 42.38)\n",
      "[xla:1]Train:  Epoch: [1][ 90/129]\tTime  0.183 ( 0.213)\tData  0.009 ( 0.015)\tLoss 8.3594e-01 (1.0977e+00)\tAcc@1  69.00 ( 40.32)\n",
      "[xla:5]Train:  Epoch: [1][ 90/129]\tTime  0.203 ( 0.209)\tData  0.009 ( 0.016)\tLoss 9.5703e-01 (1.1064e+00)\tAcc@1  62.50 ( 40.52)\n",
      "[xla:0]Train:  Epoch: [1][ 90/129]\tTime  0.192 ( 0.211)\tData  0.011 ( 0.015)\tLoss 1.2266e+00 (1.1112e+00)\tAcc@1  18.75 ( 39.84)\n",
      "[xla:4]Train:  Epoch: [1][ 90/129]\tTime  0.230 ( 0.209)\tData  0.012 ( 0.015)\tLoss 1.3750e+00 (1.0969e+00)\tAcc@1  25.00 ( 40.11)\n",
      "[xla:6]Train:  Epoch: [1][ 90/129]\tTime  0.194 ( 0.209)\tData  0.008 ( 0.016)\tLoss 1.0078e+00 (1.0992e+00)\tAcc@1  43.75 ( 38.88)\n",
      "[xla:7]Train:  Epoch: [1][ 90/129]\tTime  0.255 ( 0.209)\tData  0.014 ( 0.016)\tLoss 1.1250e+00 (1.0965e+00)\tAcc@1  31.25 ( 40.73)\n",
      "[xla:2]Train:  Epoch: [1][ 90/129]\tTime  0.211 ( 0.209)\tData  0.010 ( 0.015)\tLoss 8.5938e-01 (1.0927e+00)\tAcc@1  62.50 ( 40.12)\n",
      "[xla:2]Train:  Epoch: [1][120/129]\tTime  0.189 ( 0.212)\tData  0.009 ( 0.015)\tLoss 9.7266e-01 (1.0772e+00)\tAcc@1  62.50 ( 42.05)\n",
      "[xla:0]Train:  Epoch: [1][120/129]\tTime  0.178 ( 0.214)\tData  0.016 ( 0.015)\tLoss 1.2188e+00 (1.1017e+00)\tAcc@1  31.25 ( 41.33)\n",
      "[xla:3]Train:  Epoch: [1][120/129]\tTime  0.183 ( 0.215)\tData  0.019 ( 0.016)\tLoss 1.1094e+00 (1.0763e+00)\tAcc@1  43.75 ( 43.35)\n",
      "[xla:4]Train:  Epoch: [1][120/129]\tTime  0.196 ( 0.212)\tData  0.019 ( 0.015)\tLoss 9.4141e-01 (1.0837e+00)\tAcc@1  50.00 ( 41.68)\n",
      "[xla:5]Train:  Epoch: [1][120/129]\tTime  0.208 ( 0.212)\tData  0.010 ( 0.016)\tLoss 1.1016e+00 (1.0974e+00)\tAcc@1  37.50 ( 41.43)\n",
      "[xla:7]Train:  Epoch: [1][120/129]\tTime  0.188 ( 0.212)\tData  0.011 ( 0.015)\tLoss 1.0703e+00 (1.0893e+00)\tAcc@1  50.00 ( 41.59)\n",
      "[xla:1]Train:  Epoch: [1][120/129]\tTime  0.190 ( 0.215)\tData  0.011 ( 0.015)\tLoss 1.0625e+00 (1.0794e+00)\tAcc@1  43.75 ( 42.05)\n",
      "[xla:6]Train:  Epoch: [1][120/129]\tTime  0.201 ( 0.212)\tData  0.013 ( 0.015)\tLoss 8.5156e-01 (1.0891e+00)\tAcc@1  62.50 ( 39.88)\n",
      "[xla:3]Validation: [0/6]\tTime  0.064 ( 0.064)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 1\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/6]\tTime  0.107 ( 0.107)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.080 ( 0.080)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 1.1094e+00 (1.1094e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Validation: [0/6]\tTime  0.085 ( 0.085)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.1016e+00 (1.1016e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:5]Validation: [0/6]\tTime  0.115 ( 0.115)\tLoss 8.3203e-01 (8.3203e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Validation: [0/6]\tTime  0.127 ( 0.127)\tLoss 1.2031e+00 (1.2031e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:3]Train:  Epoch: [2][  0/129]\tTime  0.204 ( 0.204)\tData  0.050 ( 0.050)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [2][  0/129]\tTime  0.165 ( 0.165)\tData  0.042 ( 0.042)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  18.75 ( 18.75)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [2][  0/129]\tTime  0.207 ( 0.207)\tData  0.060 ( 0.060)\tLoss 1.0938e+00 (1.0938e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:5]Train:  Epoch: [2][  0/129]\tTime  0.208 ( 0.208)\tData  0.056 ( 0.056)\tLoss 1.0703e+00 (1.0703e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:6]Train:  Epoch: [2][  0/129]\tTime  0.218 ( 0.218)\tData  0.083 ( 0.083)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Train:  Epoch: [2][  0/129]\tTime  0.279 ( 0.279)\tData  0.111 ( 0.111)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [2][  0/129]\tTime  0.229 ( 0.229)\tData  0.049 ( 0.049)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:4]Train:  Epoch: [2][  0/129]\tTime  0.260 ( 0.260)\tData  0.084 ( 0.084)\tLoss 1.1562e+00 (1.1562e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [2][ 30/129]\tTime  0.212 ( 0.214)\tData  0.017 ( 0.018)\tLoss 1.0625e+00 (1.0995e+00)\tAcc@1  37.50 ( 39.32)\n",
      "[xla:5]Train:  Epoch: [2][ 30/129]\tTime  0.197 ( 0.210)\tData  0.011 ( 0.018)\tLoss 8.5938e-01 (1.0687e+00)\tAcc@1  56.25 ( 42.54)\n",
      "[xla:4]Train:  Epoch: [2][ 30/129]\tTime  0.191 ( 0.207)\tData  0.012 ( 0.017)\tLoss 1.0234e+00 (1.0495e+00)\tAcc@1  50.00 ( 42.95)\n",
      "[xla:7]Train:  Epoch: [2][ 30/129]\tTime  0.206 ( 0.212)\tData  0.011 ( 0.019)\tLoss 1.2109e+00 (1.0844e+00)\tAcc@1  31.25 ( 44.15)\n",
      "[xla:2]Train:  Epoch: [2][ 30/129]\tTime  0.197 ( 0.213)\tData  0.010 ( 0.017)\tLoss 1.0000e+00 (1.0541e+00)\tAcc@1  50.00 ( 44.56)\n",
      "[xla:3]Train:  Epoch: [2][ 30/129]\tTime  0.208 ( 0.224)\tData  0.010 ( 0.015)\tLoss 1.0312e+00 (1.0669e+00)\tAcc@1  37.50 ( 42.94)\n",
      "[xla:0]Train:  Epoch: [2][ 30/129]\tTime  0.207 ( 0.206)\tData  0.019 ( 0.015)\tLoss 1.1641e+00 (1.1033e+00)\tAcc@1  25.00 ( 39.11)\n",
      "[xla:6]Train:  Epoch: [2][ 30/129]\tTime  0.192 ( 0.210)\tData  0.008 ( 0.017)\tLoss 1.0547e+00 (1.0451e+00)\tAcc@1  37.50 ( 47.79)\n",
      "[xla:0]Train:  Epoch: [2][ 60/129]\tTime  0.156 ( 0.206)\tData  0.012 ( 0.015)\tLoss 1.0703e+00 (1.0802e+00)\tAcc@1  31.25 ( 43.56)\n",
      "[xla:5]Train:  Epoch: [2][ 60/129]\tTime  0.179 ( 0.209)\tData  0.013 ( 0.017)\tLoss 9.6484e-01 (1.0615e+00)\tAcc@1  50.00 ( 43.45)\n",
      "[xla:6]Train:  Epoch: [2][ 60/129]\tTime  0.179 ( 0.208)\tData  0.008 ( 0.016)\tLoss 1.1484e+00 (1.0464e+00)\tAcc@1  43.75 ( 47.66)\n",
      "[xla:4]Train:  Epoch: [2][ 60/129]\tTime  0.191 ( 0.207)\tData  0.013 ( 0.016)\tLoss 1.0781e+00 (1.0517e+00)\tAcc@1  37.50 ( 43.96)\n",
      "[xla:7]Train:  Epoch: [2][ 60/129]\tTime  0.203 ( 0.209)\tData  0.013 ( 0.018)\tLoss 7.0703e-01 (1.0583e+00)\tAcc@1  87.50 ( 46.73)\n",
      "[xla:3]Train:  Epoch: [2][ 60/129]\tTime  0.218 ( 0.216)\tData  0.012 ( 0.016)\tLoss 9.6875e-01 (1.0321e+00)\tAcc@1  56.25 ( 46.64)\n",
      "[xla:2]Train:  Epoch: [2][ 60/129]\tTime  0.219 ( 0.210)\tData  0.011 ( 0.016)\tLoss 9.3750e-01 (1.0395e+00)\tAcc@1  50.00 ( 46.63)\n",
      "[xla:1]Train:  Epoch: [2][ 60/129]\tTime  0.214 ( 0.211)\tData  0.013 ( 0.015)\tLoss 1.1484e+00 (1.0637e+00)\tAcc@1  31.25 ( 43.14)\n",
      "[xla:3]Train:  Epoch: [2][ 90/129]\tTime  0.162 ( 0.212)\tData  0.010 ( 0.015)\tLoss 9.1797e-01 (1.0124e+00)\tAcc@1  62.50 ( 48.92)\n",
      "[xla:0]Train:  Epoch: [2][ 90/129]\tTime  0.187 ( 0.206)\tData  0.011 ( 0.015)\tLoss 1.3359e+00 (1.0617e+00)\tAcc@1  43.75 ( 44.93)\n",
      "[xla:4]Train:  Epoch: [2][ 90/129]\tTime  0.228 ( 0.207)\tData  0.011 ( 0.015)\tLoss 1.3203e+00 (1.0427e+00)\tAcc@1  25.00 ( 45.55)\n",
      "[xla:6]Train:  Epoch: [2][ 90/129]\tTime  0.201 ( 0.208)\tData  0.010 ( 0.016)\tLoss 9.8047e-01 (1.0438e+00)\tAcc@1  43.75 ( 46.99)\n",
      "[xla:7]Train:  Epoch: [2][ 90/129]\tTime  0.214 ( 0.208)\tData  0.011 ( 0.016)\tLoss 1.0234e+00 (1.0530e+00)\tAcc@1  37.50 ( 46.64)\n",
      "[xla:1]Train:  Epoch: [2][ 90/129]\tTime  0.193 ( 0.209)\tData  0.010 ( 0.015)\tLoss 6.7188e-01 (1.0423e+00)\tAcc@1  75.00 ( 44.99)\n",
      "[xla:2]Train:  Epoch: [2][ 90/129]\tTime  0.220 ( 0.209)\tData  0.012 ( 0.015)\tLoss 8.2812e-01 (1.0411e+00)\tAcc@1  56.25 ( 46.99)\n",
      "[xla:5]Train:  Epoch: [2][ 90/129]\tTime  0.200 ( 0.208)\tData  0.011 ( 0.016)\tLoss 8.4375e-01 (1.0601e+00)\tAcc@1  56.25 ( 44.23)\n",
      "[xla:5]Train:  Epoch: [2][120/129]\tTime  0.167 ( 0.208)\tData  0.011 ( 0.016)\tLoss 9.2188e-01 (1.0455e+00)\tAcc@1  50.00 ( 45.61)\n",
      "[xla:0]Train:  Epoch: [2][120/129]\tTime  0.169 ( 0.207)\tData  0.010 ( 0.015)\tLoss 1.0234e+00 (1.0403e+00)\tAcc@1  43.75 ( 47.17)\n",
      "[xla:6]Train:  Epoch: [2][120/129]\tTime  0.219 ( 0.208)\tData  0.016 ( 0.016)\tLoss 6.5234e-01 (1.0232e+00)\tAcc@1  62.50 ( 48.47)\n",
      "[xla:2]Train:  Epoch: [2][120/129]\tTime  0.215 ( 0.209)\tData  0.014 ( 0.015)\tLoss 8.6328e-01 (1.0131e+00)\tAcc@1  62.50 ( 48.72)\n",
      "[xla:4]Train:  Epoch: [2][120/129]\tTime  0.194 ( 0.208)\tData  0.012 ( 0.015)\tLoss 9.4141e-01 (1.0208e+00)\tAcc@1  50.00 ( 47.84)\n",
      "[xla:1]Train:  Epoch: [2][120/129]\tTime  0.210 ( 0.210)\tData  0.012 ( 0.016)\tLoss 9.6484e-01 (1.0209e+00)\tAcc@1  56.25 ( 46.96)\n",
      "[xla:3]Train:  Epoch: [2][120/129]\tTime  0.194 ( 0.212)\tData  0.011 ( 0.015)\tLoss 8.8672e-01 (1.0089e+00)\tAcc@1  50.00 ( 49.24)\n",
      "[xla:7]Train:  Epoch: [2][120/129]\tTime  0.199 ( 0.209)\tData  0.010 ( 0.016)\tLoss 9.4531e-01 (1.0318e+00)\tAcc@1  56.25 ( 47.63)\n",
      "[xla:3]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 9.8047e-01 (9.8047e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 9.7656e-01 (9.7656e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Finished training epoch 2\n",
      "[xla:4]Validation: [0/6]\tTime  0.109 ( 0.109)\tLoss 1.2578e+00 (1.2578e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/6]\tTime  0.081 ( 0.081)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.128 ( 0.128)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.076 ( 0.076)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 7.6172e-01 (7.6172e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Validation: [0/6]\tTime  0.125 ( 0.125)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [3][  0/129]\tTime  0.241 ( 0.241)\tData  0.076 ( 0.076)\tLoss 8.2812e-01 (8.2812e-01)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:7]Train:  Epoch: [3][  0/129]\tTime  0.171 ( 0.171)\tData  0.036 ( 0.036)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Train:  Epoch: [3][  0/129]\tTime  0.326 ( 0.326)\tData  0.119 ( 0.119)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [3][  0/129]\tTime  0.146 ( 0.146)\tData  0.025 ( 0.025)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:2]Train:  Epoch: [3][  0/129]\tTime  0.258 ( 0.258)\tData  0.070 ( 0.070)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Train:  Epoch: [3][  0/129]\tTime  0.214 ( 0.214)\tData  0.039 ( 0.039)\tLoss 1.1172e+00 (1.1172e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [3][  0/129]\tTime  0.264 ( 0.264)\tData  0.094 ( 0.094)\tLoss 1.1406e+00 (1.1406e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:6]Train:  Epoch: [3][  0/129]\tTime  0.253 ( 0.253)\tData  0.122 ( 0.122)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [3][ 30/129]\tTime  0.186 ( 0.214)\tData  0.017 ( 0.016)\tLoss 1.3672e+00 (1.0281e+00)\tAcc@1  31.25 ( 49.19)\n",
      "[xla:2]Train:  Epoch: [3][ 30/129]\tTime  0.184 ( 0.213)\tData  0.010 ( 0.016)\tLoss 8.5156e-01 (1.0193e+00)\tAcc@1  56.25 ( 48.01)\n",
      "[xla:6]Train:  Epoch: [3][ 30/129]\tTime  0.206 ( 0.208)\tData  0.016 ( 0.017)\tLoss 1.0547e+00 (9.8765e-01)\tAcc@1  37.50 ( 49.81)\n",
      "[xla:1]Train:  Epoch: [3][ 30/129]\tTime  0.207 ( 0.209)\tData  0.014 ( 0.017)\tLoss 1.1016e+00 (1.0372e+00)\tAcc@1  43.75 ( 45.96)\n",
      "[xla:0]Train:  Epoch: [3][ 30/129]\tTime  0.178 ( 0.211)\tData  0.010 ( 0.015)\tLoss 1.0391e+00 (1.0772e+00)\tAcc@1  37.50 ( 39.92)\n",
      "[xla:3]Train:  Epoch: [3][ 30/129]\tTime  0.209 ( 0.224)\tData  0.013 ( 0.018)\tLoss 9.2188e-01 (1.0165e+00)\tAcc@1  50.00 ( 48.80)\n",
      "[xla:4]Train:  Epoch: [3][ 30/129]\tTime  0.196 ( 0.218)\tData  0.009 ( 0.018)\tLoss 9.3359e-01 (1.0032e+00)\tAcc@1  56.25 ( 50.81)\n",
      "[xla:5]Train:  Epoch: [3][ 30/129]\tTime  0.201 ( 0.211)\tData  0.008 ( 0.014)\tLoss 8.9062e-01 (1.0113e+00)\tAcc@1  56.25 ( 49.22)\n",
      "[xla:5]Train:  Epoch: [3][ 60/129]\tTime  0.203 ( 0.209)\tData  0.010 ( 0.014)\tLoss 9.6875e-01 (1.0322e+00)\tAcc@1  43.75 ( 47.25)\n",
      "[xla:2]Train:  Epoch: [3][ 60/129]\tTime  0.226 ( 0.211)\tData  0.012 ( 0.015)\tLoss 9.1016e-01 (1.0127e+00)\tAcc@1  50.00 ( 48.37)\n",
      "[xla:0]Train:  Epoch: [3][ 60/129]\tTime  0.176 ( 0.209)\tData  0.010 ( 0.016)\tLoss 9.0625e-01 (1.0503e+00)\tAcc@1  62.50 ( 46.22)\n",
      "[xla:7]Train:  Epoch: [3][ 60/129]\tTime  0.188 ( 0.211)\tData  0.012 ( 0.017)\tLoss 7.5391e-01 (1.0257e+00)\tAcc@1  62.50 ( 49.28)\n",
      "[xla:3]Train:  Epoch: [3][ 60/129]\tTime  0.196 ( 0.216)\tData  0.011 ( 0.016)\tLoss 8.6328e-01 (9.8630e-01)\tAcc@1  56.25 ( 50.52)\n",
      "[xla:1]Train:  Epoch: [3][ 60/129]\tTime  0.192 ( 0.208)\tData  0.011 ( 0.016)\tLoss 1.1719e+00 (1.0323e+00)\tAcc@1  31.25 ( 46.52)\n",
      "[xla:6]Train:  Epoch: [3][ 60/129]\tTime  0.188 ( 0.208)\tData  0.009 ( 0.016)\tLoss 1.1484e+00 (1.0135e+00)\tAcc@1  50.00 ( 48.17)\n",
      "[xla:4]Train:  Epoch: [3][ 60/129]\tTime  0.202 ( 0.213)\tData  0.009 ( 0.017)\tLoss 8.9453e-01 (1.0088e+00)\tAcc@1  56.25 ( 50.21)\n",
      "[xla:5]Train:  Epoch: [3][ 90/129]\tTime  0.161 ( 0.210)\tData  0.012 ( 0.015)\tLoss 7.3828e-01 (1.0249e+00)\tAcc@1  62.50 ( 48.64)\n",
      "[xla:3]Train:  Epoch: [3][ 90/129]\tTime  0.220 ( 0.214)\tData  0.021 ( 0.016)\tLoss 8.2031e-01 (9.6755e-01)\tAcc@1  50.00 ( 52.55)\n",
      "[xla:2]Train:  Epoch: [3][ 90/129]\tTime  0.182 ( 0.211)\tData  0.012 ( 0.015)\tLoss 8.5938e-01 (1.0205e+00)\tAcc@1  69.00 ( 48.51)\n",
      "[xla:4]Train:  Epoch: [3][ 90/129]\tTime  0.187 ( 0.213)\tData  0.012 ( 0.016)\tLoss 1.2656e+00 (1.0051e+00)\tAcc@1  31.25 ( 50.49)\n",
      "[xla:7]Train:  Epoch: [3][ 90/129]\tTime  0.179 ( 0.212)\tData  0.010 ( 0.016)\tLoss 8.2031e-01 (1.0186e+00)\tAcc@1  75.00 ( 49.87)\n",
      "[xla:0]Train:  Epoch: [3][ 90/129]\tTime  0.189 ( 0.210)\tData  0.011 ( 0.015)\tLoss 1.1953e+00 (1.0202e+00)\tAcc@1  43.75 ( 49.19)\n",
      "[xla:1]Train:  Epoch: [3][ 90/129]\tTime  0.190 ( 0.210)\tData  0.011 ( 0.016)\tLoss 7.6562e-01 (1.0114e+00)\tAcc@1  75.00 ( 48.01)\n",
      "[xla:6]Train:  Epoch: [3][ 90/129]\tTime  0.222 ( 0.210)\tData  0.011 ( 0.017)\tLoss 8.8672e-01 (1.0144e+00)\tAcc@1  50.00 ( 48.78)\n",
      "[xla:1]Train:  Epoch: [3][120/129]\tTime  0.221 ( 0.211)\tData  0.015 ( 0.015)\tLoss 9.7656e-01 (9.8718e-01)\tAcc@1  56.25 ( 50.10)\n",
      "[xla:6]Train:  Epoch: [3][120/129]\tTime  0.256 ( 0.211)\tData  0.013 ( 0.016)\tLoss 6.9922e-01 (9.8844e-01)\tAcc@1  75.00 ( 51.21)\n",
      "[xla:7]Train:  Epoch: [3][120/129]\tTime  0.215 ( 0.213)\tData  0.014 ( 0.015)\tLoss 9.5703e-01 (9.9671e-01)\tAcc@1  43.75 ( 51.25)\n",
      "[xla:4]Train:  Epoch: [3][120/129]\tTime  0.224 ( 0.214)\tData  0.011 ( 0.016)\tLoss 8.7891e-01 (9.8557e-01)\tAcc@1  62.50 ( 52.13)\n",
      "[xla:5]Train:  Epoch: [3][120/129]\tTime  0.252 ( 0.212)\tData  0.009 ( 0.014)\tLoss 6.5234e-01 (1.0057e+00)\tAcc@1  69.00 ( 50.49)\n",
      "[xla:3]Train:  Epoch: [3][120/129]\tTime  0.265 ( 0.215)\tData  0.012 ( 0.016)\tLoss 8.2422e-01 (9.6210e-01)\tAcc@1  69.00 ( 53.43)\n",
      "[xla:2]Train:  Epoch: [3][120/129]\tTime  0.279 ( 0.213)\tData  0.015 ( 0.016)\tLoss 8.2031e-01 (9.8618e-01)\tAcc@1  75.00 ( 51.05)\n",
      "[xla:0]Train:  Epoch: [3][120/129]\tTime  0.293 ( 0.212)\tData  0.012 ( 0.015)\tLoss 9.4531e-01 (9.9264e-01)\tAcc@1  62.50 ( 51.56)\n",
      "[xla:3]Validation: [0/6]\tTime  0.077 ( 0.077)\tLoss 9.0234e-01 (9.0234e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.086 ( 0.086)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 3\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/6]\tTime  0.139 ( 0.139)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Validation: [0/6]\tTime  0.130 ( 0.130)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/6]\tTime  0.125 ( 0.125)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Validation: [0/6]\tTime  0.155 ( 0.155)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Validation: [0/6]\tTime  0.139 ( 0.139)\tLoss 9.4531e-01 (9.4531e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.120 ( 0.120)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:3]Train:  Epoch: [4][  0/129]\tTime  0.286 ( 0.286)\tData  0.100 ( 0.100)\tLoss 6.9141e-01 (6.9141e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Train:  Epoch: [4][  0/129]\tTime  0.240 ( 0.240)\tData  0.053 ( 0.053)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  43.75 ( 43.75)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [4][  0/129]\tTime  0.226 ( 0.226)\tData  0.036 ( 0.036)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Train:  Epoch: [4][  0/129]\tTime  0.155 ( 0.155)\tData  0.036 ( 0.036)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [4][  0/129]\tTime  0.269 ( 0.269)\tData  0.085 ( 0.085)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Train:  Epoch: [4][  0/129]\tTime  0.161 ( 0.161)\tData  0.030 ( 0.030)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [4][  0/129]\tTime  0.239 ( 0.239)\tData  0.063 ( 0.063)\tLoss 1.0703e+00 (1.0703e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Train:  Epoch: [4][  0/129]\tTime  0.187 ( 0.187)\tData  0.036 ( 0.036)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [4][ 30/129]\tTime  0.204 ( 0.205)\tData  0.011 ( 0.015)\tLoss 1.2188e+00 (1.0098e+00)\tAcc@1  31.25 ( 51.22)\n",
      "[xla:2]Train:  Epoch: [4][ 30/129]\tTime  0.162 ( 0.202)\tData  0.011 ( 0.014)\tLoss 1.1250e+00 (1.0001e+00)\tAcc@1  43.75 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [4][ 30/129]\tTime  0.219 ( 0.224)\tData  0.012 ( 0.018)\tLoss 8.9453e-01 (9.7933e-01)\tAcc@1  56.25 ( 54.48)\n",
      "[xla:4]Train:  Epoch: [4][ 30/129]\tTime  0.194 ( 0.210)\tData  0.010 ( 0.016)\tLoss 1.0156e+00 (9.6749e-01)\tAcc@1  43.75 ( 53.86)\n",
      "[xla:0]Train:  Epoch: [4][ 30/129]\tTime  0.178 ( 0.212)\tData  0.011 ( 0.020)\tLoss 1.0469e+00 (1.0213e+00)\tAcc@1  50.00 ( 49.61)\n",
      "[xla:6]Train:  Epoch: [4][ 30/129]\tTime  0.205 ( 0.222)\tData  0.010 ( 0.017)\tLoss 9.5703e-01 (9.2881e-01)\tAcc@1  37.50 ( 54.86)\n",
      "[xla:5]Train:  Epoch: [4][ 30/129]\tTime  0.215 ( 0.214)\tData  0.011 ( 0.016)\tLoss 8.1641e-01 (9.7291e-01)\tAcc@1  75.00 ( 52.43)\n",
      "[xla:1]Train:  Epoch: [4][ 30/129]\tTime  0.217 ( 0.208)\tData  0.011 ( 0.018)\tLoss 8.4375e-01 (9.8803e-01)\tAcc@1  56.25 ( 50.21)\n",
      "[xla:2]Train:  Epoch: [4][ 60/129]\tTime  0.166 ( 0.204)\tData  0.009 ( 0.014)\tLoss 8.8281e-01 (9.8060e-01)\tAcc@1  56.25 ( 52.27)\n",
      "[xla:3]Train:  Epoch: [4][ 60/129]\tTime  0.195 ( 0.215)\tData  0.010 ( 0.017)\tLoss 8.1641e-01 (9.5428e-01)\tAcc@1  62.50 ( 56.08)\n",
      "[xla:5]Train:  Epoch: [4][ 60/129]\tTime  0.178 ( 0.209)\tData  0.010 ( 0.016)\tLoss 9.4922e-01 (9.9744e-01)\tAcc@1  43.75 ( 50.52)\n",
      "[xla:4]Train:  Epoch: [4][ 60/129]\tTime  0.189 ( 0.208)\tData  0.010 ( 0.015)\tLoss 1.1094e+00 (9.8002e-01)\tAcc@1  43.75 ( 51.66)\n",
      "[xla:1]Train:  Epoch: [4][ 60/129]\tTime  0.231 ( 0.206)\tData  0.013 ( 0.016)\tLoss 1.0391e+00 (9.8341e-01)\tAcc@1  31.25 ( 49.91)\n",
      "[xla:7]Train:  Epoch: [4][ 60/129]\tTime  0.219 ( 0.206)\tData  0.011 ( 0.015)\tLoss 7.0703e-01 (9.7471e-01)\tAcc@1  75.00 ( 54.43)\n",
      "[xla:6]Train:  Epoch: [4][ 60/129]\tTime  0.197 ( 0.214)\tData  0.014 ( 0.016)\tLoss 1.1406e+00 (9.6747e-01)\tAcc@1  43.75 ( 52.99)\n",
      "[xla:0]Train:  Epoch: [4][ 60/129]\tTime  0.196 ( 0.209)\tData  0.010 ( 0.017)\tLoss 9.2969e-01 (1.0149e+00)\tAcc@1  50.00 ( 50.53)\n",
      "[xla:0]Train:  Epoch: [4][ 90/129]\tTime  0.209 ( 0.208)\tData  0.014 ( 0.017)\tLoss 1.2344e+00 (9.8352e-01)\tAcc@1  43.75 ( 52.29)\n",
      "[xla:3]Train:  Epoch: [4][ 90/129]\tTime  0.158 ( 0.212)\tData  0.008 ( 0.016)\tLoss 7.7734e-01 (9.4081e-01)\tAcc@1  56.25 ( 56.42)\n",
      "[xla:4]Train:  Epoch: [4][ 90/129]\tTime  0.206 ( 0.207)\tData  0.015 ( 0.015)\tLoss 1.1016e+00 (9.7339e-01)\tAcc@1  37.50 ( 52.29)\n",
      "[xla:7]Train:  Epoch: [4][ 90/129]\tTime  0.220 ( 0.206)\tData  0.012 ( 0.015)\tLoss 8.5938e-01 (9.7077e-01)\tAcc@1  69.00 ( 53.52)\n",
      "[xla:6]Train:  Epoch: [4][ 90/129]\tTime  0.233 ( 0.211)\tData  0.011 ( 0.016)\tLoss 8.6719e-01 (9.7180e-01)\tAcc@1  56.25 ( 52.43)\n",
      "[xla:1]Train:  Epoch: [4][ 90/129]\tTime  0.214 ( 0.206)\tData  0.012 ( 0.015)\tLoss 8.5547e-01 (9.7351e-01)\tAcc@1  62.50 ( 50.15)\n",
      "[xla:5]Train:  Epoch: [4][ 90/129]\tTime  0.235 ( 0.209)\tData  0.014 ( 0.015)\tLoss 6.6016e-01 (9.8820e-01)\tAcc@1  62.50 ( 51.53)\n",
      "[xla:2]Train:  Epoch: [4][ 90/129]\tTime  0.229 ( 0.205)\tData  0.009 ( 0.014)\tLoss 7.6953e-01 (9.8334e-01)\tAcc@1  69.00 ( 52.36)\n",
      "[xla:1]Train:  Epoch: [4][120/129]\tTime  0.178 ( 0.207)\tData  0.011 ( 0.015)\tLoss 1.0156e+00 (9.4528e-01)\tAcc@1  56.25 ( 52.65)\n",
      "[xla:2]Train:  Epoch: [4][120/129]\tTime  0.200 ( 0.206)\tData  0.012 ( 0.015)\tLoss 9.0234e-01 (9.4822e-01)\tAcc@1  56.25 ( 54.82)\n",
      "[xla:3]Train:  Epoch: [4][120/129]\tTime  0.187 ( 0.211)\tData  0.011 ( 0.015)\tLoss 7.5781e-01 (9.2575e-01)\tAcc@1  62.50 ( 57.72)\n",
      "[xla:7]Train:  Epoch: [4][120/129]\tTime  0.177 ( 0.206)\tData  0.014 ( 0.015)\tLoss 8.0469e-01 (9.5196e-01)\tAcc@1  62.50 ( 54.98)\n",
      "[xla:6]Train:  Epoch: [4][120/129]\tTime  0.219 ( 0.211)\tData  0.012 ( 0.016)\tLoss 6.3672e-01 (9.5093e-01)\tAcc@1  69.00 ( 54.05)\n",
      "[xla:5]Train:  Epoch: [4][120/129]\tTime  0.213 ( 0.208)\tData  0.012 ( 0.015)\tLoss 6.7578e-01 (9.6885e-01)\tAcc@1  62.50 ( 53.13)\n",
      "[xla:0]Train:  Epoch: [4][120/129]\tTime  0.183 ( 0.208)\tData  0.011 ( 0.016)\tLoss 9.1797e-01 (9.5132e-01)\tAcc@1  56.25 ( 54.78)\n",
      "[xla:4]Train:  Epoch: [4][120/129]\tTime  0.221 ( 0.208)\tData  0.011 ( 0.015)\tLoss 9.2969e-01 (9.5070e-01)\tAcc@1  69.00 ( 54.30)\n",
      "[xla:2]Validation: [0/6]\tTime  0.144 ( 0.144)\tLoss 8.9062e-01 (8.9062e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Validation: [0/6]\tTime  0.083 ( 0.083)\tLoss 6.9922e-01 (6.9922e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Finished training epoch 4\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:4]Validation: [0/6]\tTime  0.091 ( 0.091)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.044 ( 0.044)\tLoss 7.2656e-01 (7.2656e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.0781e+00 (1.0781e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:7]Validation: [0/6]\tTime  0.107 ( 0.107)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.155 ( 0.155)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Validation: [0/6]\tTime  0.113 ( 0.113)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [5][  0/129]\tTime  0.196 ( 0.196)\tData  0.071 ( 0.071)\tLoss 9.2578e-01 (9.2578e-01)\tAcc@1  43.75 ( 43.75)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [5][  0/129]\tTime  0.239 ( 0.239)\tData  0.057 ( 0.057)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [5][  0/129]\tTime  0.268 ( 0.268)\tData  0.070 ( 0.070)\tLoss 8.5938e-01 (8.5938e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [5][  0/129]\tTime  0.299 ( 0.299)\tData  0.078 ( 0.078)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Train:  Epoch: [5][  0/129]\tTime  0.308 ( 0.308)\tData  0.087 ( 0.087)\tLoss 9.5312e-01 (9.5312e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [5][  0/129]\tTime  0.269 ( 0.269)\tData  0.035 ( 0.035)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Train:  Epoch: [5][  0/129]\tTime  0.311 ( 0.311)\tData  0.096 ( 0.096)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:3]Train:  Epoch: [5][  0/129]\tTime  0.318 ( 0.318)\tData  0.130 ( 0.130)\tLoss 6.6016e-01 (6.6016e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Train:  Epoch: [5][ 30/129]\tTime  0.193 ( 0.217)\tData  0.011 ( 0.017)\tLoss 9.8438e-01 (9.6875e-01)\tAcc@1  50.00 ( 52.24)\n",
      "[xla:7]Train:  Epoch: [5][ 30/129]\tTime  0.177 ( 0.219)\tData  0.010 ( 0.018)\tLoss 1.2109e+00 (9.9912e-01)\tAcc@1  31.25 ( 51.83)\n",
      "[xla:2]Train:  Epoch: [5][ 30/129]\tTime  0.172 ( 0.228)\tData  0.008 ( 0.018)\tLoss 9.5703e-01 (9.6787e-01)\tAcc@1  37.50 ( 51.65)\n",
      "[xla:5]Train:  Epoch: [5][ 30/129]\tTime  0.199 ( 0.221)\tData  0.012 ( 0.018)\tLoss 9.1016e-01 (9.4695e-01)\tAcc@1  69.00 ( 55.87)\n",
      "[xla:0]Train:  Epoch: [5][ 30/129]\tTime  0.188 ( 0.220)\tData  0.011 ( 0.017)\tLoss 1.0234e+00 (1.0021e+00)\tAcc@1  50.00 ( 49.40)\n",
      "[xla:3]Train:  Epoch: [5][ 30/129]\tTime  0.225 ( 0.214)\tData  0.014 ( 0.018)\tLoss 9.6094e-01 (9.5791e-01)\tAcc@1  50.00 ( 53.85)\n",
      "[xla:4]Train:  Epoch: [5][ 30/129]\tTime  0.228 ( 0.220)\tData  0.013 ( 0.018)\tLoss 1.0000e+00 (9.5615e-01)\tAcc@1  56.25 ( 55.69)\n",
      "[xla:6]Train:  Epoch: [5][ 30/129]\tTime  0.210 ( 0.218)\tData  0.009 ( 0.019)\tLoss 1.0156e+00 (9.1998e-01)\tAcc@1  37.50 ( 55.47)\n",
      "[xla:4]Train:  Epoch: [5][ 60/129]\tTime  0.187 ( 0.214)\tData  0.009 ( 0.016)\tLoss 1.0781e+00 (9.5146e-01)\tAcc@1  37.50 ( 55.04)\n",
      "[xla:3]Train:  Epoch: [5][ 60/129]\tTime  0.173 ( 0.211)\tData  0.011 ( 0.017)\tLoss 7.6953e-01 (9.2412e-01)\tAcc@1  69.00 ( 56.49)\n",
      "[xla:6]Train:  Epoch: [5][ 60/129]\tTime  0.215 ( 0.213)\tData  0.012 ( 0.017)\tLoss 1.0234e+00 (9.4083e-01)\tAcc@1  50.00 ( 53.30)\n",
      "[xla:1]Train:  Epoch: [5][ 60/129]\tTime  0.240 ( 0.213)\tData  0.012 ( 0.015)\tLoss 1.1484e+00 (9.6376e-01)\tAcc@1  31.25 ( 52.48)\n",
      "[xla:0]Train:  Epoch: [5][ 60/129]\tTime  0.202 ( 0.214)\tData  0.009 ( 0.016)\tLoss 7.6953e-01 (9.9430e-01)\tAcc@1  81.00 ( 51.74)\n",
      "[xla:5]Train:  Epoch: [5][ 60/129]\tTime  0.196 ( 0.215)\tData  0.011 ( 0.017)\tLoss 1.0234e+00 (9.7266e-01)\tAcc@1  43.75 ( 52.78)\n",
      "[xla:7]Train:  Epoch: [5][ 60/129]\tTime  0.186 ( 0.214)\tData  0.012 ( 0.016)\tLoss 7.4219e-01 (9.7643e-01)\tAcc@1  75.00 ( 53.82)\n",
      "[xla:2]Train:  Epoch: [5][ 60/129]\tTime  0.202 ( 0.219)\tData  0.011 ( 0.017)\tLoss 7.5391e-01 (9.5581e-01)\tAcc@1  69.00 ( 53.42)\n",
      "[xla:5]Train:  Epoch: [5][ 90/129]\tTime  0.176 ( 0.213)\tData  0.010 ( 0.017)\tLoss 6.7578e-01 (9.6974e-01)\tAcc@1  62.50 ( 53.25)\n",
      "[xla:1]Train:  Epoch: [5][ 90/129]\tTime  0.222 ( 0.211)\tData  0.013 ( 0.015)\tLoss 7.5000e-01 (9.4574e-01)\tAcc@1  69.00 ( 53.53)\n",
      "[xla:3]Train:  Epoch: [5][ 90/129]\tTime  0.187 ( 0.210)\tData  0.012 ( 0.017)\tLoss 7.6562e-01 (9.1359e-01)\tAcc@1  56.25 ( 56.69)\n",
      "[xla:6]Train:  Epoch: [5][ 90/129]\tTime  0.203 ( 0.212)\tData  0.010 ( 0.017)\tLoss 7.4219e-01 (9.4445e-01)\tAcc@1  62.50 ( 53.39)\n",
      "[xla:7]Train:  Epoch: [5][ 90/129]\tTime  0.233 ( 0.212)\tData  0.012 ( 0.016)\tLoss 8.1250e-01 (9.6987e-01)\tAcc@1  75.00 ( 53.74)\n",
      "[xla:4]Train:  Epoch: [5][ 90/129]\tTime  0.194 ( 0.212)\tData  0.012 ( 0.015)\tLoss 1.0234e+00 (9.4578e-01)\tAcc@1  56.25 ( 55.58)\n",
      "[xla:2]Train:  Epoch: [5][ 90/129]\tTime  0.194 ( 0.215)\tData  0.011 ( 0.016)\tLoss 6.7188e-01 (9.6137e-01)\tAcc@1  69.00 ( 52.99)\n",
      "[xla:0]Train:  Epoch: [5][ 90/129]\tTime  0.202 ( 0.212)\tData  0.016 ( 0.016)\tLoss 1.1250e+00 (9.6635e-01)\tAcc@1  31.25 ( 53.37)\n",
      "[xla:1]Train:  Epoch: [5][120/129]\tTime  0.197 ( 0.212)\tData  0.011 ( 0.015)\tLoss 9.2969e-01 (9.1939e-01)\tAcc@1  56.25 ( 55.36)\n",
      "[xla:4]Train:  Epoch: [5][120/129]\tTime  0.187 ( 0.213)\tData  0.012 ( 0.015)\tLoss 9.8828e-01 (9.1913e-01)\tAcc@1  69.00 ( 57.35)\n",
      "[xla:3]Train:  Epoch: [5][120/129]\tTime  0.217 ( 0.211)\tData  0.012 ( 0.017)\tLoss 8.0078e-01 (9.0273e-01)\tAcc@1  69.00 ( 58.11)\n",
      "[xla:0]Train:  Epoch: [5][120/129]\tTime  0.202 ( 0.213)\tData  0.011 ( 0.015)\tLoss 1.0156e+00 (9.3342e-01)\tAcc@1  43.75 ( 55.86)\n",
      "[xla:7]Train:  Epoch: [5][120/129]\tTime  0.195 ( 0.213)\tData  0.011 ( 0.016)\tLoss 8.6328e-01 (9.4928e-01)\tAcc@1  50.00 ( 55.05)\n",
      "[xla:6]Train:  Epoch: [5][120/129]\tTime  0.218 ( 0.212)\tData  0.011 ( 0.016)\tLoss 5.8594e-01 (9.1978e-01)\tAcc@1  81.00 ( 55.91)\n",
      "[xla:2]Train:  Epoch: [5][120/129]\tTime  0.213 ( 0.215)\tData  0.011 ( 0.016)\tLoss 7.5781e-01 (9.2669e-01)\tAcc@1  75.00 ( 55.42)\n",
      "[xla:5]Train:  Epoch: [5][120/129]\tTime  0.213 ( 0.213)\tData  0.012 ( 0.016)\tLoss 6.6016e-01 (9.5377e-01)\tAcc@1  75.00 ( 54.57)\n",
      "Finished training epoch 5\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/6]\tTime  0.163 ( 0.163)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.118 ( 0.118)\tLoss 7.1484e-01 (7.1484e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.129 ( 0.129)\tLoss 6.7188e-01 (6.7188e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.121 ( 0.121)\tLoss 8.2812e-01 (8.2812e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Validation: [0/6]\tTime  0.098 ( 0.098)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 1.2734e+00 (1.2734e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:6]Validation: [0/6]\tTime  0.120 ( 0.120)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.109 ( 0.109)\tLoss 9.0234e-01 (9.0234e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [6][  0/129]\tTime  0.198 ( 0.198)\tData  0.031 ( 0.031)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [6][  0/129]\tTime  0.270 ( 0.270)\tData  0.088 ( 0.088)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [6][  0/129]\tTime  0.179 ( 0.179)\tData  0.044 ( 0.044)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [6][  0/129]\tTime  0.237 ( 0.237)\tData  0.073 ( 0.073)\tLoss 6.9141e-01 (6.9141e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [6][  0/129]\tTime  0.276 ( 0.276)\tData  0.066 ( 0.066)\tLoss 1.0781e+00 (1.0781e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [6][  0/129]\tTime  0.222 ( 0.222)\tData  0.064 ( 0.064)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [6][  0/129]\tTime  0.223 ( 0.223)\tData  0.074 ( 0.074)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Train:  Epoch: [6][  0/129]\tTime  0.227 ( 0.227)\tData  0.085 ( 0.085)\tLoss 1.1172e+00 (1.1172e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [6][ 30/129]\tTime  0.186 ( 0.209)\tData  0.012 ( 0.017)\tLoss 7.2656e-01 (9.5224e-01)\tAcc@1  81.00 ( 56.27)\n",
      "[xla:6]Train:  Epoch: [6][ 30/129]\tTime  0.181 ( 0.209)\tData  0.009 ( 0.017)\tLoss 9.6094e-01 (9.0915e-01)\tAcc@1  43.75 ( 57.72)\n",
      "[xla:1]Train:  Epoch: [6][ 30/129]\tTime  0.197 ( 0.222)\tData  0.010 ( 0.018)\tLoss 8.5938e-01 (9.5111e-01)\tAcc@1  62.50 ( 52.02)\n",
      "[xla:7]Train:  Epoch: [6][ 30/129]\tTime  0.220 ( 0.211)\tData  0.012 ( 0.016)\tLoss 1.1250e+00 (9.7077e-01)\tAcc@1  56.25 ( 54.27)\n",
      "[xla:4]Train:  Epoch: [6][ 30/129]\tTime  0.211 ( 0.210)\tData  0.014 ( 0.016)\tLoss 9.4531e-01 (9.0827e-01)\tAcc@1  56.25 ( 55.48)\n",
      "[xla:0]Train:  Epoch: [6][ 30/129]\tTime  0.224 ( 0.222)\tData  0.017 ( 0.016)\tLoss 9.8828e-01 (9.5476e-01)\tAcc@1  69.00 ( 54.85)\n",
      "[xla:3]Train:  Epoch: [6][ 30/129]\tTime  0.186 ( 0.212)\tData  0.008 ( 0.015)\tLoss 8.1250e-01 (9.2528e-01)\tAcc@1  56.25 ( 56.06)\n",
      "[xla:2]Train:  Epoch: [6][ 30/129]\tTime  0.221 ( 0.218)\tData  0.017 ( 0.017)\tLoss 8.6719e-01 (9.6321e-01)\tAcc@1  56.25 ( 53.24)\n",
      "[xla:1]Train:  Epoch: [6][ 60/129]\tTime  0.191 ( 0.215)\tData  0.011 ( 0.016)\tLoss 1.0156e+00 (9.4173e-01)\tAcc@1  50.00 ( 54.64)\n",
      "[xla:5]Train:  Epoch: [6][ 60/129]\tTime  0.217 ( 0.209)\tData  0.011 ( 0.016)\tLoss 9.2969e-01 (9.5101e-01)\tAcc@1  50.00 ( 55.97)\n",
      "[xla:3]Train:  Epoch: [6][ 60/129]\tTime  0.189 ( 0.210)\tData  0.010 ( 0.015)\tLoss 7.4609e-01 (9.0093e-01)\tAcc@1  56.25 ( 57.81)\n",
      "[xla:4]Train:  Epoch: [6][ 60/129]\tTime  0.196 ( 0.209)\tData  0.010 ( 0.015)\tLoss 9.7266e-01 (9.0606e-01)\tAcc@1  56.25 ( 55.76)\n",
      "[xla:0]Train:  Epoch: [6][ 60/129]\tTime  0.221 ( 0.215)\tData  0.010 ( 0.015)\tLoss 7.7344e-01 (9.4057e-01)\tAcc@1  62.50 ( 55.23)\n",
      "[xla:6]Train:  Epoch: [6][ 60/129]\tTime  0.224 ( 0.209)\tData  0.011 ( 0.016)\tLoss 1.1641e+00 (9.2488e-01)\tAcc@1  43.75 ( 55.16)\n",
      "[xla:7]Train:  Epoch: [6][ 60/129]\tTime  0.196 ( 0.210)\tData  0.009 ( 0.016)\tLoss 6.7578e-01 (9.4025e-01)\tAcc@1  81.00 ( 55.97)\n",
      "[xla:2]Train:  Epoch: [6][ 60/129]\tTime  0.193 ( 0.213)\tData  0.009 ( 0.016)\tLoss 9.1797e-01 (9.3513e-01)\tAcc@1  43.75 ( 56.17)\n",
      "[xla:5]Train:  Epoch: [6][ 90/129]\tTime  0.174 ( 0.208)\tData  0.012 ( 0.016)\tLoss 5.7812e-01 (9.4870e-01)\tAcc@1  81.00 ( 55.79)\n",
      "[xla:3]Train:  Epoch: [6][ 90/129]\tTime  0.201 ( 0.209)\tData  0.013 ( 0.015)\tLoss 5.9375e-01 (8.8968e-01)\tAcc@1  75.00 ( 57.65)\n",
      "[xla:2]Train:  Epoch: [6][ 90/129]\tTime  0.186 ( 0.211)\tData  0.011 ( 0.016)\tLoss 6.9531e-01 (9.4329e-01)\tAcc@1  69.00 ( 55.52)\n",
      "[xla:1]Train:  Epoch: [6][ 90/129]\tTime  0.176 ( 0.213)\tData  0.009 ( 0.016)\tLoss 6.6016e-01 (9.3093e-01)\tAcc@1  81.00 ( 55.04)\n",
      "[xla:4]Train:  Epoch: [6][ 90/129]\tTime  0.210 ( 0.209)\tData  0.011 ( 0.015)\tLoss 8.9453e-01 (9.0694e-01)\tAcc@1  56.25 ( 55.38)\n",
      "[xla:7]Train:  Epoch: [6][ 90/129]\tTime  0.231 ( 0.209)\tData  0.013 ( 0.015)\tLoss 7.3438e-01 (9.3535e-01)\tAcc@1  69.00 ( 56.00)\n",
      "[xla:0]Train:  Epoch: [6][ 90/129]\tTime  0.223 ( 0.213)\tData  0.011 ( 0.015)\tLoss 1.1172e+00 (9.1930e-01)\tAcc@1  31.25 ( 56.40)\n",
      "[xla:6]Train:  Epoch: [6][ 90/129]\tTime  0.244 ( 0.209)\tData  0.012 ( 0.015)\tLoss 7.6562e-01 (9.2372e-01)\tAcc@1  62.50 ( 55.11)\n",
      "[xla:0]Train:  Epoch: [6][120/129]\tTime  0.207 ( 0.212)\tData  0.012 ( 0.016)\tLoss 8.7109e-01 (8.9256e-01)\tAcc@1  62.50 ( 58.09)\n",
      "[xla:4]Train:  Epoch: [6][120/129]\tTime  0.166 ( 0.209)\tData  0.008 ( 0.016)\tLoss 8.5938e-01 (8.8862e-01)\tAcc@1  69.00 ( 57.45)\n",
      "[xla:7]Train:  Epoch: [6][120/129]\tTime  0.188 ( 0.209)\tData  0.014 ( 0.015)\tLoss 8.8672e-01 (9.1703e-01)\tAcc@1  62.50 ( 57.52)\n",
      "[xla:6]Train:  Epoch: [6][120/129]\tTime  0.240 ( 0.209)\tData  0.012 ( 0.015)\tLoss 5.0781e-01 (8.9514e-01)\tAcc@1  87.50 ( 57.26)\n",
      "[xla:3]Train:  Epoch: [6][120/129]\tTime  0.179 ( 0.209)\tData  0.009 ( 0.015)\tLoss 6.2500e-01 (8.7842e-01)\tAcc@1  69.00 ( 58.97)\n",
      "[xla:5]Train:  Epoch: [6][120/129]\tTime  0.199 ( 0.209)\tData  0.013 ( 0.015)\tLoss 5.3125e-01 (9.2330e-01)\tAcc@1  75.00 ( 56.62)\n",
      "[xla:1]Train:  Epoch: [6][120/129]\tTime  0.205 ( 0.212)\tData  0.011 ( 0.015)\tLoss 8.5156e-01 (9.0661e-01)\tAcc@1  50.00 ( 56.76)\n",
      "[xla:2]Train:  Epoch: [6][120/129]\tTime  0.219 ( 0.211)\tData  0.012 ( 0.016)\tLoss 8.9844e-01 (9.0938e-01)\tAcc@1  62.50 ( 57.88)\n",
      "Finished training epoch 6\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:6]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Validation: [0/6]\tTime  0.081 ( 0.081)\tLoss 1.0938e+00 (1.0938e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Validation: [0/6]\tTime  0.088 ( 0.088)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 6.7188e-01 (6.7188e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.114 ( 0.114)\tLoss 6.8359e-01 (6.8359e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.115 ( 0.115)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.118 ( 0.118)\tLoss 7.9688e-01 (7.9688e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [7][  0/129]\tTime  0.203 ( 0.203)\tData  0.052 ( 0.052)\tLoss 9.5703e-01 (9.5703e-01)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:1]Train:  Epoch: [7][  0/129]\tTime  0.276 ( 0.276)\tData  0.074 ( 0.074)\tLoss 9.6875e-01 (9.6875e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [7][  0/129]\tTime  0.251 ( 0.251)\tData  0.088 ( 0.088)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [7][  0/129]\tTime  0.273 ( 0.273)\tData  0.076 ( 0.076)\tLoss 9.6875e-01 (9.6875e-01)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:3]Train:  Epoch: [7][  0/129]\tTime  0.222 ( 0.222)\tData  0.049 ( 0.049)\tLoss 6.7188e-01 (6.7188e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Train:  Epoch: [7][  0/129]\tTime  0.235 ( 0.235)\tData  0.073 ( 0.073)\tLoss 9.9219e-01 (9.9219e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [7][  0/129]\tTime  0.238 ( 0.238)\tData  0.059 ( 0.059)\tLoss 9.0625e-01 (9.0625e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [7][  0/129]\tTime  0.235 ( 0.235)\tData  0.096 ( 0.096)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [7][ 30/129]\tTime  0.168 ( 0.213)\tData  0.017 ( 0.016)\tLoss 8.9844e-01 (9.2351e-01)\tAcc@1  62.50 ( 55.70)\n",
      "[xla:4]Train:  Epoch: [7][ 30/129]\tTime  0.183 ( 0.216)\tData  0.013 ( 0.019)\tLoss 8.5938e-01 (8.9126e-01)\tAcc@1  62.50 ( 57.90)\n",
      "[xla:5]Train:  Epoch: [7][ 30/129]\tTime  0.198 ( 0.208)\tData  0.012 ( 0.018)\tLoss 7.9297e-01 (9.4481e-01)\tAcc@1  69.00 ( 53.06)\n",
      "[xla:0]Train:  Epoch: [7][ 30/129]\tTime  0.218 ( 0.210)\tData  0.011 ( 0.015)\tLoss 9.8047e-01 (9.6573e-01)\tAcc@1  56.25 ( 53.64)\n",
      "[xla:7]Train:  Epoch: [7][ 30/129]\tTime  0.217 ( 0.212)\tData  0.010 ( 0.017)\tLoss 1.1328e+00 (9.4607e-01)\tAcc@1  37.50 ( 54.27)\n",
      "[xla:6]Train:  Epoch: [7][ 30/129]\tTime  0.206 ( 0.219)\tData  0.012 ( 0.018)\tLoss 1.0078e+00 (8.8445e-01)\tAcc@1  50.00 ( 60.47)\n",
      "[xla:2]Train:  Epoch: [7][ 30/129]\tTime  0.200 ( 0.218)\tData  0.011 ( 0.017)\tLoss 8.7891e-01 (9.6837e-01)\tAcc@1  56.25 ( 53.24)\n",
      "[xla:1]Train:  Epoch: [7][ 30/129]\tTime  0.198 ( 0.218)\tData  0.015 ( 0.018)\tLoss 9.1016e-01 (9.2918e-01)\tAcc@1  50.00 ( 56.27)\n",
      "[xla:0]Train:  Epoch: [7][ 60/129]\tTime  0.188 ( 0.208)\tData  0.012 ( 0.015)\tLoss 8.2031e-01 (9.4762e-01)\tAcc@1  62.50 ( 54.94)\n",
      "[xla:5]Train:  Epoch: [7][ 60/129]\tTime  0.198 ( 0.208)\tData  0.010 ( 0.016)\tLoss 9.2188e-01 (9.4307e-01)\tAcc@1  50.00 ( 54.02)\n",
      "[xla:4]Train:  Epoch: [7][ 60/129]\tTime  0.182 ( 0.212)\tData  0.010 ( 0.016)\tLoss 1.0078e+00 (9.0004e-01)\tAcc@1  50.00 ( 57.81)\n",
      "[xla:1]Train:  Epoch: [7][ 60/129]\tTime  0.178 ( 0.213)\tData  0.010 ( 0.016)\tLoss 1.0938e+00 (9.2655e-01)\tAcc@1  43.75 ( 56.48)\n",
      "[xla:6]Train:  Epoch: [7][ 60/129]\tTime  0.196 ( 0.213)\tData  0.010 ( 0.016)\tLoss 9.6484e-01 (8.9639e-01)\tAcc@1  56.25 ( 58.92)\n",
      "[xla:7]Train:  Epoch: [7][ 60/129]\tTime  0.250 ( 0.209)\tData  0.014 ( 0.016)\tLoss 5.8594e-01 (9.2924e-01)\tAcc@1  81.00 ( 54.64)\n",
      "[xla:3]Train:  Epoch: [7][ 60/129]\tTime  0.211 ( 0.211)\tData  0.010 ( 0.016)\tLoss 7.6562e-01 (8.9767e-01)\tAcc@1  75.00 ( 57.00)\n",
      "[xla:2]Train:  Epoch: [7][ 60/129]\tTime  0.201 ( 0.213)\tData  0.011 ( 0.016)\tLoss 9.2578e-01 (9.3955e-01)\tAcc@1  62.50 ( 55.57)\n",
      "[xla:5]Train:  Epoch: [7][ 90/129]\tTime  0.172 ( 0.208)\tData  0.010 ( 0.016)\tLoss 5.9375e-01 (9.3832e-01)\tAcc@1  75.00 ( 54.69)\n",
      "[xla:4]Train:  Epoch: [7][ 90/129]\tTime  0.211 ( 0.211)\tData  0.015 ( 0.016)\tLoss 9.7266e-01 (9.0406e-01)\tAcc@1  50.00 ( 57.91)\n",
      "[xla:3]Train:  Epoch: [7][ 90/129]\tTime  0.188 ( 0.210)\tData  0.012 ( 0.015)\tLoss 6.3281e-01 (8.8273e-01)\tAcc@1  69.00 ( 58.01)\n",
      "[xla:6]Train:  Epoch: [7][ 90/129]\tTime  0.189 ( 0.212)\tData  0.012 ( 0.016)\tLoss 6.0938e-01 (8.8990e-01)\tAcc@1  81.00 ( 58.47)\n",
      "[xla:1]Train:  Epoch: [7][ 90/129]\tTime  0.193 ( 0.212)\tData  0.011 ( 0.015)\tLoss 7.5391e-01 (9.1033e-01)\tAcc@1  69.00 ( 58.55)\n",
      "[xla:7]Train:  Epoch: [7][ 90/129]\tTime  0.221 ( 0.209)\tData  0.011 ( 0.016)\tLoss 7.1484e-01 (9.2836e-01)\tAcc@1  69.00 ( 55.45)\n",
      "[xla:0]Train:  Epoch: [7][ 90/129]\tTime  0.200 ( 0.209)\tData  0.015 ( 0.015)\tLoss 1.2031e+00 (9.2677e-01)\tAcc@1  37.50 ( 55.79)\n",
      "[xla:2]Train:  Epoch: [7][ 90/129]\tTime  0.239 ( 0.212)\tData  0.012 ( 0.016)\tLoss 5.6250e-01 (9.3235e-01)\tAcc@1  81.00 ( 56.20)\n",
      "[xla:0]Train:  Epoch: [7][120/129]\tTime  0.186 ( 0.211)\tData  0.012 ( 0.015)\tLoss 9.1016e-01 (8.9692e-01)\tAcc@1  50.00 ( 58.24)\n",
      "[xla:7]Train:  Epoch: [7][120/129]\tTime  0.169 ( 0.211)\tData  0.012 ( 0.015)\tLoss 8.2031e-01 (9.0396e-01)\tAcc@1  69.00 ( 57.52)\n",
      "[xla:2]Train:  Epoch: [7][120/129]\tTime  0.193 ( 0.213)\tData  0.013 ( 0.017)\tLoss 7.7734e-01 (8.8838e-01)\tAcc@1  69.00 ( 59.02)\n",
      "[xla:6]Train:  Epoch: [7][120/129]\tTime  0.177 ( 0.213)\tData  0.012 ( 0.016)\tLoss 6.0547e-01 (8.6979e-01)\tAcc@1  69.00 ( 60.16)\n",
      "[xla:4]Train:  Epoch: [7][120/129]\tTime  0.208 ( 0.213)\tData  0.011 ( 0.016)\tLoss 9.6875e-01 (8.8394e-01)\tAcc@1  69.00 ( 59.53)\n",
      "[xla:1]Train:  Epoch: [7][120/129]\tTime  0.202 ( 0.213)\tData  0.012 ( 0.015)\tLoss 8.8672e-01 (8.8391e-01)\tAcc@1  62.50 ( 59.86)\n",
      "[xla:3]Train:  Epoch: [7][120/129]\tTime  0.190 ( 0.212)\tData  0.014 ( 0.015)\tLoss 6.3281e-01 (8.6745e-01)\tAcc@1  69.00 ( 59.09)\n",
      "[xla:5]Train:  Epoch: [7][120/129]\tTime  0.222 ( 0.211)\tData  0.015 ( 0.015)\tLoss 5.9375e-01 (9.2052e-01)\tAcc@1  69.00 ( 55.70)\n",
      "[xla:5]Validation: [0/6]\tTime  0.091 ( 0.091)\tLoss 6.5625e-01 (6.5625e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Finished training epoch 7\n",
      "[xla:2]Validation: [0/6]\tTime  0.085 ( 0.085)\tLoss 8.7109e-01 (8.7109e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:6]Validation: [0/6]\tTime  0.078 ( 0.078)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Validation: [0/6]\tTime  0.125 ( 0.125)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 7.7734e-01 (7.7734e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 6.5234e-01 (6.5234e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.150 ( 0.150)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.114 ( 0.114)\tLoss 1.1562e+00 (1.1562e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [8][  0/129]\tTime  0.250 ( 0.250)\tData  0.083 ( 0.083)\tLoss 9.3750e-01 (9.3750e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [8][  0/129]\tTime  0.159 ( 0.159)\tData  0.028 ( 0.028)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Train:  Epoch: [8][  0/129]\tTime  0.296 ( 0.296)\tData  0.054 ( 0.054)\tLoss 1.0547e+00 (1.0547e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:6]Train:  Epoch: [8][  0/129]\tTime  0.333 ( 0.333)\tData  0.114 ( 0.114)\tLoss 1.1172e+00 (1.1172e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [8][  0/129]\tTime  0.248 ( 0.248)\tData  0.096 ( 0.096)\tLoss 1.1875e+00 (1.1875e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Train:  Epoch: [8][  0/129]\tTime  0.361 ( 0.361)\tData  0.109 ( 0.109)\tLoss 8.8281e-01 (8.8281e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [8][  0/129]\tTime  0.317 ( 0.317)\tData  0.122 ( 0.122)\tLoss 5.5859e-01 (5.5859e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:0]Train:  Epoch: [8][  0/129]\tTime  0.229 ( 0.229)\tData  0.052 ( 0.052)\tLoss 8.2812e-01 (8.2812e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Train:  Epoch: [8][ 30/129]\tTime  0.164 ( 0.223)\tData  0.011 ( 0.018)\tLoss 8.8281e-01 (8.6870e-01)\tAcc@1  43.75 ( 58.69)\n",
      "[xla:7]Train:  Epoch: [8][ 30/129]\tTime  0.182 ( 0.217)\tData  0.012 ( 0.020)\tLoss 1.1172e+00 (9.5073e-01)\tAcc@1  50.00 ( 55.26)\n",
      "[xla:1]Train:  Epoch: [8][ 30/129]\tTime  0.192 ( 0.222)\tData  0.010 ( 0.017)\tLoss 8.7109e-01 (9.1973e-01)\tAcc@1  69.00 ( 57.30)\n",
      "[xla:2]Train:  Epoch: [8][ 30/129]\tTime  0.209 ( 0.229)\tData  0.011 ( 0.018)\tLoss 8.4766e-01 (9.3939e-01)\tAcc@1  62.50 ( 54.87)\n",
      "[xla:5]Train:  Epoch: [8][ 30/129]\tTime  0.246 ( 0.225)\tData  0.012 ( 0.017)\tLoss 6.9922e-01 (9.0549e-01)\tAcc@1  81.00 ( 61.15)\n",
      "[xla:4]Train:  Epoch: [8][ 30/129]\tTime  0.200 ( 0.220)\tData  0.012 ( 0.018)\tLoss 7.2656e-01 (8.8949e-01)\tAcc@1  75.00 ( 59.70)\n",
      "[xla:0]Train:  Epoch: [8][ 30/129]\tTime  0.236 ( 0.213)\tData  0.019 ( 0.015)\tLoss 9.6875e-01 (9.5829e-01)\tAcc@1  50.00 ( 51.85)\n",
      "[xla:3]Train:  Epoch: [8][ 30/129]\tTime  0.203 ( 0.218)\tData  0.011 ( 0.020)\tLoss 9.1406e-01 (9.2465e-01)\tAcc@1  56.25 ( 56.44)\n",
      "[xla:3]Train:  Epoch: [8][ 60/129]\tTime  0.173 ( 0.214)\tData  0.010 ( 0.018)\tLoss 7.3438e-01 (8.9536e-01)\tAcc@1  75.00 ( 57.60)\n",
      "[xla:5]Train:  Epoch: [8][ 60/129]\tTime  0.180 ( 0.218)\tData  0.016 ( 0.017)\tLoss 1.0312e+00 (9.1829e-01)\tAcc@1  43.75 ( 59.57)\n",
      "[xla:6]Train:  Epoch: [8][ 60/129]\tTime  0.236 ( 0.217)\tData  0.024 ( 0.017)\tLoss 1.0703e+00 (8.8224e-01)\tAcc@1  43.75 ( 58.94)\n",
      "[xla:4]Train:  Epoch: [8][ 60/129]\tTime  0.177 ( 0.216)\tData  0.010 ( 0.016)\tLoss 1.0859e+00 (8.8819e-01)\tAcc@1  37.50 ( 58.83)\n",
      "[xla:0]Train:  Epoch: [8][ 60/129]\tTime  0.193 ( 0.212)\tData  0.018 ( 0.016)\tLoss 8.4766e-01 (9.3154e-01)\tAcc@1  56.25 ( 54.64)\n",
      "[xla:2]Train:  Epoch: [8][ 60/129]\tTime  0.206 ( 0.220)\tData  0.011 ( 0.017)\tLoss 8.3594e-01 (9.2360e-01)\tAcc@1  62.50 ( 54.64)\n",
      "[xla:7]Train:  Epoch: [8][ 60/129]\tTime  0.200 ( 0.214)\tData  0.011 ( 0.018)\tLoss 6.1719e-01 (9.2629e-01)\tAcc@1  81.00 ( 57.18)\n",
      "[xla:1]Train:  Epoch: [8][ 60/129]\tTime  0.206 ( 0.217)\tData  0.011 ( 0.016)\tLoss 1.0078e+00 (9.1861e-01)\tAcc@1  43.75 ( 57.01)\n",
      "[xla:4]Train:  Epoch: [8][ 90/129]\tTime  0.163 ( 0.213)\tData  0.010 ( 0.015)\tLoss 8.9062e-01 (8.8741e-01)\tAcc@1  43.75 ( 58.40)\n",
      "[xla:0]Train:  Epoch: [8][ 90/129]\tTime  0.170 ( 0.211)\tData  0.014 ( 0.015)\tLoss 1.3359e+00 (9.0870e-01)\tAcc@1  31.25 ( 56.54)\n",
      "[xla:1]Train:  Epoch: [8][ 90/129]\tTime  0.183 ( 0.214)\tData  0.020 ( 0.016)\tLoss 6.9922e-01 (8.9449e-01)\tAcc@1  69.00 ( 58.14)\n",
      "[xla:7]Train:  Epoch: [8][ 90/129]\tTime  0.216 ( 0.212)\tData  0.013 ( 0.017)\tLoss 7.5391e-01 (9.2291e-01)\tAcc@1  56.25 ( 56.33)\n",
      "[xla:2]Train:  Epoch: [8][ 90/129]\tTime  0.203 ( 0.217)\tData  0.012 ( 0.016)\tLoss 6.4062e-01 (9.2192e-01)\tAcc@1  75.00 ( 55.79)\n",
      "[xla:3]Train:  Epoch: [8][ 90/129]\tTime  0.238 ( 0.213)\tData  0.012 ( 0.017)\tLoss 6.0938e-01 (8.7384e-01)\tAcc@1  69.00 ( 58.82)\n",
      "[xla:6]Train:  Epoch: [8][ 90/129]\tTime  0.204 ( 0.215)\tData  0.012 ( 0.016)\tLoss 6.3281e-01 (8.8530e-01)\tAcc@1  81.00 ( 59.09)\n",
      "[xla:5]Train:  Epoch: [8][ 90/129]\tTime  0.196 ( 0.215)\tData  0.011 ( 0.015)\tLoss 7.6172e-01 (9.1303e-01)\tAcc@1  56.25 ( 59.24)\n",
      "[xla:1]Train:  Epoch: [8][120/129]\tTime  0.164 ( 0.211)\tData  0.011 ( 0.016)\tLoss 9.3750e-01 (8.7261e-01)\tAcc@1  50.00 ( 59.12)\n",
      "[xla:0]Train:  Epoch: [8][120/129]\tTime  0.180 ( 0.209)\tData  0.020 ( 0.014)\tLoss 7.5781e-01 (8.7902e-01)\tAcc@1  62.50 ( 58.33)\n",
      "[xla:6]Train:  Epoch: [8][120/129]\tTime  0.178 ( 0.212)\tData  0.011 ( 0.016)\tLoss 6.8750e-01 (8.6754e-01)\tAcc@1  69.00 ( 60.46)\n",
      "[xla:2]Train:  Epoch: [8][120/129]\tTime  0.192 ( 0.213)\tData  0.010 ( 0.016)\tLoss 8.9844e-01 (8.8159e-01)\tAcc@1  50.00 ( 58.65)\n",
      "[xla:4]Train:  Epoch: [8][120/129]\tTime  0.194 ( 0.211)\tData  0.015 ( 0.015)\tLoss 8.2812e-01 (8.6099e-01)\tAcc@1  75.00 ( 60.35)\n",
      "[xla:3]Train:  Epoch: [8][120/129]\tTime  0.224 ( 0.210)\tData  0.010 ( 0.016)\tLoss 6.4062e-01 (8.5908e-01)\tAcc@1  56.25 ( 59.74)\n",
      "[xla:5]Train:  Epoch: [8][120/129]\tTime  0.236 ( 0.212)\tData  0.014 ( 0.016)\tLoss 5.2734e-01 (8.9553e-01)\tAcc@1  75.00 ( 59.84)\n",
      "[xla:7]Train:  Epoch: [8][120/129]\tTime  0.216 ( 0.210)\tData  0.013 ( 0.017)\tLoss 7.3828e-01 (9.0312e-01)\tAcc@1  69.00 ( 57.98)\n",
      "Finished training epoch 8\n",
      "[xla:1]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 1.1562e+00 (1.1562e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:6]Validation: [0/6]\tTime  0.081 ( 0.081)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:3]Validation: [0/6]\tTime  0.086 ( 0.086)\tLoss 7.5391e-01 (7.5391e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 8.6328e-01 (8.6328e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Validation: [0/6]\tTime  0.114 ( 0.114)\tLoss 1.0781e+00 (1.0781e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.069 ( 0.069)\tLoss 6.4062e-01 (6.4062e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 6.6797e-01 (6.6797e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Train:  Epoch: [9][  0/129]\tTime  0.179 ( 0.179)\tData  0.046 ( 0.046)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [9][  0/129]\tTime  0.169 ( 0.169)\tData  0.040 ( 0.040)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:5]Train:  Epoch: [9][  0/129]\tTime  0.203 ( 0.203)\tData  0.047 ( 0.047)\tLoss 1.1016e+00 (1.1016e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [9][  0/129]\tTime  0.251 ( 0.251)\tData  0.078 ( 0.078)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:7]Train:  Epoch: [9][  0/129]\tTime  0.229 ( 0.229)\tData  0.043 ( 0.043)\tLoss 9.5703e-01 (9.5703e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [9][  0/129]\tTime  0.173 ( 0.173)\tData  0.056 ( 0.056)\tLoss 8.3203e-01 (8.3203e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Train:  Epoch: [9][  0/129]\tTime  0.171 ( 0.171)\tData  0.025 ( 0.025)\tLoss 8.3203e-01 (8.3203e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:4]Train:  Epoch: [9][  0/129]\tTime  0.176 ( 0.176)\tData  0.034 ( 0.034)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [9][ 30/129]\tTime  0.153 ( 0.209)\tData  0.011 ( 0.018)\tLoss 8.3984e-01 (9.3700e-01)\tAcc@1  62.50 ( 54.25)\n",
      "[xla:7]Train:  Epoch: [9][ 30/129]\tTime  0.216 ( 0.213)\tData  0.024 ( 0.016)\tLoss 1.1562e+00 (9.2566e-01)\tAcc@1  25.00 ( 54.65)\n",
      "[xla:4]Train:  Epoch: [9][ 30/129]\tTime  0.178 ( 0.204)\tData  0.012 ( 0.015)\tLoss 8.6328e-01 (8.9579e-01)\tAcc@1  62.50 ( 57.31)\n",
      "[xla:3]Train:  Epoch: [9][ 30/129]\tTime  0.180 ( 0.216)\tData  0.011 ( 0.017)\tLoss 9.6875e-01 (8.9806e-01)\tAcc@1  50.00 ( 57.49)\n",
      "[xla:6]Train:  Epoch: [9][ 30/129]\tTime  0.193 ( 0.222)\tData  0.012 ( 0.017)\tLoss 9.9609e-01 (8.5938e-01)\tAcc@1  31.25 ( 58.47)\n",
      "[xla:5]Train:  Epoch: [9][ 30/129]\tTime  0.208 ( 0.215)\tData  0.009 ( 0.016)\tLoss 7.8516e-01 (9.1116e-01)\tAcc@1  75.00 ( 60.71)\n",
      "[xla:0]Train:  Epoch: [9][ 30/129]\tTime  0.224 ( 0.207)\tData  0.009 ( 0.016)\tLoss 9.6875e-01 (9.5766e-01)\tAcc@1  69.00 ( 53.86)\n",
      "[xla:1]Train:  Epoch: [9][ 30/129]\tTime  0.243 ( 0.217)\tData  0.011 ( 0.017)\tLoss 8.6719e-01 (8.9680e-01)\tAcc@1  62.50 ( 58.48)\n",
      "[xla:5]Train:  Epoch: [9][ 60/129]\tTime  0.189 ( 0.213)\tData  0.022 ( 0.016)\tLoss 9.3750e-01 (9.1951e-01)\tAcc@1  62.50 ( 58.43)\n",
      "[xla:0]Train:  Epoch: [9][ 60/129]\tTime  0.222 ( 0.209)\tData  0.021 ( 0.016)\tLoss 7.4219e-01 (9.1422e-01)\tAcc@1  56.25 ( 56.48)\n",
      "[xla:1]Train:  Epoch: [9][ 60/129]\tTime  0.223 ( 0.214)\tData  0.020 ( 0.016)\tLoss 9.3359e-01 (9.0644e-01)\tAcc@1  50.00 ( 58.22)\n",
      "[xla:2]Train:  Epoch: [9][ 60/129]\tTime  0.224 ( 0.212)\tData  0.022 ( 0.017)\tLoss 8.9844e-01 (8.9191e-01)\tAcc@1  50.00 ( 59.04)\n",
      "[xla:6]Train:  Epoch: [9][ 60/129]\tTime  0.218 ( 0.217)\tData  0.019 ( 0.016)\tLoss 9.9609e-01 (8.8461e-01)\tAcc@1  50.00 ( 57.89)\n",
      "[xla:4]Train:  Epoch: [9][ 60/129]\tTime  0.243 ( 0.209)\tData  0.012 ( 0.016)\tLoss 8.7891e-01 (8.8595e-01)\tAcc@1  56.25 ( 57.82)\n",
      "[xla:7]Train:  Epoch: [9][ 60/129]\tTime  0.196 ( 0.213)\tData  0.008 ( 0.016)\tLoss 5.7031e-01 (9.0029e-01)\tAcc@1  81.00 ( 57.60)\n",
      "[xla:3]Train:  Epoch: [9][ 60/129]\tTime  0.270 ( 0.215)\tData  0.012 ( 0.016)\tLoss 7.0703e-01 (8.6764e-01)\tAcc@1  69.00 ( 59.68)\n",
      "[xla:4]Train:  Epoch: [9][ 90/129]\tTime  0.182 ( 0.208)\tData  0.011 ( 0.015)\tLoss 9.8047e-01 (8.7745e-01)\tAcc@1  43.75 ( 59.30)\n",
      "[xla:2]Train:  Epoch: [9][ 90/129]\tTime  0.209 ( 0.210)\tData  0.013 ( 0.016)\tLoss 6.0938e-01 (9.0033e-01)\tAcc@1  75.00 ( 58.82)\n",
      "[xla:6]Train:  Epoch: [9][ 90/129]\tTime  0.171 ( 0.214)\tData  0.009 ( 0.015)\tLoss 6.0938e-01 (8.7607e-01)\tAcc@1  75.00 ( 58.81)\n",
      "[xla:1]Train:  Epoch: [9][ 90/129]\tTime  0.200 ( 0.212)\tData  0.011 ( 0.016)\tLoss 6.1719e-01 (8.8277e-01)\tAcc@1  69.00 ( 59.37)\n",
      "[xla:5]Train:  Epoch: [9][ 90/129]\tTime  0.214 ( 0.212)\tData  0.013 ( 0.016)\tLoss 7.6172e-01 (9.1556e-01)\tAcc@1  69.00 ( 58.90)\n",
      "[xla:0]Train:  Epoch: [9][ 90/129]\tTime  0.214 ( 0.209)\tData  0.011 ( 0.015)\tLoss 1.2500e+00 (8.8906e-01)\tAcc@1  37.50 ( 57.93)\n",
      "[xla:7]Train:  Epoch: [9][ 90/129]\tTime  0.216 ( 0.212)\tData  0.009 ( 0.015)\tLoss 6.5625e-01 (8.9208e-01)\tAcc@1  75.00 ( 58.18)\n",
      "[xla:3]Train:  Epoch: [9][ 90/129]\tTime  0.205 ( 0.212)\tData  0.012 ( 0.016)\tLoss 6.5625e-01 (8.6354e-01)\tAcc@1  81.00 ( 59.58)\n",
      "[xla:2]Train:  Epoch: [9][120/129]\tTime  0.191 ( 0.209)\tData  0.011 ( 0.015)\tLoss 8.4766e-01 (8.6948e-01)\tAcc@1  62.50 ( 60.71)\n",
      "[xla:0]Train:  Epoch: [9][120/129]\tTime  0.187 ( 0.208)\tData  0.012 ( 0.015)\tLoss 8.4375e-01 (8.5979e-01)\tAcc@1  62.50 ( 59.90)\n",
      "[xla:5]Train:  Epoch: [9][120/129]\tTime  0.189 ( 0.210)\tData  0.012 ( 0.015)\tLoss 4.6484e-01 (8.9650e-01)\tAcc@1  81.00 ( 59.54)\n",
      "[xla:1]Train:  Epoch: [9][120/129]\tTime  0.175 ( 0.210)\tData  0.009 ( 0.015)\tLoss 8.9844e-01 (8.5986e-01)\tAcc@1  50.00 ( 60.56)\n",
      "[xla:6]Train:  Epoch: [9][120/129]\tTime  0.201 ( 0.212)\tData  0.011 ( 0.015)\tLoss 5.0391e-01 (8.5983e-01)\tAcc@1  81.00 ( 60.30)\n",
      "[xla:4]Train:  Epoch: [9][120/129]\tTime  0.229 ( 0.207)\tData  0.011 ( 0.015)\tLoss 9.1406e-01 (8.5400e-01)\tAcc@1  69.00 ( 61.13)\n",
      "[xla:3]Train:  Epoch: [9][120/129]\tTime  0.195 ( 0.210)\tData  0.011 ( 0.016)\tLoss 6.1719e-01 (8.5056e-01)\tAcc@1  75.00 ( 60.31)\n",
      "[xla:7]Train:  Epoch: [9][120/129]\tTime  0.205 ( 0.210)\tData  0.010 ( 0.015)\tLoss 8.5156e-01 (8.7831e-01)\tAcc@1  56.25 ( 59.31)\n",
      "[xla:6]Validation: [0/6]\tTime  0.079 ( 0.079)\tLoss 9.9609e-01 (9.9609e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 1.1875e+00 (1.1875e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.096 ( 0.096)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:3]Validation: [0/6]\tTime  0.127 ( 0.127)\tLoss 7.5000e-01 (7.5000e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 9\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:5]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 6.3672e-01 (6.3672e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.121 ( 0.121)\tLoss 8.5547e-01 (8.5547e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Validation: [0/6]\tTime  0.123 ( 0.123)\tLoss 6.2500e-01 (6.2500e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.132 ( 0.132)\tLoss 1.2500e+00 (1.2500e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [10][  0/129]\tTime  0.170 ( 0.170)\tData  0.037 ( 0.037)\tLoss 8.1250e-01 (8.1250e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [10][  0/129]\tTime  0.202 ( 0.202)\tData  0.057 ( 0.057)\tLoss 1.1406e+00 (1.1406e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [10][  0/129]\tTime  0.239 ( 0.239)\tData  0.095 ( 0.095)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:3]Train:  Epoch: [10][  0/129]\tTime  0.204 ( 0.204)\tData  0.065 ( 0.065)\tLoss 7.0703e-01 (7.0703e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Train:  Epoch: [10][  0/129]\tTime  0.219 ( 0.219)\tData  0.071 ( 0.071)\tLoss 9.6484e-01 (9.6484e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Train:  Epoch: [10][  0/129]\tTime  0.204 ( 0.204)\tData  0.030 ( 0.030)\tLoss 7.8125e-01 (7.8125e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [10][  0/129]\tTime  0.213 ( 0.213)\tData  0.043 ( 0.043)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [10][  0/129]\tTime  0.233 ( 0.233)\tData  0.076 ( 0.076)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [10][ 30/129]\tTime  0.189 ( 0.223)\tData  0.018 ( 0.017)\tLoss 8.7109e-01 (8.9869e-01)\tAcc@1  50.00 ( 56.65)\n",
      "[xla:6]Train:  Epoch: [10][ 30/129]\tTime  0.177 ( 0.232)\tData  0.010 ( 0.017)\tLoss 9.1016e-01 (8.6190e-01)\tAcc@1  50.00 ( 59.51)\n",
      "[xla:4]Train:  Epoch: [10][ 30/129]\tTime  0.212 ( 0.216)\tData  0.018 ( 0.017)\tLoss 7.3438e-01 (8.4803e-01)\tAcc@1  69.00 ( 59.14)\n",
      "[xla:7]Train:  Epoch: [10][ 30/129]\tTime  0.183 ( 0.227)\tData  0.008 ( 0.018)\tLoss 1.1094e+00 (9.1948e-01)\tAcc@1  43.75 ( 57.29)\n",
      "[xla:1]Train:  Epoch: [10][ 30/129]\tTime  0.215 ( 0.233)\tData  0.013 ( 0.017)\tLoss 8.3594e-01 (8.8407e-01)\tAcc@1  62.50 ( 59.31)\n",
      "[xla:2]Train:  Epoch: [10][ 30/129]\tTime  0.230 ( 0.220)\tData  0.011 ( 0.019)\tLoss 9.1797e-01 (9.3914e-01)\tAcc@1  56.25 ( 54.47)\n",
      "[xla:5]Train:  Epoch: [10][ 30/129]\tTime  0.224 ( 0.218)\tData  0.011 ( 0.019)\tLoss 8.7109e-01 (8.9730e-01)\tAcc@1  56.25 ( 57.27)\n",
      "[xla:0]Train:  Epoch: [10][ 30/129]\tTime  0.209 ( 0.218)\tData  0.009 ( 0.014)\tLoss 9.8047e-01 (9.1683e-01)\tAcc@1  56.25 ( 55.27)\n",
      "[xla:2]Train:  Epoch: [10][ 60/129]\tTime  0.173 ( 0.214)\tData  0.011 ( 0.017)\tLoss 9.9219e-01 (8.9773e-01)\tAcc@1  56.25 ( 57.49)\n",
      "[xla:3]Train:  Epoch: [10][ 60/129]\tTime  0.203 ( 0.216)\tData  0.010 ( 0.016)\tLoss 6.5625e-01 (8.5496e-01)\tAcc@1  69.00 ( 61.21)\n",
      "[xla:7]Train:  Epoch: [10][ 60/129]\tTime  0.231 ( 0.218)\tData  0.012 ( 0.016)\tLoss 5.0000e-01 (8.8230e-01)\tAcc@1  87.50 ( 59.65)\n",
      "[xla:5]Train:  Epoch: [10][ 60/129]\tTime  0.190 ( 0.213)\tData  0.012 ( 0.016)\tLoss 1.0078e+00 (9.0535e-01)\tAcc@1  56.25 ( 57.71)\n",
      "[xla:0]Train:  Epoch: [10][ 60/129]\tTime  0.185 ( 0.213)\tData  0.012 ( 0.016)\tLoss 7.2656e-01 (8.9472e-01)\tAcc@1  62.50 ( 57.81)\n",
      "[xla:4]Train:  Epoch: [10][ 60/129]\tTime  0.201 ( 0.212)\tData  0.013 ( 0.016)\tLoss 9.1797e-01 (8.4477e-01)\tAcc@1  43.75 ( 61.20)\n",
      "[xla:6]Train:  Epoch: [10][ 60/129]\tTime  0.207 ( 0.221)\tData  0.010 ( 0.015)\tLoss 1.0078e+00 (8.6399e-01)\tAcc@1  50.00 ( 59.98)\n",
      "[xla:1]Train:  Epoch: [10][ 60/129]\tTime  0.209 ( 0.221)\tData  0.012 ( 0.017)\tLoss 1.0000e+00 (8.9844e-01)\tAcc@1  62.50 ( 58.22)\n",
      "[xla:7]Train:  Epoch: [10][ 90/129]\tTime  0.180 ( 0.216)\tData  0.013 ( 0.015)\tLoss 6.7578e-01 (8.7204e-01)\tAcc@1  69.00 ( 59.85)\n",
      "[xla:5]Train:  Epoch: [10][ 90/129]\tTime  0.178 ( 0.213)\tData  0.013 ( 0.016)\tLoss 7.0703e-01 (8.9522e-01)\tAcc@1  75.00 ( 58.27)\n",
      "[xla:6]Train:  Epoch: [10][ 90/129]\tTime  0.178 ( 0.218)\tData  0.008 ( 0.015)\tLoss 6.2500e-01 (8.6526e-01)\tAcc@1  75.00 ( 59.58)\n",
      "[xla:3]Train:  Epoch: [10][ 90/129]\tTime  0.204 ( 0.216)\tData  0.011 ( 0.016)\tLoss 5.9766e-01 (8.4083e-01)\tAcc@1  69.00 ( 61.72)\n",
      "[xla:2]Train:  Epoch: [10][ 90/129]\tTime  0.207 ( 0.214)\tData  0.017 ( 0.016)\tLoss 6.5625e-01 (8.9736e-01)\tAcc@1  75.00 ( 58.14)\n",
      "[xla:4]Train:  Epoch: [10][ 90/129]\tTime  0.230 ( 0.213)\tData  0.010 ( 0.016)\tLoss 9.1797e-01 (8.5285e-01)\tAcc@1  43.75 ( 60.68)\n",
      "[xla:0]Train:  Epoch: [10][ 90/129]\tTime  0.218 ( 0.213)\tData  0.024 ( 0.016)\tLoss 1.1250e+00 (8.7676e-01)\tAcc@1  37.50 ( 59.70)\n",
      "[xla:1]Train:  Epoch: [10][ 90/129]\tTime  0.233 ( 0.219)\tData  0.015 ( 0.016)\tLoss 6.6406e-01 (8.8152e-01)\tAcc@1  69.00 ( 59.10)\n",
      "[xla:0]Train:  Epoch: [10][120/129]\tTime  0.185 ( 0.212)\tData  0.012 ( 0.015)\tLoss 7.0703e-01 (8.4727e-01)\tAcc@1  75.00 ( 61.23)\n",
      "[xla:4]Train:  Epoch: [10][120/129]\tTime  0.192 ( 0.212)\tData  0.010 ( 0.015)\tLoss 7.5000e-01 (8.3636e-01)\tAcc@1  75.00 ( 62.32)\n",
      "[xla:3]Train:  Epoch: [10][120/129]\tTime  0.177 ( 0.214)\tData  0.009 ( 0.015)\tLoss 5.9766e-01 (8.2922e-01)\tAcc@1  75.00 ( 62.86)\n",
      "[xla:2]Train:  Epoch: [10][120/129]\tTime  0.183 ( 0.213)\tData  0.011 ( 0.016)\tLoss 7.5781e-01 (8.5926e-01)\tAcc@1  69.00 ( 61.03)\n",
      "[xla:6]Train:  Epoch: [10][120/129]\tTime  0.200 ( 0.216)\tData  0.012 ( 0.015)\tLoss 6.9141e-01 (8.4704e-01)\tAcc@1  62.50 ( 60.77)\n",
      "[xla:1]Train:  Epoch: [10][120/129]\tTime  0.231 ( 0.216)\tData  0.014 ( 0.016)\tLoss 7.2656e-01 (8.5552e-01)\tAcc@1  62.50 ( 60.76)\n",
      "[xla:7]Train:  Epoch: [10][120/129]\tTime  0.212 ( 0.214)\tData  0.012 ( 0.015)\tLoss 7.9297e-01 (8.5266e-01)\tAcc@1  62.50 ( 61.30)\n",
      "[xla:5]Train:  Epoch: [10][120/129]\tTime  0.188 ( 0.212)\tData  0.010 ( 0.016)\tLoss 3.3398e-01 (8.7931e-01)\tAcc@1 100.00 ( 59.80)\n",
      "[xla:5]Validation: [0/6]\tTime  0.068 ( 0.068)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Finished training epoch 10\n",
      "[xla:7]Validation: [0/6]\tTime  0.116 ( 0.116)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:6]Validation: [0/6]\tTime  0.084 ( 0.084)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.159 ( 0.159)\tLoss 8.6328e-01 (8.6328e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Validation: [0/6]\tTime  0.116 ( 0.116)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.127 ( 0.127)\tLoss 1.1953e+00 (1.1953e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.051 ( 0.051)\tLoss 6.2500e-01 (6.2500e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.125 ( 0.125)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [11][  0/129]\tTime  0.208 ( 0.208)\tData  0.070 ( 0.070)\tLoss 9.3750e-01 (9.3750e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [11][  0/129]\tTime  0.173 ( 0.173)\tData  0.062 ( 0.062)\tLoss 9.5312e-01 (9.5312e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [11][  0/129]\tTime  0.227 ( 0.227)\tData  0.076 ( 0.076)\tLoss 9.4531e-01 (9.4531e-01)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [11][  0/129]\tTime  0.187 ( 0.187)\tData  0.032 ( 0.032)\tLoss 8.9453e-01 (8.9453e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Train:  Epoch: [11][  0/129]\tTime  0.252 ( 0.252)\tData  0.074 ( 0.074)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [11][  0/129]\tTime  0.239 ( 0.239)\tData  0.114 ( 0.114)\tLoss 5.5859e-01 (5.5859e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Train:  Epoch: [11][  0/129]\tTime  0.266 ( 0.266)\tData  0.095 ( 0.095)\tLoss 1.0547e+00 (1.0547e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Train:  Epoch: [11][  0/129]\tTime  0.209 ( 0.209)\tData  0.075 ( 0.075)\tLoss 7.6172e-01 (7.6172e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [11][ 30/129]\tTime  0.161 ( 0.204)\tData  0.013 ( 0.016)\tLoss 1.0000e+00 (9.2150e-01)\tAcc@1  50.00 ( 58.91)\n",
      "[xla:4]Train:  Epoch: [11][ 30/129]\tTime  0.164 ( 0.211)\tData  0.012 ( 0.016)\tLoss 7.7344e-01 (8.8521e-01)\tAcc@1  75.00 ( 61.10)\n",
      "[xla:6]Train:  Epoch: [11][ 30/129]\tTime  0.186 ( 0.211)\tData  0.010 ( 0.018)\tLoss 8.3984e-01 (8.2976e-01)\tAcc@1  50.00 ( 62.32)\n",
      "[xla:5]Train:  Epoch: [11][ 30/129]\tTime  0.209 ( 0.223)\tData  0.010 ( 0.017)\tLoss 7.6172e-01 (8.7361e-01)\tAcc@1  75.00 ( 60.69)\n",
      "[xla:3]Train:  Epoch: [11][ 30/129]\tTime  0.189 ( 0.211)\tData  0.015 ( 0.017)\tLoss 8.7109e-01 (9.0990e-01)\tAcc@1  56.25 ( 57.31)\n",
      "[xla:7]Train:  Epoch: [11][ 30/129]\tTime  0.205 ( 0.214)\tData  0.024 ( 0.021)\tLoss 9.6484e-01 (8.9340e-01)\tAcc@1  56.25 ( 59.51)\n",
      "[xla:2]Train:  Epoch: [11][ 30/129]\tTime  0.214 ( 0.215)\tData  0.012 ( 0.017)\tLoss 8.6719e-01 (9.1425e-01)\tAcc@1  69.00 ( 57.07)\n",
      "[xla:1]Train:  Epoch: [11][ 30/129]\tTime  0.209 ( 0.213)\tData  0.012 ( 0.019)\tLoss 9.4141e-01 (8.7450e-01)\tAcc@1  56.25 ( 60.89)\n",
      "[xla:6]Train:  Epoch: [11][ 60/129]\tTime  0.179 ( 0.210)\tData  0.011 ( 0.016)\tLoss 8.4766e-01 (8.4862e-01)\tAcc@1  69.00 ( 61.30)\n",
      "[xla:3]Train:  Epoch: [11][ 60/129]\tTime  0.165 ( 0.210)\tData  0.011 ( 0.017)\tLoss 7.1094e-01 (8.7186e-01)\tAcc@1  62.50 ( 59.76)\n",
      "[xla:2]Train:  Epoch: [11][ 60/129]\tTime  0.198 ( 0.212)\tData  0.011 ( 0.016)\tLoss 8.7891e-01 (8.7849e-01)\tAcc@1  62.50 ( 59.35)\n",
      "[xla:7]Train:  Epoch: [11][ 60/129]\tTime  0.180 ( 0.211)\tData  0.010 ( 0.019)\tLoss 6.0547e-01 (8.6981e-01)\tAcc@1  81.00 ( 60.78)\n",
      "[xla:1]Train:  Epoch: [11][ 60/129]\tTime  0.236 ( 0.211)\tData  0.011 ( 0.019)\tLoss 9.4531e-01 (8.8890e-01)\tAcc@1  37.50 ( 59.33)\n",
      "[xla:0]Train:  Epoch: [11][ 60/129]\tTime  0.195 ( 0.207)\tData  0.012 ( 0.017)\tLoss 6.9531e-01 (8.7609e-01)\tAcc@1  69.00 ( 60.49)\n",
      "[xla:5]Train:  Epoch: [11][ 60/129]\tTime  0.213 ( 0.216)\tData  0.011 ( 0.016)\tLoss 9.4141e-01 (8.7340e-01)\tAcc@1  43.75 ( 60.39)\n",
      "[xla:4]Train:  Epoch: [11][ 60/129]\tTime  0.211 ( 0.211)\tData  0.009 ( 0.015)\tLoss 8.5938e-01 (8.7430e-01)\tAcc@1  69.00 ( 60.70)\n",
      "[xla:4]Train:  Epoch: [11][ 90/129]\tTime  0.152 ( 0.210)\tData  0.017 ( 0.015)\tLoss 9.1406e-01 (8.5920e-01)\tAcc@1  43.75 ( 61.44)\n",
      "[xla:2]Train:  Epoch: [11][ 90/129]\tTime  0.167 ( 0.211)\tData  0.012 ( 0.016)\tLoss 7.1875e-01 (8.8829e-01)\tAcc@1  69.00 ( 58.76)\n",
      "[xla:3]Train:  Epoch: [11][ 90/129]\tTime  0.185 ( 0.210)\tData  0.017 ( 0.016)\tLoss 5.7422e-01 (8.4944e-01)\tAcc@1  87.50 ( 60.68)\n",
      "[xla:7]Train:  Epoch: [11][ 90/129]\tTime  0.182 ( 0.211)\tData  0.017 ( 0.018)\tLoss 5.8594e-01 (8.7500e-01)\tAcc@1  75.00 ( 60.12)\n",
      "[xla:5]Train:  Epoch: [11][ 90/129]\tTime  0.198 ( 0.215)\tData  0.011 ( 0.015)\tLoss 6.3281e-01 (8.6770e-01)\tAcc@1  75.00 ( 60.76)\n",
      "[xla:6]Train:  Epoch: [11][ 90/129]\tTime  0.205 ( 0.211)\tData  0.018 ( 0.016)\tLoss 6.8750e-01 (8.4989e-01)\tAcc@1  75.00 ( 61.57)\n",
      "[xla:1]Train:  Epoch: [11][ 90/129]\tTime  0.225 ( 0.211)\tData  0.010 ( 0.017)\tLoss 5.7031e-01 (8.6860e-01)\tAcc@1  81.00 ( 60.10)\n",
      "[xla:0]Train:  Epoch: [11][ 90/129]\tTime  0.217 ( 0.209)\tData  0.010 ( 0.016)\tLoss 1.1328e+00 (8.5470e-01)\tAcc@1  43.75 ( 61.37)\n",
      "[xla:7]Train:  Epoch: [11][120/129]\tTime  0.198 ( 0.210)\tData  0.014 ( 0.017)\tLoss 8.0469e-01 (8.5855e-01)\tAcc@1  56.25 ( 60.66)\n",
      "[xla:0]Train:  Epoch: [11][120/129]\tTime  0.171 ( 0.208)\tData  0.014 ( 0.016)\tLoss 7.3047e-01 (8.3431e-01)\tAcc@1  56.25 ( 62.79)\n",
      "[xla:1]Train:  Epoch: [11][120/129]\tTime  0.192 ( 0.210)\tData  0.011 ( 0.017)\tLoss 8.7891e-01 (8.5347e-01)\tAcc@1  50.00 ( 61.38)\n",
      "[xla:6]Train:  Epoch: [11][120/129]\tTime  0.212 ( 0.210)\tData  0.012 ( 0.015)\tLoss 5.7031e-01 (8.4042e-01)\tAcc@1  62.50 ( 62.33)\n",
      "[xla:5]Train:  Epoch: [11][120/129]\tTime  0.201 ( 0.213)\tData  0.011 ( 0.015)\tLoss 3.1250e-01 (8.6167e-01)\tAcc@1 100.00 ( 60.78)\n",
      "[xla:4]Train:  Epoch: [11][120/129]\tTime  0.223 ( 0.210)\tData  0.012 ( 0.015)\tLoss 1.0391e+00 (8.4488e-01)\tAcc@1  56.25 ( 62.95)\n",
      "[xla:3]Train:  Epoch: [11][120/129]\tTime  0.209 ( 0.210)\tData  0.013 ( 0.016)\tLoss 6.1719e-01 (8.3017e-01)\tAcc@1  75.00 ( 61.75)\n",
      "[xla:2]Train:  Epoch: [11][120/129]\tTime  0.196 ( 0.211)\tData  0.009 ( 0.015)\tLoss 8.5938e-01 (8.5542e-01)\tAcc@1  62.50 ( 60.73)\n",
      "[xla:7]Validation: [0/6]\tTime  0.075 ( 0.075)\tLoss 9.9219e-01 (9.9219e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 11\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Validation: [0/6]\tTime  0.085 ( 0.085)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.116 ( 0.116)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 6.3281e-01 (6.3281e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 1.2578e+00 (1.2578e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.142 ( 0.142)\tLoss 1.1953e+00 (1.1953e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [12][  0/129]\tTime  0.123 ( 0.123)\tData  0.025 ( 0.025)\tLoss 9.8047e-01 (9.8047e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [12][  0/129]\tTime  0.124 ( 0.124)\tData  0.004 ( 0.004)\tLoss 7.8125e-01 (7.8125e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [12][  0/129]\tTime  0.210 ( 0.210)\tData  0.046 ( 0.046)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [12][  0/129]\tTime  0.223 ( 0.223)\tData  0.083 ( 0.083)\tLoss 8.1641e-01 (8.1641e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [12][  0/129]\tTime  0.246 ( 0.246)\tData  0.059 ( 0.059)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [12][  0/129]\tTime  0.262 ( 0.262)\tData  0.107 ( 0.107)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [12][  0/129]\tTime  0.189 ( 0.189)\tData  0.031 ( 0.031)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:4]Train:  Epoch: [12][  0/129]\tTime  0.190 ( 0.190)\tData  0.055 ( 0.055)\tLoss 9.8047e-01 (9.8047e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [12][ 30/129]\tTime  0.199 ( 0.233)\tData  0.011 ( 0.021)\tLoss 8.7891e-01 (8.2082e-01)\tAcc@1  50.00 ( 62.92)\n",
      "[xla:3]Train:  Epoch: [12][ 30/129]\tTime  0.210 ( 0.237)\tData  0.021 ( 0.018)\tLoss 8.1641e-01 (8.6857e-01)\tAcc@1  62.50 ( 59.74)\n",
      "[xla:0]Train:  Epoch: [12][ 30/129]\tTime  0.184 ( 0.235)\tData  0.011 ( 0.017)\tLoss 9.0234e-01 (9.2263e-01)\tAcc@1  50.00 ( 57.09)\n",
      "[xla:5]Train:  Epoch: [12][ 30/129]\tTime  0.235 ( 0.233)\tData  0.014 ( 0.019)\tLoss 8.2812e-01 (8.9869e-01)\tAcc@1  50.00 ( 58.48)\n",
      "[xla:1]Train:  Epoch: [12][ 30/129]\tTime  0.217 ( 0.229)\tData  0.012 ( 0.017)\tLoss 8.2031e-01 (8.7387e-01)\tAcc@1  69.00 ( 61.55)\n",
      "[xla:2]Train:  Epoch: [12][ 30/129]\tTime  0.269 ( 0.235)\tData  0.024 ( 0.017)\tLoss 7.6172e-01 (9.0801e-01)\tAcc@1  75.00 ( 57.48)\n",
      "[xla:4]Train:  Epoch: [12][ 30/129]\tTime  0.208 ( 0.229)\tData  0.009 ( 0.016)\tLoss 7.9297e-01 (8.6278e-01)\tAcc@1  56.25 ( 61.72)\n",
      "[xla:7]Train:  Epoch: [12][ 30/129]\tTime  0.238 ( 0.248)\tData  0.012 ( 0.018)\tLoss 1.0156e+00 (8.9806e-01)\tAcc@1  37.50 ( 56.46)\n",
      "[xla:2]Train:  Epoch: [12][ 60/129]\tTime  0.178 ( 0.224)\tData  0.012 ( 0.016)\tLoss 9.5312e-01 (8.6796e-01)\tAcc@1  50.00 ( 60.16)\n",
      "[xla:1]Train:  Epoch: [12][ 60/129]\tTime  0.165 ( 0.221)\tData  0.011 ( 0.016)\tLoss 9.8438e-01 (8.8720e-01)\tAcc@1  50.00 ( 58.94)\n",
      "[xla:5]Train:  Epoch: [12][ 60/129]\tTime  0.243 ( 0.223)\tData  0.011 ( 0.017)\tLoss 8.5938e-01 (8.7955e-01)\tAcc@1  37.50 ( 59.05)\n",
      "[xla:0]Train:  Epoch: [12][ 60/129]\tTime  0.172 ( 0.225)\tData  0.011 ( 0.017)\tLoss 6.2891e-01 (8.7705e-01)\tAcc@1  87.50 ( 61.10)\n",
      "[xla:7]Train:  Epoch: [12][ 60/129]\tTime  0.202 ( 0.230)\tData  0.012 ( 0.016)\tLoss 5.4688e-01 (8.7468e-01)\tAcc@1  87.50 ( 58.62)\n",
      "[xla:3]Train:  Epoch: [12][ 60/129]\tTime  0.216 ( 0.226)\tData  0.013 ( 0.017)\tLoss 6.6016e-01 (8.4836e-01)\tAcc@1  75.00 ( 61.72)\n",
      "[xla:4]Train:  Epoch: [12][ 60/129]\tTime  0.227 ( 0.221)\tData  0.022 ( 0.015)\tLoss 8.0859e-01 (8.4785e-01)\tAcc@1  62.50 ( 62.02)\n",
      "[xla:6]Train:  Epoch: [12][ 60/129]\tTime  0.215 ( 0.224)\tData  0.012 ( 0.017)\tLoss 8.8672e-01 (8.4894e-01)\tAcc@1  62.50 ( 61.50)\n",
      "[xla:5]Train:  Epoch: [12][ 90/129]\tTime  0.181 ( 0.218)\tData  0.012 ( 0.016)\tLoss 6.2500e-01 (8.7221e-01)\tAcc@1  75.00 ( 59.93)\n",
      "[xla:2]Train:  Epoch: [12][ 90/129]\tTime  0.165 ( 0.219)\tData  0.011 ( 0.016)\tLoss 6.9141e-01 (8.7607e-01)\tAcc@1  75.00 ( 59.72)\n",
      "[xla:4]Train:  Epoch: [12][ 90/129]\tTime  0.187 ( 0.216)\tData  0.014 ( 0.016)\tLoss 9.6875e-01 (8.3866e-01)\tAcc@1  50.00 ( 62.73)\n",
      "[xla:7]Train:  Epoch: [12][ 90/129]\tTime  0.211 ( 0.223)\tData  0.010 ( 0.016)\tLoss 7.2266e-01 (8.7032e-01)\tAcc@1  69.00 ( 58.61)\n",
      "[xla:1]Train:  Epoch: [12][ 90/129]\tTime  0.191 ( 0.217)\tData  0.013 ( 0.017)\tLoss 6.4062e-01 (8.5807e-01)\tAcc@1  81.00 ( 60.46)\n",
      "[xla:6]Train:  Epoch: [12][ 90/129]\tTime  0.194 ( 0.218)\tData  0.014 ( 0.016)\tLoss 6.4844e-01 (8.5006e-01)\tAcc@1  75.00 ( 60.53)\n",
      "[xla:3]Train:  Epoch: [12][ 90/129]\tTime  0.208 ( 0.220)\tData  0.011 ( 0.016)\tLoss 6.6406e-01 (8.3128e-01)\tAcc@1  56.25 ( 61.91)\n",
      "[xla:0]Train:  Epoch: [12][ 90/129]\tTime  0.223 ( 0.219)\tData  0.011 ( 0.016)\tLoss 1.1953e+00 (8.5581e-01)\tAcc@1  31.25 ( 61.78)\n",
      "[xla:5]Train:  Epoch: [12][120/129]\tTime  0.180 ( 0.215)\tData  0.012 ( 0.015)\tLoss 3.7891e-01 (8.6519e-01)\tAcc@1  87.50 ( 60.67)\n",
      "[xla:3]Train:  Epoch: [12][120/129]\tTime  0.171 ( 0.216)\tData  0.011 ( 0.016)\tLoss 5.8594e-01 (8.1450e-01)\tAcc@1  75.00 ( 63.00)\n",
      "[xla:7]Train:  Epoch: [12][120/129]\tTime  0.196 ( 0.219)\tData  0.010 ( 0.015)\tLoss 7.7734e-01 (8.5250e-01)\tAcc@1  69.00 ( 60.11)\n",
      "[xla:4]Train:  Epoch: [12][120/129]\tTime  0.176 ( 0.214)\tData  0.011 ( 0.016)\tLoss 8.5938e-01 (8.2159e-01)\tAcc@1  75.00 ( 63.81)\n",
      "[xla:2]Train:  Epoch: [12][120/129]\tTime  0.193 ( 0.216)\tData  0.010 ( 0.015)\tLoss 8.3984e-01 (8.4015e-01)\tAcc@1  62.50 ( 62.17)\n",
      "[xla:6]Train:  Epoch: [12][120/129]\tTime  0.180 ( 0.215)\tData  0.011 ( 0.016)\tLoss 4.5312e-01 (8.2603e-01)\tAcc@1  81.00 ( 61.96)\n",
      "[xla:0]Train:  Epoch: [12][120/129]\tTime  0.211 ( 0.216)\tData  0.010 ( 0.016)\tLoss 6.9531e-01 (8.2449e-01)\tAcc@1  69.00 ( 63.42)\n",
      "[xla:1]Train:  Epoch: [12][120/129]\tTime  0.223 ( 0.214)\tData  0.013 ( 0.016)\tLoss 8.0078e-01 (8.3686e-01)\tAcc@1  56.25 ( 61.90)\n",
      "Finished training epoch 12\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/6]\tTime  0.144 ( 0.144)\tLoss 1.1875e+00 (1.1875e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.068 ( 0.068)\tLoss 7.4219e-01 (7.4219e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.139 ( 0.139)\tLoss 8.9453e-01 (8.9453e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:4]Validation: [0/6]\tTime  0.078 ( 0.078)\tLoss 1.2578e+00 (1.2578e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.072 ( 0.072)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [13][  0/129]\tTime  0.134 ( 0.134)\tData  0.021 ( 0.021)\tLoss 8.9844e-01 (8.9844e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [13][  0/129]\tTime  0.185 ( 0.185)\tData  0.029 ( 0.029)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [13][  0/129]\tTime  0.170 ( 0.170)\tData  0.041 ( 0.041)\tLoss 5.1953e-01 (5.1953e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Train:  Epoch: [13][  0/129]\tTime  0.204 ( 0.204)\tData  0.071 ( 0.071)\tLoss 8.7109e-01 (8.7109e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [13][  0/129]\tTime  0.188 ( 0.188)\tData  0.040 ( 0.040)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [13][  0/129]\tTime  0.170 ( 0.170)\tData  0.046 ( 0.046)\tLoss 9.5312e-01 (9.5312e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [13][  0/129]\tTime  0.203 ( 0.203)\tData  0.063 ( 0.063)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:5]Train:  Epoch: [13][  0/129]\tTime  0.199 ( 0.199)\tData  0.044 ( 0.044)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [13][ 30/129]\tTime  0.169 ( 0.206)\tData  0.013 ( 0.016)\tLoss 7.7344e-01 (9.1079e-01)\tAcc@1  75.00 ( 55.85)\n",
      "[xla:5]Train:  Epoch: [13][ 30/129]\tTime  0.192 ( 0.205)\tData  0.017 ( 0.016)\tLoss 8.3594e-01 (8.7601e-01)\tAcc@1  62.50 ( 59.14)\n",
      "[xla:4]Train:  Epoch: [13][ 30/129]\tTime  0.181 ( 0.208)\tData  0.010 ( 0.016)\tLoss 8.3984e-01 (8.4715e-01)\tAcc@1  69.00 ( 60.35)\n",
      "[xla:6]Train:  Epoch: [13][ 30/129]\tTime  0.203 ( 0.205)\tData  0.012 ( 0.015)\tLoss 1.1250e+00 (8.2132e-01)\tAcc@1  31.25 ( 59.91)\n",
      "[xla:3]Train:  Epoch: [13][ 30/129]\tTime  0.222 ( 0.213)\tData  0.013 ( 0.014)\tLoss 9.3750e-01 (8.6164e-01)\tAcc@1  50.00 ( 58.90)\n",
      "[xla:1]Train:  Epoch: [13][ 30/129]\tTime  0.188 ( 0.215)\tData  0.009 ( 0.017)\tLoss 8.7109e-01 (8.5433e-01)\tAcc@1  62.50 ( 62.92)\n",
      "[xla:7]Train:  Epoch: [13][ 30/129]\tTime  0.210 ( 0.206)\tData  0.015 ( 0.017)\tLoss 9.3750e-01 (8.9567e-01)\tAcc@1  50.00 ( 57.67)\n",
      "[xla:0]Train:  Epoch: [13][ 30/129]\tTime  0.245 ( 0.221)\tData  0.011 ( 0.016)\tLoss 9.5703e-01 (9.1772e-01)\tAcc@1  56.25 ( 55.27)\n",
      "[xla:3]Train:  Epoch: [13][ 60/129]\tTime  0.188 ( 0.212)\tData  0.012 ( 0.014)\tLoss 6.5625e-01 (8.3434e-01)\tAcc@1  62.50 ( 61.60)\n",
      "[xla:2]Train:  Epoch: [13][ 60/129]\tTime  0.209 ( 0.208)\tData  0.018 ( 0.015)\tLoss 8.7500e-01 (8.7311e-01)\tAcc@1  56.25 ( 59.25)\n",
      "[xla:4]Train:  Epoch: [13][ 60/129]\tTime  0.211 ( 0.209)\tData  0.012 ( 0.016)\tLoss 9.8828e-01 (8.3696e-01)\tAcc@1  56.25 ( 61.83)\n",
      "[xla:7]Train:  Epoch: [13][ 60/129]\tTime  0.253 ( 0.208)\tData  0.017 ( 0.017)\tLoss 5.4297e-01 (8.7301e-01)\tAcc@1  87.50 ( 58.21)\n",
      "[xla:5]Train:  Epoch: [13][ 60/129]\tTime  0.215 ( 0.208)\tData  0.016 ( 0.015)\tLoss 1.0859e+00 (8.6943e-01)\tAcc@1  37.50 ( 59.57)\n",
      "[xla:0]Train:  Epoch: [13][ 60/129]\tTime  0.217 ( 0.216)\tData  0.015 ( 0.015)\tLoss 7.4609e-01 (8.7471e-01)\tAcc@1  62.50 ( 59.36)\n",
      "[xla:6]Train:  Epoch: [13][ 60/129]\tTime  0.254 ( 0.208)\tData  0.018 ( 0.015)\tLoss 1.0547e+00 (8.3184e-01)\tAcc@1  56.25 ( 61.09)\n",
      "[xla:1]Train:  Epoch: [13][ 60/129]\tTime  0.205 ( 0.213)\tData  0.008 ( 0.017)\tLoss 1.0391e+00 (8.7814e-01)\tAcc@1  31.25 ( 60.58)\n",
      "[xla:3]Train:  Epoch: [13][ 90/129]\tTime  0.177 ( 0.209)\tData  0.013 ( 0.015)\tLoss 5.3125e-01 (8.2774e-01)\tAcc@1  94.00 ( 62.40)\n",
      "[xla:4]Train:  Epoch: [13][ 90/129]\tTime  0.173 ( 0.208)\tData  0.012 ( 0.016)\tLoss 8.9844e-01 (8.4439e-01)\tAcc@1  56.25 ( 62.06)\n",
      "[xla:6]Train:  Epoch: [13][ 90/129]\tTime  0.186 ( 0.207)\tData  0.012 ( 0.014)\tLoss 5.7812e-01 (8.3577e-01)\tAcc@1  75.00 ( 60.40)\n",
      "[xla:7]Train:  Epoch: [13][ 90/129]\tTime  0.193 ( 0.207)\tData  0.012 ( 0.016)\tLoss 5.2344e-01 (8.6826e-01)\tAcc@1  87.50 ( 58.74)\n",
      "[xla:0]Train:  Epoch: [13][ 90/129]\tTime  0.197 ( 0.212)\tData  0.011 ( 0.015)\tLoss 1.2578e+00 (8.5004e-01)\tAcc@1  43.75 ( 60.95)\n",
      "[xla:5]Train:  Epoch: [13][ 90/129]\tTime  0.225 ( 0.207)\tData  0.011 ( 0.014)\tLoss 6.0547e-01 (8.7024e-01)\tAcc@1  69.00 ( 59.80)\n",
      "[xla:1]Train:  Epoch: [13][ 90/129]\tTime  0.204 ( 0.210)\tData  0.011 ( 0.016)\tLoss 6.4844e-01 (8.5306e-01)\tAcc@1  87.50 ( 62.46)\n",
      "[xla:2]Train:  Epoch: [13][ 90/129]\tTime  0.189 ( 0.207)\tData  0.011 ( 0.015)\tLoss 6.6797e-01 (8.8670e-01)\tAcc@1  69.00 ( 59.11)\n",
      "[xla:1]Train:  Epoch: [13][120/129]\tTime  0.197 ( 0.210)\tData  0.013 ( 0.016)\tLoss 7.9688e-01 (8.3547e-01)\tAcc@1  62.50 ( 63.50)\n",
      "[xla:2]Train:  Epoch: [13][120/129]\tTime  0.222 ( 0.208)\tData  0.020 ( 0.016)\tLoss 8.3203e-01 (8.4583e-01)\tAcc@1  56.25 ( 61.51)\n",
      "[xla:6]Train:  Epoch: [13][120/129]\tTime  0.195 ( 0.207)\tData  0.010 ( 0.014)\tLoss 5.4688e-01 (8.1603e-01)\tAcc@1  81.00 ( 62.26)\n",
      "[xla:7]Train:  Epoch: [13][120/129]\tTime  0.203 ( 0.207)\tData  0.009 ( 0.016)\tLoss 7.1484e-01 (8.4580e-01)\tAcc@1  62.50 ( 60.61)\n",
      "[xla:5]Train:  Epoch: [13][120/129]\tTime  0.184 ( 0.207)\tData  0.012 ( 0.015)\tLoss 4.2383e-01 (8.5965e-01)\tAcc@1  87.50 ( 60.94)\n",
      "[xla:0]Train:  Epoch: [13][120/129]\tTime  0.229 ( 0.211)\tData  0.012 ( 0.015)\tLoss 8.0859e-01 (8.2078e-01)\tAcc@1  62.50 ( 63.14)\n",
      "[xla:4]Train:  Epoch: [13][120/129]\tTime  0.194 ( 0.208)\tData  0.012 ( 0.015)\tLoss 8.3984e-01 (8.2772e-01)\tAcc@1  56.25 ( 62.89)\n",
      "[xla:3]Train:  Epoch: [13][120/129]\tTime  0.199 ( 0.210)\tData  0.009 ( 0.015)\tLoss 4.9023e-01 (8.1365e-01)\tAcc@1  81.00 ( 62.79)\n",
      "Finished training epoch 13\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/6]\tTime  0.104 ( 0.104)\tLoss 6.1719e-01 (6.1719e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Validation: [0/6]\tTime  0.145 ( 0.145)\tLoss 9.0234e-01 (9.0234e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Validation: [0/6]\tTime  0.138 ( 0.138)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.083 ( 0.083)\tLoss 1.1953e+00 (1.1953e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Validation: [0/6]\tTime  0.119 ( 0.119)\tLoss 1.2812e+00 (1.2812e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.073 ( 0.073)\tLoss 7.5391e-01 (7.5391e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Validation: [0/6]\tTime  0.096 ( 0.096)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [14][  0/129]\tTime  0.223 ( 0.223)\tData  0.042 ( 0.042)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [14][  0/129]\tTime  0.190 ( 0.190)\tData  0.036 ( 0.036)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [14][  0/129]\tTime  0.184 ( 0.184)\tData  0.039 ( 0.039)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [14][  0/129]\tTime  0.171 ( 0.171)\tData  0.035 ( 0.035)\tLoss 8.5938e-01 (8.5938e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Train:  Epoch: [14][  0/129]\tTime  0.218 ( 0.218)\tData  0.066 ( 0.066)\tLoss 9.6875e-01 (9.6875e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Train:  Epoch: [14][  0/129]\tTime  0.239 ( 0.239)\tData  0.059 ( 0.059)\tLoss 6.8359e-01 (6.8359e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [14][  0/129]\tTime  0.227 ( 0.227)\tData  0.046 ( 0.046)\tLoss 6.3672e-01 (6.3672e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [14][  0/129]\tTime  0.216 ( 0.216)\tData  0.080 ( 0.080)\tLoss 9.1406e-01 (9.1406e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [14][ 30/129]\tTime  0.187 ( 0.221)\tData  0.014 ( 0.018)\tLoss 8.3594e-01 (8.4917e-01)\tAcc@1  62.50 ( 61.94)\n",
      "[xla:5]Train:  Epoch: [14][ 30/129]\tTime  0.184 ( 0.214)\tData  0.010 ( 0.017)\tLoss 7.4609e-01 (8.5295e-01)\tAcc@1  56.25 ( 61.73)\n",
      "[xla:3]Train:  Epoch: [14][ 30/129]\tTime  0.216 ( 0.217)\tData  0.016 ( 0.016)\tLoss 9.6484e-01 (8.7412e-01)\tAcc@1  56.25 ( 58.70)\n",
      "[xla:2]Train:  Epoch: [14][ 30/129]\tTime  0.181 ( 0.220)\tData  0.009 ( 0.017)\tLoss 8.0859e-01 (8.8351e-01)\tAcc@1  56.25 ( 56.44)\n",
      "[xla:7]Train:  Epoch: [14][ 30/129]\tTime  0.186 ( 0.222)\tData  0.011 ( 0.016)\tLoss 1.1719e+00 (8.9176e-01)\tAcc@1  50.00 ( 61.91)\n",
      "[xla:6]Train:  Epoch: [14][ 30/129]\tTime  0.189 ( 0.232)\tData  0.012 ( 0.016)\tLoss 8.7500e-01 (8.1048e-01)\tAcc@1  69.00 ( 63.35)\n",
      "[xla:0]Train:  Epoch: [14][ 30/129]\tTime  0.206 ( 0.236)\tData  0.009 ( 0.017)\tLoss 1.1172e+00 (9.2553e-01)\tAcc@1  50.00 ( 55.85)\n",
      "[xla:4]Train:  Epoch: [14][ 30/129]\tTime  0.222 ( 0.222)\tData  0.012 ( 0.020)\tLoss 7.6172e-01 (8.3002e-01)\tAcc@1  69.00 ( 60.91)\n",
      "[xla:6]Train:  Epoch: [14][ 60/129]\tTime  0.210 ( 0.226)\tData  0.012 ( 0.016)\tLoss 8.4375e-01 (8.2319e-01)\tAcc@1  69.00 ( 62.98)\n",
      "[xla:1]Train:  Epoch: [14][ 60/129]\tTime  0.175 ( 0.221)\tData  0.010 ( 0.017)\tLoss 8.4766e-01 (8.4516e-01)\tAcc@1  56.25 ( 61.92)\n",
      "[xla:3]Train:  Epoch: [14][ 60/129]\tTime  0.181 ( 0.219)\tData  0.010 ( 0.015)\tLoss 5.7812e-01 (8.3594e-01)\tAcc@1  81.00 ( 61.90)\n",
      "[xla:0]Train:  Epoch: [14][ 60/129]\tTime  0.223 ( 0.228)\tData  0.012 ( 0.016)\tLoss 6.8359e-01 (8.6812e-01)\tAcc@1  75.00 ( 60.16)\n",
      "[xla:7]Train:  Epoch: [14][ 60/129]\tTime  0.230 ( 0.221)\tData  0.019 ( 0.016)\tLoss 5.2734e-01 (8.5637e-01)\tAcc@1  94.00 ( 62.83)\n",
      "[xla:5]Train:  Epoch: [14][ 60/129]\tTime  0.193 ( 0.218)\tData  0.009 ( 0.016)\tLoss 9.1797e-01 (8.5777e-01)\tAcc@1  50.00 ( 61.52)\n",
      "[xla:2]Train:  Epoch: [14][ 60/129]\tTime  0.189 ( 0.221)\tData  0.010 ( 0.017)\tLoss 8.3203e-01 (8.6594e-01)\tAcc@1  56.25 ( 59.03)\n",
      "[xla:4]Train:  Epoch: [14][ 60/129]\tTime  0.248 ( 0.221)\tData  0.024 ( 0.018)\tLoss 8.0078e-01 (8.2358e-01)\tAcc@1  69.00 ( 62.53)\n",
      "[xla:4]Train:  Epoch: [14][ 90/129]\tTime  0.212 ( 0.216)\tData  0.014 ( 0.017)\tLoss 8.3984e-01 (8.2516e-01)\tAcc@1  56.25 ( 63.16)\n",
      "[xla:7]Train:  Epoch: [14][ 90/129]\tTime  0.171 ( 0.216)\tData  0.010 ( 0.016)\tLoss 6.2109e-01 (8.5693e-01)\tAcc@1  69.00 ( 62.05)\n",
      "[xla:3]Train:  Epoch: [14][ 90/129]\tTime  0.195 ( 0.215)\tData  0.018 ( 0.015)\tLoss 5.8203e-01 (8.2330e-01)\tAcc@1  75.00 ( 62.03)\n",
      "[xla:5]Train:  Epoch: [14][ 90/129]\tTime  0.205 ( 0.214)\tData  0.011 ( 0.016)\tLoss 8.2422e-01 (8.5452e-01)\tAcc@1  69.00 ( 61.44)\n",
      "[xla:6]Train:  Epoch: [14][ 90/129]\tTime  0.211 ( 0.220)\tData  0.012 ( 0.015)\tLoss 4.7656e-01 (8.2143e-01)\tAcc@1  81.00 ( 62.56)\n",
      "[xla:0]Train:  Epoch: [14][ 90/129]\tTime  0.203 ( 0.221)\tData  0.011 ( 0.015)\tLoss 1.2500e+00 (8.4656e-01)\tAcc@1  37.50 ( 61.50)\n",
      "[xla:1]Train:  Epoch: [14][ 90/129]\tTime  0.182 ( 0.216)\tData  0.009 ( 0.016)\tLoss 6.3281e-01 (8.3877e-01)\tAcc@1  69.00 ( 61.78)\n",
      "[xla:2]Train:  Epoch: [14][ 90/129]\tTime  0.203 ( 0.216)\tData  0.010 ( 0.016)\tLoss 6.2109e-01 (8.6523e-01)\tAcc@1  75.00 ( 59.29)\n",
      "[xla:2]Train:  Epoch: [14][120/129]\tTime  0.182 ( 0.212)\tData  0.009 ( 0.016)\tLoss 9.4141e-01 (8.2946e-01)\tAcc@1  50.00 ( 61.49)\n",
      "[xla:5]Train:  Epoch: [14][120/129]\tTime  0.216 ( 0.211)\tData  0.013 ( 0.015)\tLoss 4.0039e-01 (8.4031e-01)\tAcc@1  81.00 ( 62.28)\n",
      "[xla:4]Train:  Epoch: [14][120/129]\tTime  0.227 ( 0.213)\tData  0.014 ( 0.017)\tLoss 8.2031e-01 (8.1121e-01)\tAcc@1  62.50 ( 63.77)\n",
      "[xla:6]Train:  Epoch: [14][120/129]\tTime  0.202 ( 0.216)\tData  0.010 ( 0.015)\tLoss 4.9219e-01 (7.9723e-01)\tAcc@1  69.00 ( 64.11)\n",
      "[xla:0]Train:  Epoch: [14][120/129]\tTime  0.191 ( 0.217)\tData  0.010 ( 0.015)\tLoss 8.5547e-01 (8.1789e-01)\tAcc@1  62.50 ( 63.30)\n",
      "[xla:7]Train:  Epoch: [14][120/129]\tTime  0.195 ( 0.213)\tData  0.010 ( 0.016)\tLoss 8.5938e-01 (8.3967e-01)\tAcc@1  69.00 ( 62.69)\n",
      "[xla:3]Train:  Epoch: [14][120/129]\tTime  0.210 ( 0.212)\tData  0.012 ( 0.014)\tLoss 4.7656e-01 (7.9783e-01)\tAcc@1  87.50 ( 63.45)\n",
      "[xla:1]Train:  Epoch: [14][120/129]\tTime  0.218 ( 0.213)\tData  0.009 ( 0.016)\tLoss 7.1484e-01 (8.1739e-01)\tAcc@1  69.00 ( 63.36)\n",
      "[xla:3]Validation: [0/6]\tTime  0.100 ( 0.100)\tLoss 7.6172e-01 (7.6172e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 14\n",
      "[xla:7]Validation: [0/6]\tTime  0.082 ( 0.082)\tLoss 1.0547e+00 (1.0547e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:5]Validation: [0/6]\tTime  0.102 ( 0.102)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:6]Validation: [0/6]\tTime  0.074 ( 0.074)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.048 ( 0.048)\tLoss 6.2891e-01 (6.2891e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 1.1719e+00 (1.1719e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.088 ( 0.088)\tLoss 1.2969e+00 (1.2969e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.146 ( 0.146)\tLoss 8.9062e-01 (8.9062e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [15][  0/129]\tTime  0.176 ( 0.176)\tData  0.066 ( 0.066)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:6]Train:  Epoch: [15][  0/129]\tTime  0.166 ( 0.166)\tData  0.043 ( 0.043)\tLoss 8.9453e-01 (8.9453e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [15][  0/129]\tTime  0.232 ( 0.232)\tData  0.056 ( 0.056)\tLoss 7.8516e-01 (7.8516e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [15][  0/129]\tTime  0.207 ( 0.207)\tData  0.044 ( 0.044)\tLoss 8.9453e-01 (8.9453e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [15][  0/129]\tTime  0.204 ( 0.204)\tData  0.074 ( 0.074)\tLoss 9.5312e-01 (9.5312e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [15][  0/129]\tTime  0.252 ( 0.252)\tData  0.061 ( 0.061)\tLoss 9.4922e-01 (9.4922e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [15][  0/129]\tTime  0.270 ( 0.270)\tData  0.113 ( 0.113)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Train:  Epoch: [15][  0/129]\tTime  0.210 ( 0.210)\tData  0.073 ( 0.073)\tLoss 9.4922e-01 (9.4922e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:5]Train:  Epoch: [15][ 30/129]\tTime  0.185 ( 0.208)\tData  0.012 ( 0.018)\tLoss 6.3672e-01 (8.3002e-01)\tAcc@1  75.00 ( 61.31)\n",
      "[xla:7]Train:  Epoch: [15][ 30/129]\tTime  0.195 ( 0.214)\tData  0.013 ( 0.016)\tLoss 1.0547e+00 (9.0776e-01)\tAcc@1  69.00 ( 59.10)\n",
      "[xla:2]Train:  Epoch: [15][ 30/129]\tTime  0.205 ( 0.205)\tData  0.020 ( 0.017)\tLoss 7.3047e-01 (8.6530e-01)\tAcc@1  75.00 ( 59.49)\n",
      "[xla:0]Train:  Epoch: [15][ 30/129]\tTime  0.178 ( 0.212)\tData  0.009 ( 0.016)\tLoss 1.0000e+00 (9.2553e-01)\tAcc@1  56.25 ( 56.70)\n",
      "[xla:6]Train:  Epoch: [15][ 30/129]\tTime  0.203 ( 0.215)\tData  0.008 ( 0.015)\tLoss 1.0000e+00 (8.0633e-01)\tAcc@1  43.75 ( 62.92)\n",
      "[xla:3]Train:  Epoch: [15][ 30/129]\tTime  0.187 ( 0.225)\tData  0.008 ( 0.017)\tLoss 8.2422e-01 (8.5660e-01)\tAcc@1  56.25 ( 59.89)\n",
      "[xla:1]Train:  Epoch: [15][ 30/129]\tTime  0.223 ( 0.211)\tData  0.012 ( 0.014)\tLoss 8.9062e-01 (8.2220e-01)\tAcc@1  62.50 ( 64.31)\n",
      "[xla:4]Train:  Epoch: [15][ 30/129]\tTime  0.240 ( 0.211)\tData  0.014 ( 0.017)\tLoss 6.4844e-01 (8.5900e-01)\tAcc@1  75.00 ( 60.76)\n",
      "[xla:0]Train:  Epoch: [15][ 60/129]\tTime  0.193 ( 0.209)\tData  0.015 ( 0.014)\tLoss 7.5781e-01 (8.6821e-01)\tAcc@1  62.50 ( 59.16)\n",
      "[xla:7]Train:  Epoch: [15][ 60/129]\tTime  0.164 ( 0.211)\tData  0.011 ( 0.016)\tLoss 4.4922e-01 (8.7532e-01)\tAcc@1  81.00 ( 60.99)\n",
      "[xla:2]Train:  Epoch: [15][ 60/129]\tTime  0.183 ( 0.206)\tData  0.011 ( 0.017)\tLoss 9.4141e-01 (8.3594e-01)\tAcc@1  56.25 ( 60.79)\n",
      "[xla:1]Train:  Epoch: [15][ 60/129]\tTime  0.201 ( 0.209)\tData  0.010 ( 0.015)\tLoss 8.9453e-01 (8.4644e-01)\tAcc@1  50.00 ( 63.14)\n",
      "[xla:5]Train:  Epoch: [15][ 60/129]\tTime  0.181 ( 0.208)\tData  0.011 ( 0.017)\tLoss 9.6094e-01 (8.3882e-01)\tAcc@1  50.00 ( 62.23)\n",
      "[xla:6]Train:  Epoch: [15][ 60/129]\tTime  0.216 ( 0.211)\tData  0.010 ( 0.014)\tLoss 9.9609e-01 (8.1449e-01)\tAcc@1  43.75 ( 63.44)\n",
      "[xla:3]Train:  Epoch: [15][ 60/129]\tTime  0.205 ( 0.216)\tData  0.009 ( 0.016)\tLoss 7.6172e-01 (8.4580e-01)\tAcc@1  62.50 ( 61.20)\n",
      "[xla:4]Train:  Epoch: [15][ 60/129]\tTime  0.218 ( 0.209)\tData  0.010 ( 0.016)\tLoss 8.1250e-01 (8.3831e-01)\tAcc@1  50.00 ( 62.97)\n",
      "[xla:4]Train:  Epoch: [15][ 90/129]\tTime  0.174 ( 0.207)\tData  0.011 ( 0.015)\tLoss 9.0625e-01 (8.4411e-01)\tAcc@1  43.75 ( 62.62)\n",
      "[xla:2]Train:  Epoch: [15][ 90/129]\tTime  0.175 ( 0.206)\tData  0.012 ( 0.016)\tLoss 7.0703e-01 (8.5143e-01)\tAcc@1  69.00 ( 60.61)\n",
      "[xla:1]Train:  Epoch: [15][ 90/129]\tTime  0.185 ( 0.208)\tData  0.011 ( 0.014)\tLoss 8.7891e-01 (8.3568e-01)\tAcc@1  69.00 ( 62.74)\n",
      "[xla:0]Train:  Epoch: [15][ 90/129]\tTime  0.184 ( 0.208)\tData  0.012 ( 0.015)\tLoss 1.2109e+00 (8.4285e-01)\tAcc@1  37.50 ( 61.23)\n",
      "[xla:3]Train:  Epoch: [15][ 90/129]\tTime  0.191 ( 0.213)\tData  0.021 ( 0.016)\tLoss 6.0938e-01 (8.2458e-01)\tAcc@1  62.50 ( 61.63)\n",
      "[xla:5]Train:  Epoch: [15][ 90/129]\tTime  0.210 ( 0.207)\tData  0.010 ( 0.017)\tLoss 7.8125e-01 (8.4675e-01)\tAcc@1  69.00 ( 60.68)\n",
      "[xla:7]Train:  Epoch: [15][ 90/129]\tTime  0.222 ( 0.209)\tData  0.012 ( 0.016)\tLoss 6.9922e-01 (8.7672e-01)\tAcc@1  69.00 ( 60.54)\n",
      "[xla:6]Train:  Epoch: [15][ 90/129]\tTime  0.202 ( 0.209)\tData  0.012 ( 0.015)\tLoss 6.8359e-01 (8.2533e-01)\tAcc@1  56.25 ( 63.07)\n",
      "[xla:1]Train:  Epoch: [15][120/129]\tTime  0.167 ( 0.206)\tData  0.013 ( 0.014)\tLoss 8.1641e-01 (8.1260e-01)\tAcc@1  62.50 ( 64.27)\n",
      "[xla:5]Train:  Epoch: [15][120/129]\tTime  0.197 ( 0.205)\tData  0.013 ( 0.016)\tLoss 4.2969e-01 (8.3997e-01)\tAcc@1  81.00 ( 61.71)\n",
      "[xla:3]Train:  Epoch: [15][120/129]\tTime  0.181 ( 0.210)\tData  0.011 ( 0.015)\tLoss 4.6094e-01 (7.9655e-01)\tAcc@1  87.50 ( 63.77)\n",
      "[xla:0]Train:  Epoch: [15][120/129]\tTime  0.218 ( 0.206)\tData  0.015 ( 0.015)\tLoss 7.7344e-01 (8.1219e-01)\tAcc@1  62.50 ( 63.41)\n",
      "[xla:4]Train:  Epoch: [15][120/129]\tTime  0.193 ( 0.206)\tData  0.012 ( 0.015)\tLoss 6.8750e-01 (8.2441e-01)\tAcc@1  81.00 ( 63.47)\n",
      "[xla:6]Train:  Epoch: [15][120/129]\tTime  0.183 ( 0.207)\tData  0.011 ( 0.014)\tLoss 5.7031e-01 (8.1148e-01)\tAcc@1  62.50 ( 63.67)\n",
      "[xla:2]Train:  Epoch: [15][120/129]\tTime  0.209 ( 0.205)\tData  0.011 ( 0.016)\tLoss 8.8281e-01 (8.2269e-01)\tAcc@1  43.75 ( 62.49)\n",
      "[xla:7]Train:  Epoch: [15][120/129]\tTime  0.197 ( 0.207)\tData  0.013 ( 0.016)\tLoss 8.0078e-01 (8.4654e-01)\tAcc@1  56.25 ( 61.97)\n",
      "[xla:3]Validation: [0/6]\tTime  0.084 ( 0.084)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.112 ( 0.112)\tLoss 1.2031e+00 (1.2031e+00)\tAcc@1  43.75 ( 43.75)\n",
      "Finished training epoch 15\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:4]Validation: [0/6]\tTime  0.089 ( 0.089)\tLoss 1.2734e+00 (1.2734e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 5.8984e-01 (5.8984e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.089 ( 0.089)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.131 ( 0.131)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Validation: [0/6]\tTime  0.110 ( 0.110)\tLoss 8.9062e-01 (8.9062e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [16][  0/129]\tTime  0.174 ( 0.174)\tData  0.044 ( 0.044)\tLoss 5.3125e-01 (5.3125e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [16][  0/129]\tTime  0.215 ( 0.215)\tData  0.068 ( 0.068)\tLoss 9.6484e-01 (9.6484e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [16][  0/129]\tTime  0.230 ( 0.230)\tData  0.065 ( 0.065)\tLoss 9.5703e-01 (9.5703e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Train:  Epoch: [16][  0/129]\tTime  0.230 ( 0.230)\tData  0.082 ( 0.082)\tLoss 8.7891e-01 (8.7891e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [16][  0/129]\tTime  0.241 ( 0.241)\tData  0.061 ( 0.061)\tLoss 1.0938e+00 (1.0938e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [16][  0/129]\tTime  0.203 ( 0.203)\tData  0.040 ( 0.040)\tLoss 7.3828e-01 (7.3828e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [16][  0/129]\tTime  0.273 ( 0.273)\tData  0.083 ( 0.083)\tLoss 9.2188e-01 (9.2188e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Train:  Epoch: [16][  0/129]\tTime  0.270 ( 0.270)\tData  0.121 ( 0.121)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [16][ 30/129]\tTime  0.225 ( 0.216)\tData  0.012 ( 0.018)\tLoss 7.0312e-01 (8.3323e-01)\tAcc@1  81.00 ( 62.91)\n",
      "[xla:6]Train:  Epoch: [16][ 30/129]\tTime  0.254 ( 0.211)\tData  0.013 ( 0.017)\tLoss 1.0859e+00 (8.1218e-01)\tAcc@1  37.50 ( 64.54)\n",
      "[xla:1]Train:  Epoch: [16][ 30/129]\tTime  0.246 ( 0.223)\tData  0.010 ( 0.017)\tLoss 8.9844e-01 (8.3783e-01)\tAcc@1  56.25 ( 62.08)\n",
      "[xla:2]Train:  Epoch: [16][ 30/129]\tTime  0.254 ( 0.213)\tData  0.012 ( 0.018)\tLoss 6.5234e-01 (8.3821e-01)\tAcc@1  69.00 ( 60.12)\n",
      "[xla:3]Train:  Epoch: [16][ 30/129]\tTime  0.251 ( 0.232)\tData  0.010 ( 0.019)\tLoss 7.8125e-01 (8.7576e-01)\tAcc@1  69.00 ( 57.31)\n",
      "[xla:7]Train:  Epoch: [16][ 30/129]\tTime  0.282 ( 0.217)\tData  0.010 ( 0.017)\tLoss 9.1406e-01 (8.8420e-01)\tAcc@1  62.50 ( 61.37)\n",
      "[xla:5]Train:  Epoch: [16][ 30/129]\tTime  0.264 ( 0.216)\tData  0.011 ( 0.017)\tLoss 7.5391e-01 (8.6794e-01)\tAcc@1  75.00 ( 60.92)\n",
      "[xla:0]Train:  Epoch: [16][ 30/129]\tTime  0.379 ( 0.217)\tData  0.011 ( 0.016)\tLoss 9.4141e-01 (8.7714e-01)\tAcc@1  50.00 ( 60.11)\n",
      "[xla:0]Train:  Epoch: [16][ 60/129]\tTime  0.176 ( 0.219)\tData  0.011 ( 0.015)\tLoss 7.8516e-01 (8.5540e-01)\tAcc@1  69.00 ( 60.70)\n",
      "[xla:7]Train:  Epoch: [16][ 60/129]\tTime  0.163 ( 0.221)\tData  0.010 ( 0.017)\tLoss 4.9609e-01 (8.6021e-01)\tAcc@1 100.00 ( 62.05)\n",
      "[xla:6]Train:  Epoch: [16][ 60/129]\tTime  0.175 ( 0.219)\tData  0.010 ( 0.018)\tLoss 8.7109e-01 (8.2399e-01)\tAcc@1  62.50 ( 62.95)\n",
      "[xla:2]Train:  Epoch: [16][ 60/129]\tTime  0.184 ( 0.220)\tData  0.011 ( 0.016)\tLoss 9.4141e-01 (8.2102e-01)\tAcc@1  50.00 ( 61.50)\n",
      "[xla:1]Train:  Epoch: [16][ 60/129]\tTime  0.214 ( 0.225)\tData  0.011 ( 0.016)\tLoss 9.2969e-01 (8.4926e-01)\tAcc@1  62.50 ( 61.17)\n",
      "[xla:5]Train:  Epoch: [16][ 60/129]\tTime  0.189 ( 0.221)\tData  0.015 ( 0.016)\tLoss 9.7656e-01 (8.6552e-01)\tAcc@1  50.00 ( 60.71)\n",
      "[xla:3]Train:  Epoch: [16][ 60/129]\tTime  0.214 ( 0.229)\tData  0.012 ( 0.017)\tLoss 5.8203e-01 (8.3178e-01)\tAcc@1  87.50 ( 60.37)\n",
      "[xla:4]Train:  Epoch: [16][ 60/129]\tTime  0.238 ( 0.222)\tData  0.011 ( 0.017)\tLoss 8.7109e-01 (8.2150e-01)\tAcc@1  62.50 ( 63.73)\n",
      "[xla:6]Train:  Epoch: [16][ 90/129]\tTime  0.169 ( 0.215)\tData  0.011 ( 0.017)\tLoss 6.3672e-01 (8.2351e-01)\tAcc@1  69.00 ( 62.53)\n",
      "[xla:7]Train:  Epoch: [16][ 90/129]\tTime  0.189 ( 0.216)\tData  0.011 ( 0.016)\tLoss 6.0547e-01 (8.5890e-01)\tAcc@1  75.00 ( 61.46)\n",
      "[xla:3]Train:  Epoch: [16][ 90/129]\tTime  0.181 ( 0.222)\tData  0.011 ( 0.016)\tLoss 7.2266e-01 (8.2293e-01)\tAcc@1  75.00 ( 61.49)\n",
      "[xla:4]Train:  Epoch: [16][ 90/129]\tTime  0.179 ( 0.217)\tData  0.015 ( 0.016)\tLoss 9.4922e-01 (8.1894e-01)\tAcc@1  62.50 ( 63.40)\n",
      "[xla:2]Train:  Epoch: [16][ 90/129]\tTime  0.203 ( 0.215)\tData  0.010 ( 0.015)\tLoss 5.7812e-01 (8.4113e-01)\tAcc@1  75.00 ( 61.15)\n",
      "[xla:0]Train:  Epoch: [16][ 90/129]\tTime  0.196 ( 0.215)\tData  0.011 ( 0.015)\tLoss 1.2188e+00 (8.3070e-01)\tAcc@1  31.25 ( 62.21)\n",
      "[xla:5]Train:  Epoch: [16][ 90/129]\tTime  0.184 ( 0.216)\tData  0.011 ( 0.016)\tLoss 7.1875e-01 (8.5852e-01)\tAcc@1  69.00 ( 60.91)\n",
      "[xla:1]Train:  Epoch: [16][ 90/129]\tTime  0.195 ( 0.219)\tData  0.020 ( 0.016)\tLoss 6.9531e-01 (8.4448e-01)\tAcc@1  69.00 ( 61.22)\n",
      "[xla:4]Train:  Epoch: [16][120/129]\tTime  0.146 ( 0.214)\tData  0.008 ( 0.016)\tLoss 8.4375e-01 (7.9736e-01)\tAcc@1  69.00 ( 64.37)\n",
      "[xla:5]Train:  Epoch: [16][120/129]\tTime  0.203 ( 0.214)\tData  0.020 ( 0.016)\tLoss 3.8672e-01 (8.4814e-01)\tAcc@1  87.50 ( 61.52)\n",
      "[xla:2]Train:  Epoch: [16][120/129]\tTime  0.221 ( 0.213)\tData  0.011 ( 0.015)\tLoss 8.1250e-01 (8.1203e-01)\tAcc@1  62.50 ( 63.19)\n",
      "[xla:6]Train:  Epoch: [16][120/129]\tTime  0.212 ( 0.213)\tData  0.016 ( 0.016)\tLoss 4.4922e-01 (8.0485e-01)\tAcc@1  81.00 ( 64.02)\n",
      "[xla:0]Train:  Epoch: [16][120/129]\tTime  0.209 ( 0.213)\tData  0.012 ( 0.015)\tLoss 7.7734e-01 (8.0254e-01)\tAcc@1  62.50 ( 63.68)\n",
      "[xla:3]Train:  Epoch: [16][120/129]\tTime  0.227 ( 0.218)\tData  0.011 ( 0.015)\tLoss 4.7852e-01 (8.0233e-01)\tAcc@1 100.00 ( 63.81)\n",
      "[xla:1]Train:  Epoch: [16][120/129]\tTime  0.217 ( 0.216)\tData  0.011 ( 0.016)\tLoss 8.4766e-01 (8.1502e-01)\tAcc@1  56.25 ( 63.04)\n",
      "[xla:7]Train:  Epoch: [16][120/129]\tTime  0.222 ( 0.214)\tData  0.012 ( 0.016)\tLoss 6.6016e-01 (8.3505e-01)\tAcc@1  69.00 ( 62.85)\n",
      "[xla:2]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 8.6328e-01 (8.6328e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Validation: [0/6]\tTime  0.107 ( 0.107)\tLoss 7.4219e-01 (7.4219e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.125 ( 0.125)\tLoss 1.2500e+00 (1.2500e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Finished training epoch 16\n",
      "[xla:5]Validation: [0/6]\tTime  0.082 ( 0.082)\tLoss 6.1719e-01 (6.1719e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Validation: [0/6]\tTime  0.113 ( 0.113)\tLoss 1.2500e+00 (1.2500e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 5.9375e-01 (5.9375e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.135 ( 0.135)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Train:  Epoch: [17][  0/129]\tTime  0.148 ( 0.148)\tData  0.043 ( 0.043)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [17][  0/129]\tTime  0.226 ( 0.226)\tData  0.083 ( 0.083)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  87.50 ( 87.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [17][  0/129]\tTime  0.170 ( 0.170)\tData  0.034 ( 0.034)\tLoss 7.6172e-01 (7.6172e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Train:  Epoch: [17][  0/129]\tTime  0.198 ( 0.198)\tData  0.040 ( 0.040)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Train:  Epoch: [17][  0/129]\tTime  0.237 ( 0.237)\tData  0.059 ( 0.059)\tLoss 9.3359e-01 (9.3359e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [17][  0/129]\tTime  0.123 ( 0.123)\tData  0.000 ( 0.000)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Train:  Epoch: [17][  0/129]\tTime  0.268 ( 0.268)\tData  0.105 ( 0.105)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Train:  Epoch: [17][  0/129]\tTime  0.229 ( 0.229)\tData  0.080 ( 0.080)\tLoss 9.8047e-01 (9.8047e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [17][ 30/129]\tTime  0.170 ( 0.208)\tData  0.010 ( 0.015)\tLoss 9.6875e-01 (7.9776e-01)\tAcc@1  43.75 ( 64.31)\n",
      "[xla:5]Train:  Epoch: [17][ 30/129]\tTime  0.173 ( 0.214)\tData  0.011 ( 0.018)\tLoss 7.3438e-01 (8.3455e-01)\tAcc@1  62.50 ( 63.56)\n",
      "[xla:0]Train:  Epoch: [17][ 30/129]\tTime  0.175 ( 0.210)\tData  0.011 ( 0.015)\tLoss 9.9219e-01 (8.7928e-01)\tAcc@1  50.00 ( 58.92)\n",
      "[xla:7]Train:  Epoch: [17][ 30/129]\tTime  0.184 ( 0.213)\tData  0.011 ( 0.018)\tLoss 9.9609e-01 (8.5843e-01)\tAcc@1  50.00 ( 61.94)\n",
      "[xla:4]Train:  Epoch: [17][ 30/129]\tTime  0.175 ( 0.215)\tData  0.012 ( 0.015)\tLoss 6.3672e-01 (8.3972e-01)\tAcc@1  81.00 ( 60.53)\n",
      "[xla:3]Train:  Epoch: [17][ 30/129]\tTime  0.212 ( 0.224)\tData  0.011 ( 0.017)\tLoss 7.3828e-01 (8.4236e-01)\tAcc@1  75.00 ( 60.31)\n",
      "[xla:1]Train:  Epoch: [17][ 30/129]\tTime  0.235 ( 0.214)\tData  0.011 ( 0.017)\tLoss 9.5703e-01 (8.4715e-01)\tAcc@1  56.25 ( 64.36)\n",
      "[xla:2]Train:  Epoch: [17][ 30/129]\tTime  0.225 ( 0.228)\tData  0.011 ( 0.019)\tLoss 7.8906e-01 (8.7966e-01)\tAcc@1  62.50 ( 57.90)\n",
      "[xla:2]Train:  Epoch: [17][ 60/129]\tTime  0.211 ( 0.218)\tData  0.018 ( 0.018)\tLoss 8.5156e-01 (8.4804e-01)\tAcc@1  56.25 ( 60.98)\n",
      "[xla:7]Train:  Epoch: [17][ 60/129]\tTime  0.231 ( 0.212)\tData  0.022 ( 0.018)\tLoss 5.0781e-01 (8.3770e-01)\tAcc@1  87.50 ( 62.13)\n",
      "[xla:6]Train:  Epoch: [17][ 60/129]\tTime  0.188 ( 0.210)\tData  0.011 ( 0.016)\tLoss 9.4922e-01 (8.1673e-01)\tAcc@1  69.00 ( 62.81)\n",
      "[xla:0]Train:  Epoch: [17][ 60/129]\tTime  0.194 ( 0.210)\tData  0.010 ( 0.015)\tLoss 6.8750e-01 (8.3860e-01)\tAcc@1  75.00 ( 62.34)\n",
      "[xla:5]Train:  Epoch: [17][ 60/129]\tTime  0.194 ( 0.213)\tData  0.010 ( 0.017)\tLoss 9.3750e-01 (8.3792e-01)\tAcc@1  50.00 ( 62.03)\n",
      "[xla:4]Train:  Epoch: [17][ 60/129]\tTime  0.229 ( 0.213)\tData  0.015 ( 0.015)\tLoss 9.0625e-01 (8.3184e-01)\tAcc@1  43.75 ( 61.09)\n",
      "[xla:3]Train:  Epoch: [17][ 60/129]\tTime  0.193 ( 0.217)\tData  0.011 ( 0.016)\tLoss 6.0938e-01 (8.2124e-01)\tAcc@1  75.00 ( 62.52)\n",
      "[xla:1]Train:  Epoch: [17][ 60/129]\tTime  0.234 ( 0.212)\tData  0.016 ( 0.017)\tLoss 9.0234e-01 (8.5207e-01)\tAcc@1  50.00 ( 62.95)\n",
      "[xla:4]Train:  Epoch: [17][ 90/129]\tTime  0.187 ( 0.211)\tData  0.012 ( 0.015)\tLoss 9.8438e-01 (8.2946e-01)\tAcc@1  37.50 ( 62.45)\n",
      "[xla:3]Train:  Epoch: [17][ 90/129]\tTime  0.193 ( 0.215)\tData  0.015 ( 0.017)\tLoss 6.8750e-01 (8.0896e-01)\tAcc@1  62.50 ( 62.81)\n",
      "[xla:2]Train:  Epoch: [17][ 90/129]\tTime  0.195 ( 0.216)\tData  0.009 ( 0.017)\tLoss 5.8203e-01 (8.5427e-01)\tAcc@1  81.00 ( 60.60)\n",
      "[xla:5]Train:  Epoch: [17][ 90/129]\tTime  0.228 ( 0.212)\tData  0.012 ( 0.016)\tLoss 8.3203e-01 (8.3611e-01)\tAcc@1  75.00 ( 61.92)\n",
      "[xla:6]Train:  Epoch: [17][ 90/129]\tTime  0.212 ( 0.210)\tData  0.009 ( 0.016)\tLoss 6.1719e-01 (8.1752e-01)\tAcc@1  75.00 ( 62.24)\n",
      "[xla:7]Train:  Epoch: [17][ 90/129]\tTime  0.237 ( 0.211)\tData  0.013 ( 0.017)\tLoss 6.0938e-01 (8.3939e-01)\tAcc@1  69.00 ( 62.00)\n",
      "[xla:0]Train:  Epoch: [17][ 90/129]\tTime  0.215 ( 0.210)\tData  0.009 ( 0.015)\tLoss 1.2812e+00 (8.1540e-01)\tAcc@1  56.25 ( 63.37)\n",
      "[xla:1]Train:  Epoch: [17][ 90/129]\tTime  0.227 ( 0.212)\tData  0.010 ( 0.016)\tLoss 8.9062e-01 (8.4585e-01)\tAcc@1  56.25 ( 63.16)\n",
      "[xla:3]Train:  Epoch: [17][120/129]\tTime  0.194 ( 0.212)\tData  0.024 ( 0.016)\tLoss 5.6250e-01 (7.9292e-01)\tAcc@1  75.00 ( 64.09)\n",
      "[xla:4]Train:  Epoch: [17][120/129]\tTime  0.179 ( 0.210)\tData  0.009 ( 0.015)\tLoss 9.0625e-01 (8.1302e-01)\tAcc@1  69.00 ( 63.87)\n",
      "[xla:6]Train:  Epoch: [17][120/129]\tTime  0.181 ( 0.209)\tData  0.010 ( 0.015)\tLoss 6.4453e-01 (7.9432e-01)\tAcc@1  75.00 ( 63.86)\n",
      "[xla:7]Train:  Epoch: [17][120/129]\tTime  0.178 ( 0.209)\tData  0.009 ( 0.016)\tLoss 8.0469e-01 (8.1219e-01)\tAcc@1  69.00 ( 63.83)\n",
      "[xla:2]Train:  Epoch: [17][120/129]\tTime  0.182 ( 0.213)\tData  0.010 ( 0.016)\tLoss 8.8672e-01 (8.2206e-01)\tAcc@1  62.50 ( 62.53)\n",
      "[xla:1]Train:  Epoch: [17][120/129]\tTime  0.224 ( 0.210)\tData  0.015 ( 0.016)\tLoss 8.2422e-01 (8.1991e-01)\tAcc@1  69.00 ( 63.37)\n",
      "[xla:5]Train:  Epoch: [17][120/129]\tTime  0.227 ( 0.210)\tData  0.013 ( 0.016)\tLoss 3.9062e-01 (8.2577e-01)\tAcc@1  87.50 ( 62.54)\n",
      "[xla:0]Train:  Epoch: [17][120/129]\tTime  0.242 ( 0.209)\tData  0.014 ( 0.015)\tLoss 7.1484e-01 (7.9371e-01)\tAcc@1  62.50 ( 64.36)\n",
      "Finished training epoch 17\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 5.8594e-01 (5.8594e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.098 ( 0.098)\tLoss 8.5938e-01 (8.5938e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.088 ( 0.088)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.088 ( 0.088)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.138 ( 0.138)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [18][  0/129]\tTime  0.218 ( 0.218)\tData  0.072 ( 0.072)\tLoss 9.4922e-01 (9.4922e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [18][  0/129]\tTime  0.180 ( 0.180)\tData  0.039 ( 0.039)\tLoss 8.2031e-01 (8.2031e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Train:  Epoch: [18][  0/129]\tTime  0.289 ( 0.289)\tData  0.079 ( 0.079)\tLoss 8.9844e-01 (8.9844e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [18][  0/129]\tTime  0.314 ( 0.314)\tData  0.121 ( 0.121)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [18][  0/129]\tTime  0.259 ( 0.259)\tData  0.088 ( 0.088)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Train:  Epoch: [18][  0/129]\tTime  0.276 ( 0.276)\tData  0.106 ( 0.106)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Train:  Epoch: [18][  0/129]\tTime  0.317 ( 0.317)\tData  0.118 ( 0.118)\tLoss 8.0469e-01 (8.0469e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Train:  Epoch: [18][  0/129]\tTime  0.217 ( 0.217)\tData  0.067 ( 0.067)\tLoss 9.5312e-01 (9.5312e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [18][ 30/129]\tTime  0.159 ( 0.219)\tData  0.011 ( 0.016)\tLoss 7.5000e-01 (8.7626e-01)\tAcc@1  75.00 ( 59.88)\n",
      "[xla:4]Train:  Epoch: [18][ 30/129]\tTime  0.174 ( 0.213)\tData  0.020 ( 0.021)\tLoss 7.7344e-01 (8.2220e-01)\tAcc@1  69.00 ( 63.15)\n",
      "[xla:1]Train:  Epoch: [18][ 30/129]\tTime  0.196 ( 0.214)\tData  0.010 ( 0.021)\tLoss 9.3750e-01 (8.3518e-01)\tAcc@1  50.00 ( 62.35)\n",
      "[xla:7]Train:  Epoch: [18][ 30/129]\tTime  0.181 ( 0.214)\tData  0.014 ( 0.017)\tLoss 1.0625e+00 (8.8054e-01)\tAcc@1  43.75 ( 59.88)\n",
      "[xla:0]Train:  Epoch: [18][ 30/129]\tTime  0.193 ( 0.216)\tData  0.019 ( 0.016)\tLoss 8.4375e-01 (8.7550e-01)\tAcc@1  56.25 ( 58.69)\n",
      "[xla:5]Train:  Epoch: [18][ 30/129]\tTime  0.192 ( 0.207)\tData  0.010 ( 0.015)\tLoss 6.9531e-01 (8.3833e-01)\tAcc@1  81.00 ( 63.77)\n",
      "[xla:6]Train:  Epoch: [18][ 30/129]\tTime  0.222 ( 0.213)\tData  0.020 ( 0.019)\tLoss 9.1797e-01 (8.1300e-01)\tAcc@1  50.00 ( 64.95)\n",
      "[xla:3]Train:  Epoch: [18][ 30/129]\tTime  0.237 ( 0.213)\tData  0.011 ( 0.018)\tLoss 7.6172e-01 (8.4753e-01)\tAcc@1  75.00 ( 62.94)\n",
      "[xla:5]Train:  Epoch: [18][ 60/129]\tTime  0.187 ( 0.212)\tData  0.012 ( 0.016)\tLoss 9.8438e-01 (8.4260e-01)\tAcc@1  62.50 ( 61.85)\n",
      "[xla:4]Train:  Epoch: [18][ 60/129]\tTime  0.177 ( 0.216)\tData  0.010 ( 0.017)\tLoss 8.6719e-01 (8.1698e-01)\tAcc@1  62.50 ( 63.76)\n",
      "[xla:6]Train:  Epoch: [18][ 60/129]\tTime  0.191 ( 0.215)\tData  0.010 ( 0.017)\tLoss 9.3359e-01 (8.2396e-01)\tAcc@1  56.25 ( 62.95)\n",
      "[xla:1]Train:  Epoch: [18][ 60/129]\tTime  0.189 ( 0.216)\tData  0.011 ( 0.019)\tLoss 8.2812e-01 (8.5387e-01)\tAcc@1  62.50 ( 61.81)\n",
      "[xla:3]Train:  Epoch: [18][ 60/129]\tTime  0.187 ( 0.215)\tData  0.011 ( 0.017)\tLoss 5.3125e-01 (8.2659e-01)\tAcc@1  87.50 ( 64.27)\n",
      "[xla:7]Train:  Epoch: [18][ 60/129]\tTime  0.194 ( 0.216)\tData  0.011 ( 0.017)\tLoss 5.8594e-01 (8.5464e-01)\tAcc@1  75.00 ( 60.06)\n",
      "[xla:0]Train:  Epoch: [18][ 60/129]\tTime  0.203 ( 0.217)\tData  0.012 ( 0.015)\tLoss 6.4844e-01 (8.3629e-01)\tAcc@1  75.00 ( 61.29)\n",
      "[xla:2]Train:  Epoch: [18][ 60/129]\tTime  0.209 ( 0.219)\tData  0.017 ( 0.015)\tLoss 1.0156e+00 (8.3357e-01)\tAcc@1  62.50 ( 62.39)\n",
      "[xla:4]Train:  Epoch: [18][ 90/129]\tTime  0.199 ( 0.218)\tData  0.013 ( 0.017)\tLoss 1.0156e+00 (8.2147e-01)\tAcc@1  43.75 ( 64.04)\n",
      "[xla:2]Train:  Epoch: [18][ 90/129]\tTime  0.212 ( 0.220)\tData  0.012 ( 0.015)\tLoss 5.7812e-01 (8.4534e-01)\tAcc@1  75.00 ( 62.44)\n",
      "[xla:3]Train:  Epoch: [18][ 90/129]\tTime  0.204 ( 0.217)\tData  0.012 ( 0.017)\tLoss 5.8594e-01 (8.0834e-01)\tAcc@1  69.00 ( 64.45)\n",
      "[xla:0]Train:  Epoch: [18][ 90/129]\tTime  0.193 ( 0.218)\tData  0.009 ( 0.015)\tLoss 1.2109e+00 (8.1660e-01)\tAcc@1  50.00 ( 63.00)\n",
      "[xla:1]Train:  Epoch: [18][ 90/129]\tTime  0.184 ( 0.218)\tData  0.008 ( 0.017)\tLoss 5.0781e-01 (8.3008e-01)\tAcc@1  81.00 ( 63.49)\n",
      "[xla:6]Train:  Epoch: [18][ 90/129]\tTime  0.200 ( 0.217)\tData  0.008 ( 0.016)\tLoss 7.0703e-01 (8.1533e-01)\tAcc@1  62.50 ( 63.35)\n",
      "[xla:7]Train:  Epoch: [18][ 90/129]\tTime  0.221 ( 0.218)\tData  0.012 ( 0.017)\tLoss 6.7969e-01 (8.5427e-01)\tAcc@1  69.00 ( 60.31)\n",
      "[xla:5]Train:  Epoch: [18][ 90/129]\tTime  0.250 ( 0.215)\tData  0.012 ( 0.016)\tLoss 9.1016e-01 (8.5152e-01)\tAcc@1  62.50 ( 61.18)\n",
      "[xla:4]Train:  Epoch: [18][120/129]\tTime  0.185 ( 0.215)\tData  0.011 ( 0.016)\tLoss 9.0234e-01 (8.0143e-01)\tAcc@1  56.25 ( 65.26)\n",
      "[xla:3]Train:  Epoch: [18][120/129]\tTime  0.202 ( 0.214)\tData  0.012 ( 0.016)\tLoss 5.5078e-01 (7.8964e-01)\tAcc@1  69.00 ( 65.63)\n",
      "[xla:7]Train:  Epoch: [18][120/129]\tTime  0.182 ( 0.215)\tData  0.011 ( 0.017)\tLoss 8.7500e-01 (8.3436e-01)\tAcc@1  62.50 ( 62.10)\n",
      "[xla:5]Train:  Epoch: [18][120/129]\tTime  0.181 ( 0.213)\tData  0.011 ( 0.016)\tLoss 3.8086e-01 (8.2932e-01)\tAcc@1  94.00 ( 63.00)\n",
      "[xla:0]Train:  Epoch: [18][120/129]\tTime  0.207 ( 0.215)\tData  0.011 ( 0.015)\tLoss 8.7109e-01 (7.9200e-01)\tAcc@1  62.50 ( 64.74)\n",
      "[xla:1]Train:  Epoch: [18][120/129]\tTime  0.197 ( 0.215)\tData  0.012 ( 0.017)\tLoss 7.7344e-01 (8.1219e-01)\tAcc@1  62.50 ( 64.38)\n",
      "[xla:6]Train:  Epoch: [18][120/129]\tTime  0.200 ( 0.215)\tData  0.011 ( 0.016)\tLoss 5.0391e-01 (7.9189e-01)\tAcc@1  75.00 ( 64.64)\n",
      "[xla:2]Train:  Epoch: [18][120/129]\tTime  0.211 ( 0.217)\tData  0.012 ( 0.015)\tLoss 8.9062e-01 (8.1562e-01)\tAcc@1  56.25 ( 63.91)\n",
      "[xla:7]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.053 ( 0.053)\tLoss 6.2109e-01 (6.2109e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Finished training epoch 18\n",
      "[xla:4]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 8.2812e-01 (8.2812e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 1.2656e+00 (1.2656e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.116 ( 0.116)\tLoss 7.2656e-01 (7.2656e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [19][  0/129]\tTime  0.286 ( 0.286)\tData  0.101 ( 0.101)\tLoss 8.9844e-01 (8.9844e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [19][  0/129]\tTime  0.195 ( 0.195)\tData  0.044 ( 0.044)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [19][  0/129]\tTime  0.218 ( 0.218)\tData  0.043 ( 0.043)\tLoss 8.0469e-01 (8.0469e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [19][  0/129]\tTime  0.210 ( 0.210)\tData  0.038 ( 0.038)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [19][  0/129]\tTime  0.259 ( 0.259)\tData  0.075 ( 0.075)\tLoss 5.8984e-01 (5.8984e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Train:  Epoch: [19][  0/129]\tTime  0.242 ( 0.242)\tData  0.101 ( 0.101)\tLoss 9.8047e-01 (9.8047e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Train:  Epoch: [19][  0/129]\tTime  0.291 ( 0.291)\tData  0.070 ( 0.070)\tLoss 8.6719e-01 (8.6719e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [19][  0/129]\tTime  0.259 ( 0.259)\tData  0.083 ( 0.083)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [19][ 30/129]\tTime  0.186 ( 0.211)\tData  0.011 ( 0.016)\tLoss 9.4141e-01 (8.9554e-01)\tAcc@1  56.25 ( 57.70)\n",
      "[xla:1]Train:  Epoch: [19][ 30/129]\tTime  0.197 ( 0.208)\tData  0.012 ( 0.017)\tLoss 9.5312e-01 (8.2390e-01)\tAcc@1  50.00 ( 63.55)\n",
      "[xla:6]Train:  Epoch: [19][ 30/129]\tTime  0.189 ( 0.208)\tData  0.015 ( 0.018)\tLoss 7.2266e-01 (7.6802e-01)\tAcc@1  75.00 ( 65.16)\n",
      "[xla:2]Train:  Epoch: [19][ 30/129]\tTime  0.195 ( 0.209)\tData  0.013 ( 0.016)\tLoss 7.9688e-01 (8.5005e-01)\tAcc@1  69.00 ( 60.52)\n",
      "[xla:3]Train:  Epoch: [19][ 30/129]\tTime  0.209 ( 0.210)\tData  0.013 ( 0.017)\tLoss 8.9062e-01 (8.3695e-01)\tAcc@1  62.50 ( 61.33)\n",
      "[xla:5]Train:  Epoch: [19][ 30/129]\tTime  0.181 ( 0.218)\tData  0.011 ( 0.018)\tLoss 7.0312e-01 (8.6265e-01)\tAcc@1  81.00 ( 62.14)\n",
      "[xla:7]Train:  Epoch: [19][ 30/129]\tTime  0.173 ( 0.223)\tData  0.009 ( 0.018)\tLoss 9.3750e-01 (8.6240e-01)\tAcc@1  56.25 ( 61.91)\n",
      "[xla:4]Train:  Epoch: [19][ 30/129]\tTime  0.208 ( 0.210)\tData  0.011 ( 0.015)\tLoss 6.8750e-01 (8.3632e-01)\tAcc@1  75.00 ( 61.72)\n",
      "[xla:3]Train:  Epoch: [19][ 60/129]\tTime  0.188 ( 0.210)\tData  0.015 ( 0.017)\tLoss 6.5625e-01 (8.1615e-01)\tAcc@1  69.00 ( 62.75)\n",
      "[xla:6]Train:  Epoch: [19][ 60/129]\tTime  0.214 ( 0.209)\tData  0.009 ( 0.016)\tLoss 8.9062e-01 (7.9944e-01)\tAcc@1  62.50 ( 64.68)\n",
      "[xla:4]Train:  Epoch: [19][ 60/129]\tTime  0.201 ( 0.209)\tData  0.011 ( 0.016)\tLoss 1.0000e+00 (8.1455e-01)\tAcc@1  43.75 ( 62.93)\n",
      "[xla:7]Train:  Epoch: [19][ 60/129]\tTime  0.202 ( 0.217)\tData  0.012 ( 0.017)\tLoss 5.0391e-01 (8.3280e-01)\tAcc@1  87.50 ( 62.75)\n",
      "[xla:5]Train:  Epoch: [19][ 60/129]\tTime  0.205 ( 0.214)\tData  0.013 ( 0.016)\tLoss 1.0547e+00 (8.5861e-01)\tAcc@1  50.00 ( 61.51)\n",
      "[xla:2]Train:  Epoch: [19][ 60/129]\tTime  0.193 ( 0.209)\tData  0.010 ( 0.017)\tLoss 9.6875e-01 (8.4004e-01)\tAcc@1  56.25 ( 61.40)\n",
      "[xla:1]Train:  Epoch: [19][ 60/129]\tTime  0.206 ( 0.209)\tData  0.012 ( 0.016)\tLoss 8.9453e-01 (8.3232e-01)\tAcc@1  50.00 ( 63.27)\n",
      "[xla:0]Train:  Epoch: [19][ 60/129]\tTime  0.190 ( 0.211)\tData  0.009 ( 0.016)\tLoss 6.6406e-01 (8.4119e-01)\tAcc@1  81.00 ( 60.99)\n",
      "[xla:6]Train:  Epoch: [19][ 90/129]\tTime  0.165 ( 0.207)\tData  0.011 ( 0.015)\tLoss 5.8984e-01 (8.0155e-01)\tAcc@1  81.00 ( 64.87)\n",
      "[xla:4]Train:  Epoch: [19][ 90/129]\tTime  0.190 ( 0.208)\tData  0.011 ( 0.016)\tLoss 1.0312e+00 (8.1812e-01)\tAcc@1  43.75 ( 63.61)\n",
      "[xla:0]Train:  Epoch: [19][ 90/129]\tTime  0.196 ( 0.209)\tData  0.017 ( 0.016)\tLoss 1.0703e+00 (8.2649e-01)\tAcc@1  43.75 ( 61.15)\n",
      "[xla:1]Train:  Epoch: [19][ 90/129]\tTime  0.188 ( 0.208)\tData  0.011 ( 0.016)\tLoss 6.2109e-01 (8.1600e-01)\tAcc@1  75.00 ( 64.00)\n",
      "[xla:3]Train:  Epoch: [19][ 90/129]\tTime  0.193 ( 0.209)\tData  0.010 ( 0.016)\tLoss 5.8984e-01 (8.0074e-01)\tAcc@1  69.00 ( 63.52)\n",
      "[xla:5]Train:  Epoch: [19][ 90/129]\tTime  0.193 ( 0.211)\tData  0.011 ( 0.016)\tLoss 8.1250e-01 (8.5092e-01)\tAcc@1  69.00 ( 62.13)\n",
      "[xla:2]Train:  Epoch: [19][ 90/129]\tTime  0.209 ( 0.208)\tData  0.012 ( 0.016)\tLoss 6.1328e-01 (8.4680e-01)\tAcc@1  75.00 ( 61.37)\n",
      "[xla:7]Train:  Epoch: [19][ 90/129]\tTime  0.214 ( 0.213)\tData  0.011 ( 0.016)\tLoss 6.8750e-01 (8.3474e-01)\tAcc@1  62.50 ( 61.85)\n",
      "[xla:0]Train:  Epoch: [19][120/129]\tTime  0.176 ( 0.207)\tData  0.012 ( 0.016)\tLoss 8.0469e-01 (7.9637e-01)\tAcc@1  69.00 ( 63.60)\n",
      "[xla:5]Train:  Epoch: [19][120/129]\tTime  0.168 ( 0.209)\tData  0.011 ( 0.016)\tLoss 4.1602e-01 (8.3988e-01)\tAcc@1  87.50 ( 62.21)\n",
      "[xla:6]Train:  Epoch: [19][120/129]\tTime  0.171 ( 0.207)\tData  0.011 ( 0.014)\tLoss 4.8438e-01 (7.8685e-01)\tAcc@1  75.00 ( 66.10)\n",
      "[xla:2]Train:  Epoch: [19][120/129]\tTime  0.193 ( 0.207)\tData  0.012 ( 0.015)\tLoss 7.0703e-01 (8.1428e-01)\tAcc@1  62.50 ( 63.36)\n",
      "[xla:7]Train:  Epoch: [19][120/129]\tTime  0.208 ( 0.211)\tData  0.011 ( 0.016)\tLoss 7.8125e-01 (8.1905e-01)\tAcc@1  75.00 ( 63.01)\n",
      "[xla:4]Train:  Epoch: [19][120/129]\tTime  0.182 ( 0.207)\tData  0.009 ( 0.015)\tLoss 8.7891e-01 (7.9991e-01)\tAcc@1  69.00 ( 65.05)\n",
      "[xla:1]Train:  Epoch: [19][120/129]\tTime  0.206 ( 0.207)\tData  0.020 ( 0.016)\tLoss 6.4062e-01 (7.9602e-01)\tAcc@1  69.00 ( 64.20)\n",
      "[xla:3]Train:  Epoch: [19][120/129]\tTime  0.217 ( 0.208)\tData  0.016 ( 0.015)\tLoss 4.4336e-01 (7.8642e-01)\tAcc@1  87.50 ( 64.30)\n",
      "[xla:5]Validation: [0/6]\tTime  0.087 ( 0.087)\tLoss 5.9375e-01 (5.9375e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Finished training epoch 19\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 8.4766e-01 (8.4766e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Validation: [0/6]\tTime  0.042 ( 0.042)\tLoss 5.8984e-01 (5.8984e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.072 ( 0.072)\tLoss 7.2266e-01 (7.2266e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.083 ( 0.083)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Validation: [0/6]\tTime  0.076 ( 0.076)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.069 ( 0.069)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.072 ( 0.072)\tLoss 1.2500e+00 (1.2500e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [20][  0/129]\tTime  0.209 ( 0.209)\tData  0.043 ( 0.043)\tLoss 9.6094e-01 (9.6094e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [20][  0/129]\tTime  0.203 ( 0.203)\tData  0.065 ( 0.065)\tLoss 8.1641e-01 (8.1641e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [20][  0/129]\tTime  0.197 ( 0.197)\tData  0.037 ( 0.037)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Train:  Epoch: [20][  0/129]\tTime  0.217 ( 0.217)\tData  0.047 ( 0.047)\tLoss 7.5000e-01 (7.5000e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Train:  Epoch: [20][  0/129]\tTime  0.245 ( 0.245)\tData  0.092 ( 0.092)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [20][  0/129]\tTime  0.272 ( 0.272)\tData  0.095 ( 0.095)\tLoss 5.5469e-01 (5.5469e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Train:  Epoch: [20][  0/129]\tTime  0.198 ( 0.198)\tData  0.043 ( 0.043)\tLoss 8.7109e-01 (8.7109e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Train:  Epoch: [20][  0/129]\tTime  0.211 ( 0.211)\tData  0.077 ( 0.077)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Train:  Epoch: [20][ 30/129]\tTime  0.202 ( 0.216)\tData  0.010 ( 0.015)\tLoss 6.4844e-01 (8.6391e-01)\tAcc@1  62.50 ( 60.49)\n",
      "[xla:1]Train:  Epoch: [20][ 30/129]\tTime  0.192 ( 0.203)\tData  0.013 ( 0.016)\tLoss 9.7266e-01 (8.1433e-01)\tAcc@1  50.00 ( 63.13)\n",
      "[xla:3]Train:  Epoch: [20][ 30/129]\tTime  0.195 ( 0.208)\tData  0.011 ( 0.017)\tLoss 8.4375e-01 (8.6316e-01)\tAcc@1  56.25 ( 60.48)\n",
      "[xla:2]Train:  Epoch: [20][ 30/129]\tTime  0.225 ( 0.215)\tData  0.013 ( 0.016)\tLoss 7.9688e-01 (8.5648e-01)\tAcc@1  50.00 ( 59.48)\n",
      "[xla:0]Train:  Epoch: [20][ 30/129]\tTime  0.206 ( 0.210)\tData  0.015 ( 0.017)\tLoss 9.8047e-01 (8.7651e-01)\tAcc@1  50.00 ( 60.90)\n",
      "[xla:6]Train:  Epoch: [20][ 30/129]\tTime  0.223 ( 0.210)\tData  0.011 ( 0.016)\tLoss 9.6875e-01 (8.0645e-01)\tAcc@1  50.00 ( 64.18)\n",
      "[xla:4]Train:  Epoch: [20][ 30/129]\tTime  0.196 ( 0.209)\tData  0.008 ( 0.018)\tLoss 6.3672e-01 (8.1905e-01)\tAcc@1  75.00 ( 62.99)\n",
      "[xla:7]Train:  Epoch: [20][ 30/129]\tTime  0.242 ( 0.204)\tData  0.015 ( 0.015)\tLoss 9.9609e-01 (8.6038e-01)\tAcc@1  56.25 ( 60.73)\n",
      "[xla:0]Train:  Epoch: [20][ 60/129]\tTime  0.235 ( 0.210)\tData  0.040 ( 0.016)\tLoss 7.4609e-01 (8.3876e-01)\tAcc@1  62.50 ( 62.31)\n",
      "[xla:5]Train:  Epoch: [20][ 60/129]\tTime  0.307 ( 0.215)\tData  0.016 ( 0.015)\tLoss 1.1016e+00 (8.5605e-01)\tAcc@1  43.75 ( 61.20)\n",
      "[xla:4]Train:  Epoch: [20][ 60/129]\tTime  0.247 ( 0.211)\tData  0.011 ( 0.017)\tLoss 8.4766e-01 (8.0702e-01)\tAcc@1  56.25 ( 64.60)\n",
      "[xla:6]Train:  Epoch: [20][ 60/129]\tTime  0.240 ( 0.211)\tData  0.012 ( 0.017)\tLoss 1.0078e+00 (8.1737e-01)\tAcc@1  43.75 ( 63.27)\n",
      "[xla:1]Train:  Epoch: [20][ 60/129]\tTime  0.301 ( 0.208)\tData  0.041 ( 0.015)\tLoss 9.4922e-01 (8.3623e-01)\tAcc@1  43.75 ( 61.92)\n",
      "[xla:3]Train:  Epoch: [20][ 60/129]\tTime  0.287 ( 0.211)\tData  0.050 ( 0.016)\tLoss 6.4062e-01 (8.2518e-01)\tAcc@1  69.00 ( 62.22)\n",
      "[xla:2]Train:  Epoch: [20][ 60/129]\tTime  0.323 ( 0.214)\tData  0.059 ( 0.017)\tLoss 8.8672e-01 (8.3245e-01)\tAcc@1  62.50 ( 62.21)\n",
      "[xla:7]Train:  Epoch: [20][ 60/129]\tTime  0.250 ( 0.209)\tData  0.011 ( 0.015)\tLoss 5.1562e-01 (8.3876e-01)\tAcc@1  87.50 ( 62.75)\n",
      "[xla:7]Train:  Epoch: [20][ 90/129]\tTime  0.119 ( 0.212)\tData  0.010 ( 0.015)\tLoss 7.7734e-01 (8.3461e-01)\tAcc@1  75.00 ( 62.13)\n",
      "[xla:1]Train:  Epoch: [20][ 90/129]\tTime  0.209 ( 0.213)\tData  0.012 ( 0.015)\tLoss 7.5391e-01 (8.2205e-01)\tAcc@1  81.00 ( 63.29)\n",
      "[xla:5]Train:  Epoch: [20][ 90/129]\tTime  0.217 ( 0.218)\tData  0.014 ( 0.014)\tLoss 7.1094e-01 (8.5203e-01)\tAcc@1  69.00 ( 61.44)\n",
      "[xla:4]Train:  Epoch: [20][ 90/129]\tTime  0.262 ( 0.215)\tData  0.015 ( 0.016)\tLoss 1.0938e+00 (8.1452e-01)\tAcc@1  43.75 ( 64.33)\n",
      "[xla:2]Train:  Epoch: [20][ 90/129]\tTime  0.270 ( 0.217)\tData  0.021 ( 0.017)\tLoss 5.7422e-01 (8.3772e-01)\tAcc@1  81.00 ( 62.46)\n",
      "[xla:6]Train:  Epoch: [20][ 90/129]\tTime  0.220 ( 0.215)\tData  0.008 ( 0.016)\tLoss 5.5078e-01 (8.1040e-01)\tAcc@1  75.00 ( 62.88)\n",
      "[xla:3]Train:  Epoch: [20][ 90/129]\tTime  0.229 ( 0.215)\tData  0.010 ( 0.016)\tLoss 7.7734e-01 (8.0889e-01)\tAcc@1  69.00 ( 63.17)\n",
      "[xla:0]Train:  Epoch: [20][ 90/129]\tTime  0.228 ( 0.216)\tData  0.009 ( 0.016)\tLoss 1.1484e+00 (8.0679e-01)\tAcc@1  56.25 ( 63.77)\n",
      "[xla:0]Train:  Epoch: [20][120/129]\tTime  0.163 ( 0.213)\tData  0.014 ( 0.015)\tLoss 8.5156e-01 (7.8211e-01)\tAcc@1  62.50 ( 65.32)\n",
      "[xla:1]Train:  Epoch: [20][120/129]\tTime  0.240 ( 0.212)\tData  0.012 ( 0.015)\tLoss 8.2812e-01 (7.9968e-01)\tAcc@1  56.25 ( 64.39)\n",
      "[xla:3]Train:  Epoch: [20][120/129]\tTime  0.210 ( 0.213)\tData  0.013 ( 0.016)\tLoss 4.8828e-01 (7.8719e-01)\tAcc@1  81.00 ( 64.26)\n",
      "[xla:5]Train:  Epoch: [20][120/129]\tTime  0.177 ( 0.215)\tData  0.008 ( 0.015)\tLoss 4.1016e-01 (8.3169e-01)\tAcc@1  81.00 ( 62.43)\n",
      "[xla:7]Train:  Epoch: [20][120/129]\tTime  0.213 ( 0.212)\tData  0.011 ( 0.015)\tLoss 6.5234e-01 (8.0911e-01)\tAcc@1  75.00 ( 63.92)\n",
      "[xla:6]Train:  Epoch: [20][120/129]\tTime  0.210 ( 0.213)\tData  0.015 ( 0.016)\tLoss 4.8242e-01 (7.8750e-01)\tAcc@1  75.00 ( 64.10)\n",
      "[xla:2]Train:  Epoch: [20][120/129]\tTime  0.220 ( 0.215)\tData  0.012 ( 0.016)\tLoss 8.3203e-01 (8.0407e-01)\tAcc@1  62.50 ( 64.55)\n",
      "[xla:4]Train:  Epoch: [20][120/129]\tTime  0.213 ( 0.213)\tData  0.009 ( 0.015)\tLoss 8.7500e-01 (7.9445e-01)\tAcc@1  69.00 ( 65.53)\n",
      "Finished training epoch 20\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:4]Validation: [0/6]\tTime  0.059 ( 0.059)\tLoss 1.2109e+00 (1.2109e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.104 ( 0.104)\tLoss 8.3203e-01 (8.3203e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Validation: [0/6]\tTime  0.096 ( 0.096)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/6]\tTime  0.110 ( 0.110)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Validation: [0/6]\tTime  0.096 ( 0.096)\tLoss 1.2969e+00 (1.2969e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [21][  0/129]\tTime  0.175 ( 0.175)\tData  0.067 ( 0.067)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [21][  0/129]\tTime  0.197 ( 0.197)\tData  0.062 ( 0.062)\tLoss 7.5000e-01 (7.5000e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [21][  0/129]\tTime  0.181 ( 0.181)\tData  0.063 ( 0.063)\tLoss 6.6406e-01 (6.6406e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Train:  Epoch: [21][  0/129]\tTime  0.219 ( 0.219)\tData  0.047 ( 0.047)\tLoss 9.4922e-01 (9.4922e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [21][  0/129]\tTime  0.244 ( 0.244)\tData  0.038 ( 0.038)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [21][  0/129]\tTime  0.189 ( 0.189)\tData  0.049 ( 0.049)\tLoss 7.8516e-01 (7.8516e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [21][  0/129]\tTime  0.166 ( 0.166)\tData  0.048 ( 0.048)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Train:  Epoch: [21][  0/129]\tTime  0.180 ( 0.180)\tData  0.034 ( 0.034)\tLoss 7.6562e-01 (7.6562e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [21][ 30/129]\tTime  0.167 ( 0.209)\tData  0.010 ( 0.015)\tLoss 7.7734e-01 (8.6681e-01)\tAcc@1  69.00 ( 58.28)\n",
      "[xla:1]Train:  Epoch: [21][ 30/129]\tTime  0.175 ( 0.203)\tData  0.012 ( 0.015)\tLoss 1.0391e+00 (8.5610e-01)\tAcc@1  56.25 ( 63.94)\n",
      "[xla:4]Train:  Epoch: [21][ 30/129]\tTime  0.199 ( 0.226)\tData  0.012 ( 0.017)\tLoss 7.3047e-01 (8.2737e-01)\tAcc@1  75.00 ( 62.12)\n",
      "[xla:7]Train:  Epoch: [21][ 30/129]\tTime  0.196 ( 0.207)\tData  0.011 ( 0.015)\tLoss 9.6094e-01 (8.6946e-01)\tAcc@1  69.00 ( 62.31)\n",
      "[xla:3]Train:  Epoch: [21][ 30/129]\tTime  0.186 ( 0.214)\tData  0.011 ( 0.018)\tLoss 8.3984e-01 (8.3871e-01)\tAcc@1  69.00 ( 61.75)\n",
      "[xla:6]Train:  Epoch: [21][ 30/129]\tTime  0.207 ( 0.213)\tData  0.012 ( 0.017)\tLoss 8.0078e-01 (7.8982e-01)\tAcc@1  62.50 ( 64.33)\n",
      "[xla:0]Train:  Epoch: [21][ 30/129]\tTime  0.194 ( 0.222)\tData  0.008 ( 0.018)\tLoss 8.6328e-01 (8.6946e-01)\tAcc@1  56.25 ( 59.31)\n",
      "[xla:5]Train:  Epoch: [21][ 30/129]\tTime  0.242 ( 0.213)\tData  0.016 ( 0.016)\tLoss 8.1250e-01 (8.1748e-01)\tAcc@1  56.25 ( 64.94)\n",
      "[xla:2]Train:  Epoch: [21][ 60/129]\tTime  0.147 ( 0.207)\tData  0.012 ( 0.015)\tLoss 1.0078e+00 (8.3648e-01)\tAcc@1  43.75 ( 61.00)\n",
      "[xla:0]Train:  Epoch: [21][ 60/129]\tTime  0.202 ( 0.214)\tData  0.012 ( 0.016)\tLoss 7.0703e-01 (8.1634e-01)\tAcc@1  75.00 ( 63.23)\n",
      "[xla:3]Train:  Epoch: [21][ 60/129]\tTime  0.206 ( 0.210)\tData  0.011 ( 0.017)\tLoss 6.9531e-01 (8.1749e-01)\tAcc@1  75.00 ( 62.65)\n",
      "[xla:1]Train:  Epoch: [21][ 60/129]\tTime  0.169 ( 0.205)\tData  0.010 ( 0.015)\tLoss 9.8438e-01 (8.5659e-01)\tAcc@1  37.50 ( 62.22)\n",
      "[xla:6]Train:  Epoch: [21][ 60/129]\tTime  0.211 ( 0.209)\tData  0.011 ( 0.016)\tLoss 8.3594e-01 (8.0757e-01)\tAcc@1  56.25 ( 62.93)\n",
      "[xla:7]Train:  Epoch: [21][ 60/129]\tTime  0.191 ( 0.207)\tData  0.011 ( 0.015)\tLoss 5.0391e-01 (8.3991e-01)\tAcc@1  87.50 ( 62.41)\n",
      "[xla:4]Train:  Epoch: [21][ 60/129]\tTime  0.180 ( 0.216)\tData  0.012 ( 0.016)\tLoss 8.8281e-01 (8.1461e-01)\tAcc@1  37.50 ( 62.44)\n",
      "[xla:5]Train:  Epoch: [21][ 60/129]\tTime  0.203 ( 0.209)\tData  0.009 ( 0.014)\tLoss 9.0625e-01 (8.3296e-01)\tAcc@1  56.25 ( 63.37)\n",
      "[xla:1]Train:  Epoch: [21][ 90/129]\tTime  0.165 ( 0.204)\tData  0.012 ( 0.015)\tLoss 6.0156e-01 (8.3669e-01)\tAcc@1  75.00 ( 63.15)\n",
      "[xla:7]Train:  Epoch: [21][ 90/129]\tTime  0.201 ( 0.206)\tData  0.012 ( 0.014)\tLoss 5.4297e-01 (8.3847e-01)\tAcc@1  69.00 ( 62.59)\n",
      "[xla:2]Train:  Epoch: [21][ 90/129]\tTime  0.195 ( 0.206)\tData  0.009 ( 0.014)\tLoss 7.5781e-01 (8.4905e-01)\tAcc@1  62.50 ( 61.02)\n",
      "[xla:5]Train:  Epoch: [21][ 90/129]\tTime  0.203 ( 0.207)\tData  0.011 ( 0.014)\tLoss 6.8359e-01 (8.3025e-01)\tAcc@1  81.00 ( 63.30)\n",
      "[xla:3]Train:  Epoch: [21][ 90/129]\tTime  0.177 ( 0.208)\tData  0.008 ( 0.016)\tLoss 6.9141e-01 (8.0445e-01)\tAcc@1  69.00 ( 63.02)\n",
      "[xla:6]Train:  Epoch: [21][ 90/129]\tTime  0.203 ( 0.208)\tData  0.010 ( 0.016)\tLoss 6.2891e-01 (8.1641e-01)\tAcc@1  69.00 ( 62.95)\n",
      "[xla:0]Train:  Epoch: [21][ 90/129]\tTime  0.206 ( 0.211)\tData  0.011 ( 0.015)\tLoss 1.0547e+00 (8.0115e-01)\tAcc@1  50.00 ( 64.25)\n",
      "[xla:4]Train:  Epoch: [21][ 90/129]\tTime  0.220 ( 0.212)\tData  0.010 ( 0.015)\tLoss 1.0703e+00 (8.1860e-01)\tAcc@1  50.00 ( 62.81)\n",
      "[xla:2]Train:  Epoch: [21][120/129]\tTime  0.178 ( 0.205)\tData  0.011 ( 0.014)\tLoss 8.3203e-01 (8.1812e-01)\tAcc@1  69.00 ( 63.17)\n",
      "[xla:3]Train:  Epoch: [21][120/129]\tTime  0.195 ( 0.207)\tData  0.011 ( 0.016)\tLoss 3.9648e-01 (7.8109e-01)\tAcc@1  94.00 ( 65.01)\n",
      "[xla:6]Train:  Epoch: [21][120/129]\tTime  0.191 ( 0.206)\tData  0.011 ( 0.015)\tLoss 4.8047e-01 (7.8819e-01)\tAcc@1  81.00 ( 65.23)\n",
      "[xla:1]Train:  Epoch: [21][120/129]\tTime  0.187 ( 0.204)\tData  0.012 ( 0.015)\tLoss 8.7891e-01 (8.0963e-01)\tAcc@1  62.50 ( 63.60)\n",
      "[xla:4]Train:  Epoch: [21][120/129]\tTime  0.221 ( 0.210)\tData  0.012 ( 0.015)\tLoss 7.4609e-01 (7.9991e-01)\tAcc@1  69.00 ( 64.35)\n",
      "[xla:7]Train:  Epoch: [21][120/129]\tTime  0.184 ( 0.205)\tData  0.009 ( 0.014)\tLoss 7.8125e-01 (8.1495e-01)\tAcc@1  75.00 ( 63.61)\n",
      "[xla:5]Train:  Epoch: [21][120/129]\tTime  0.187 ( 0.206)\tData  0.010 ( 0.014)\tLoss 3.4180e-01 (8.1473e-01)\tAcc@1  87.50 ( 63.78)\n",
      "[xla:0]Train:  Epoch: [21][120/129]\tTime  0.205 ( 0.209)\tData  0.011 ( 0.015)\tLoss 7.6953e-01 (7.7345e-01)\tAcc@1  69.00 ( 66.15)\n",
      "[xla:6]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.065 ( 0.065)\tLoss 8.3203e-01 (8.3203e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.071 ( 0.071)\tLoss 1.3203e+00 (1.3203e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Finished training epoch 21\n",
      "[xla:5]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 6.2109e-01 (6.2109e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:4]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.113 ( 0.113)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 7.1094e-01 (7.1094e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 5.7031e-01 (5.7031e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Train:  Epoch: [22][  0/129]\tTime  0.235 ( 0.235)\tData  0.081 ( 0.081)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [22][  0/129]\tTime  0.244 ( 0.244)\tData  0.085 ( 0.085)\tLoss 9.4531e-01 (9.4531e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [22][  0/129]\tTime  0.250 ( 0.250)\tData  0.085 ( 0.085)\tLoss 7.9688e-01 (7.9688e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:4]Train:  Epoch: [22][  0/129]\tTime  0.176 ( 0.176)\tData  0.037 ( 0.037)\tLoss 9.2969e-01 (9.2969e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [22][  0/129]\tTime  0.251 ( 0.251)\tData  0.095 ( 0.095)\tLoss 8.5938e-01 (8.5938e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [22][  0/129]\tTime  0.174 ( 0.174)\tData  0.017 ( 0.017)\tLoss 6.4844e-01 (6.4844e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [22][  0/129]\tTime  0.174 ( 0.174)\tData  0.030 ( 0.030)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Train:  Epoch: [22][  0/129]\tTime  0.180 ( 0.180)\tData  0.025 ( 0.025)\tLoss 8.9844e-01 (8.9844e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [22][ 30/129]\tTime  0.169 ( 0.212)\tData  0.010 ( 0.018)\tLoss 5.5859e-01 (8.4350e-01)\tAcc@1  81.00 ( 61.91)\n",
      "[xla:1]Train:  Epoch: [22][ 30/129]\tTime  0.206 ( 0.215)\tData  0.017 ( 0.020)\tLoss 8.9453e-01 (8.1074e-01)\tAcc@1  62.50 ( 62.31)\n",
      "[xla:6]Train:  Epoch: [22][ 30/129]\tTime  0.216 ( 0.224)\tData  0.012 ( 0.017)\tLoss 1.0312e+00 (8.1212e-01)\tAcc@1  37.50 ( 62.72)\n",
      "[xla:2]Train:  Epoch: [22][ 30/129]\tTime  0.186 ( 0.216)\tData  0.012 ( 0.019)\tLoss 7.6562e-01 (8.7009e-01)\tAcc@1  62.50 ( 58.89)\n",
      "[xla:3]Train:  Epoch: [22][ 30/129]\tTime  0.177 ( 0.203)\tData  0.011 ( 0.014)\tLoss 7.8516e-01 (8.5131e-01)\tAcc@1  69.00 ( 60.74)\n",
      "[xla:7]Train:  Epoch: [22][ 30/129]\tTime  0.192 ( 0.204)\tData  0.009 ( 0.015)\tLoss 1.0156e+00 (8.7374e-01)\tAcc@1  43.75 ( 59.90)\n",
      "[xla:0]Train:  Epoch: [22][ 30/129]\tTime  0.211 ( 0.205)\tData  0.011 ( 0.014)\tLoss 9.8438e-01 (8.7160e-01)\tAcc@1  62.50 ( 61.15)\n",
      "[xla:4]Train:  Epoch: [22][ 30/129]\tTime  0.258 ( 0.212)\tData  0.011 ( 0.015)\tLoss 6.3281e-01 (8.3543e-01)\tAcc@1  81.00 ( 63.56)\n",
      "[xla:2]Train:  Epoch: [22][ 60/129]\tTime  0.174 ( 0.211)\tData  0.011 ( 0.016)\tLoss 1.0156e+00 (8.3594e-01)\tAcc@1  62.50 ( 61.39)\n",
      "[xla:0]Train:  Epoch: [22][ 60/129]\tTime  0.195 ( 0.205)\tData  0.012 ( 0.015)\tLoss 8.4766e-01 (8.3248e-01)\tAcc@1  62.50 ( 61.82)\n",
      "[xla:6]Train:  Epoch: [22][ 60/129]\tTime  0.194 ( 0.215)\tData  0.012 ( 0.015)\tLoss 9.6484e-01 (8.1519e-01)\tAcc@1  56.25 ( 63.43)\n",
      "[xla:5]Train:  Epoch: [22][ 60/129]\tTime  0.200 ( 0.209)\tData  0.016 ( 0.016)\tLoss 8.9844e-01 (8.3959e-01)\tAcc@1  56.25 ( 61.89)\n",
      "[xla:7]Train:  Epoch: [22][ 60/129]\tTime  0.215 ( 0.205)\tData  0.010 ( 0.015)\tLoss 5.5078e-01 (8.5256e-01)\tAcc@1  81.00 ( 61.39)\n",
      "[xla:4]Train:  Epoch: [22][ 60/129]\tTime  0.179 ( 0.208)\tData  0.008 ( 0.015)\tLoss 9.3359e-01 (8.1327e-01)\tAcc@1  62.50 ( 65.21)\n",
      "[xla:1]Train:  Epoch: [22][ 60/129]\tTime  0.204 ( 0.211)\tData  0.013 ( 0.017)\tLoss 9.6484e-01 (8.2716e-01)\tAcc@1  43.75 ( 62.22)\n",
      "[xla:3]Train:  Epoch: [22][ 60/129]\tTime  0.195 ( 0.205)\tData  0.009 ( 0.014)\tLoss 6.8359e-01 (8.1250e-01)\tAcc@1  75.00 ( 63.79)\n",
      "[xla:2]Train:  Epoch: [22][ 90/129]\tTime  0.172 ( 0.212)\tData  0.011 ( 0.015)\tLoss 6.5625e-01 (8.4203e-01)\tAcc@1  81.00 ( 61.70)\n",
      "[xla:0]Train:  Epoch: [22][ 90/129]\tTime  0.190 ( 0.208)\tData  0.015 ( 0.015)\tLoss 1.2656e+00 (8.1383e-01)\tAcc@1  43.75 ( 63.42)\n",
      "[xla:6]Train:  Epoch: [22][ 90/129]\tTime  0.238 ( 0.215)\tData  0.016 ( 0.015)\tLoss 5.3906e-01 (8.0743e-01)\tAcc@1  87.50 ( 64.02)\n",
      "[xla:1]Train:  Epoch: [22][ 90/129]\tTime  0.227 ( 0.212)\tData  0.010 ( 0.017)\tLoss 6.0156e-01 (8.1164e-01)\tAcc@1  81.00 ( 62.93)\n",
      "[xla:5]Train:  Epoch: [22][ 90/129]\tTime  0.214 ( 0.211)\tData  0.020 ( 0.016)\tLoss 6.3281e-01 (8.3023e-01)\tAcc@1  69.00 ( 62.18)\n",
      "[xla:4]Train:  Epoch: [22][ 90/129]\tTime  0.239 ( 0.211)\tData  0.012 ( 0.014)\tLoss 1.0547e+00 (8.1083e-01)\tAcc@1  43.75 ( 65.69)\n",
      "[xla:3]Train:  Epoch: [22][ 90/129]\tTime  0.206 ( 0.208)\tData  0.021 ( 0.015)\tLoss 6.0938e-01 (7.9844e-01)\tAcc@1  81.00 ( 63.65)\n",
      "[xla:7]Train:  Epoch: [22][ 90/129]\tTime  0.219 ( 0.208)\tData  0.014 ( 0.015)\tLoss 6.2500e-01 (8.4442e-01)\tAcc@1  69.00 ( 61.15)\n",
      "[xla:6]Train:  Epoch: [22][120/129]\tTime  0.173 ( 0.216)\tData  0.011 ( 0.015)\tLoss 5.0781e-01 (7.8233e-01)\tAcc@1  75.00 ( 65.41)\n",
      "[xla:5]Train:  Epoch: [22][120/129]\tTime  0.183 ( 0.213)\tData  0.012 ( 0.016)\tLoss 3.8086e-01 (8.0822e-01)\tAcc@1  87.50 ( 63.35)\n",
      "[xla:7]Train:  Epoch: [22][120/129]\tTime  0.200 ( 0.211)\tData  0.012 ( 0.015)\tLoss 8.3203e-01 (8.1392e-01)\tAcc@1  56.25 ( 62.38)\n",
      "[xla:2]Train:  Epoch: [22][120/129]\tTime  0.198 ( 0.214)\tData  0.017 ( 0.015)\tLoss 8.2031e-01 (8.0700e-01)\tAcc@1  56.25 ( 63.77)\n",
      "[xla:1]Train:  Epoch: [22][120/129]\tTime  0.238 ( 0.214)\tData  0.011 ( 0.016)\tLoss 8.6328e-01 (7.8824e-01)\tAcc@1  62.50 ( 64.43)\n",
      "[xla:0]Train:  Epoch: [22][120/129]\tTime  0.209 ( 0.211)\tData  0.012 ( 0.015)\tLoss 7.6172e-01 (7.8533e-01)\tAcc@1  69.00 ( 65.25)\n",
      "[xla:4]Train:  Epoch: [22][120/129]\tTime  0.222 ( 0.213)\tData  0.011 ( 0.015)\tLoss 9.0625e-01 (7.9487e-01)\tAcc@1  56.25 ( 65.99)\n",
      "[xla:3]Train:  Epoch: [22][120/129]\tTime  0.219 ( 0.211)\tData  0.011 ( 0.015)\tLoss 4.9219e-01 (7.7962e-01)\tAcc@1  87.50 ( 64.97)\n",
      "Finished training epoch 22\n",
      "[xla:6]Validation: [0/6]\tTime  0.118 ( 0.118)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.071 ( 0.071)\tLoss 8.4375e-01 (8.4375e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.064 ( 0.064)\tLoss 1.3047e+00 (1.3047e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.079 ( 0.079)\tLoss 5.8594e-01 (5.8594e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 7.1484e-01 (7.1484e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 9.6875e-01 (9.6875e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Validation: [0/6]\tTime  0.176 ( 0.176)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [23][  0/129]\tTime  0.172 ( 0.172)\tData  0.017 ( 0.017)\tLoss 6.3281e-01 (6.3281e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [23][  0/129]\tTime  0.229 ( 0.229)\tData  0.091 ( 0.091)\tLoss 8.0469e-01 (8.0469e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Train:  Epoch: [23][  0/129]\tTime  0.263 ( 0.263)\tData  0.096 ( 0.096)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [23][  0/129]\tTime  0.209 ( 0.209)\tData  0.057 ( 0.057)\tLoss 5.7031e-01 (5.7031e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Train:  Epoch: [23][  0/129]\tTime  0.209 ( 0.209)\tData  0.071 ( 0.071)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [23][  0/129]\tTime  0.186 ( 0.186)\tData  0.067 ( 0.067)\tLoss 9.1016e-01 (9.1016e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [23][  0/129]\tTime  0.175 ( 0.175)\tData  0.038 ( 0.038)\tLoss 7.7344e-01 (7.7344e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [23][  0/129]\tTime  0.179 ( 0.179)\tData  0.040 ( 0.040)\tLoss 7.1094e-01 (7.1094e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [23][ 30/129]\tTime  0.184 ( 0.208)\tData  0.011 ( 0.018)\tLoss 8.1250e-01 (8.4955e-01)\tAcc@1  56.25 ( 59.94)\n",
      "[xla:6]Train:  Epoch: [23][ 30/129]\tTime  0.185 ( 0.217)\tData  0.012 ( 0.017)\tLoss 9.6484e-01 (7.8768e-01)\tAcc@1  37.50 ( 64.35)\n",
      "[xla:4]Train:  Epoch: [23][ 30/129]\tTime  0.183 ( 0.203)\tData  0.019 ( 0.016)\tLoss 6.4062e-01 (8.2775e-01)\tAcc@1  81.00 ( 63.56)\n",
      "[xla:1]Train:  Epoch: [23][ 30/129]\tTime  0.209 ( 0.218)\tData  0.017 ( 0.018)\tLoss 7.1094e-01 (7.9889e-01)\tAcc@1  69.00 ( 65.73)\n",
      "[xla:0]Train:  Epoch: [23][ 30/129]\tTime  0.205 ( 0.216)\tData  0.014 ( 0.015)\tLoss 1.0000e+00 (8.6152e-01)\tAcc@1  56.25 ( 59.92)\n",
      "[xla:3]Train:  Epoch: [23][ 30/129]\tTime  0.196 ( 0.211)\tData  0.018 ( 0.016)\tLoss 7.6953e-01 (8.5005e-01)\tAcc@1  62.50 ( 60.10)\n",
      "[xla:7]Train:  Epoch: [23][ 30/129]\tTime  0.213 ( 0.208)\tData  0.022 ( 0.016)\tLoss 8.5547e-01 (8.6908e-01)\tAcc@1  62.50 ( 61.10)\n",
      "[xla:5]Train:  Epoch: [23][ 30/129]\tTime  0.205 ( 0.203)\tData  0.013 ( 0.014)\tLoss 7.6562e-01 (8.6353e-01)\tAcc@1  62.50 ( 62.98)\n",
      "[xla:0]Train:  Epoch: [23][ 60/129]\tTime  0.180 ( 0.211)\tData  0.011 ( 0.015)\tLoss 8.7500e-01 (8.2614e-01)\tAcc@1  56.25 ( 63.26)\n",
      "[xla:3]Train:  Epoch: [23][ 60/129]\tTime  0.184 ( 0.209)\tData  0.012 ( 0.016)\tLoss 6.2500e-01 (8.2377e-01)\tAcc@1  75.00 ( 62.42)\n",
      "[xla:4]Train:  Epoch: [23][ 60/129]\tTime  0.198 ( 0.205)\tData  0.011 ( 0.017)\tLoss 9.8438e-01 (8.1673e-01)\tAcc@1  56.25 ( 64.39)\n",
      "[xla:5]Train:  Epoch: [23][ 60/129]\tTime  0.187 ( 0.205)\tData  0.010 ( 0.015)\tLoss 1.0391e+00 (8.4574e-01)\tAcc@1  43.75 ( 62.87)\n",
      "[xla:2]Train:  Epoch: [23][ 60/129]\tTime  0.192 ( 0.208)\tData  0.010 ( 0.016)\tLoss 8.9844e-01 (8.2877e-01)\tAcc@1  56.25 ( 61.61)\n",
      "[xla:7]Train:  Epoch: [23][ 60/129]\tTime  0.210 ( 0.207)\tData  0.011 ( 0.015)\tLoss 5.1562e-01 (8.3446e-01)\tAcc@1  87.50 ( 61.81)\n",
      "[xla:6]Train:  Epoch: [23][ 60/129]\tTime  0.210 ( 0.213)\tData  0.010 ( 0.017)\tLoss 9.0234e-01 (8.1301e-01)\tAcc@1  62.50 ( 63.16)\n",
      "[xla:1]Train:  Epoch: [23][ 60/129]\tTime  0.204 ( 0.213)\tData  0.010 ( 0.016)\tLoss 8.1641e-01 (8.3459e-01)\tAcc@1  56.25 ( 63.84)\n",
      "[xla:0]Train:  Epoch: [23][ 90/129]\tTime  0.154 ( 0.208)\tData  0.011 ( 0.015)\tLoss 1.2969e+00 (8.1954e-01)\tAcc@1  43.75 ( 62.96)\n",
      "[xla:5]Train:  Epoch: [23][ 90/129]\tTime  0.197 ( 0.204)\tData  0.018 ( 0.015)\tLoss 8.0078e-01 (8.4216e-01)\tAcc@1  69.00 ( 62.90)\n",
      "[xla:1]Train:  Epoch: [23][ 90/129]\tTime  0.197 ( 0.209)\tData  0.011 ( 0.016)\tLoss 6.0938e-01 (8.2044e-01)\tAcc@1  69.00 ( 64.03)\n",
      "[xla:7]Train:  Epoch: [23][ 90/129]\tTime  0.196 ( 0.205)\tData  0.011 ( 0.014)\tLoss 5.3516e-01 (8.3997e-01)\tAcc@1  81.00 ( 61.43)\n",
      "[xla:2]Train:  Epoch: [23][ 90/129]\tTime  0.213 ( 0.206)\tData  0.012 ( 0.015)\tLoss 6.7969e-01 (8.3594e-01)\tAcc@1  75.00 ( 61.64)\n",
      "[xla:4]Train:  Epoch: [23][ 90/129]\tTime  0.196 ( 0.204)\tData  0.011 ( 0.016)\tLoss 9.8828e-01 (8.1969e-01)\tAcc@1  50.00 ( 63.84)\n",
      "[xla:3]Train:  Epoch: [23][ 90/129]\tTime  0.225 ( 0.207)\tData  0.012 ( 0.015)\tLoss 5.9375e-01 (8.1379e-01)\tAcc@1  69.00 ( 63.15)\n",
      "[xla:6]Train:  Epoch: [23][ 90/129]\tTime  0.225 ( 0.209)\tData  0.017 ( 0.016)\tLoss 5.7812e-01 (8.1005e-01)\tAcc@1  75.00 ( 62.12)\n",
      "[xla:0]Train:  Epoch: [23][120/129]\tTime  0.182 ( 0.208)\tData  0.010 ( 0.015)\tLoss 7.6172e-01 (7.8417e-01)\tAcc@1  69.00 ( 65.02)\n",
      "[xla:3]Train:  Epoch: [23][120/129]\tTime  0.185 ( 0.207)\tData  0.012 ( 0.015)\tLoss 4.4531e-01 (7.8478e-01)\tAcc@1  69.00 ( 65.21)\n",
      "[xla:4]Train:  Epoch: [23][120/129]\tTime  0.197 ( 0.205)\tData  0.022 ( 0.015)\tLoss 7.8906e-01 (8.0009e-01)\tAcc@1  75.00 ( 64.59)\n",
      "[xla:5]Train:  Epoch: [23][120/129]\tTime  0.169 ( 0.205)\tData  0.008 ( 0.015)\tLoss 3.1641e-01 (8.2031e-01)\tAcc@1 100.00 ( 64.41)\n",
      "[xla:7]Train:  Epoch: [23][120/129]\tTime  0.198 ( 0.206)\tData  0.012 ( 0.014)\tLoss 7.4219e-01 (8.1834e-01)\tAcc@1  75.00 ( 62.95)\n",
      "[xla:6]Train:  Epoch: [23][120/129]\tTime  0.199 ( 0.209)\tData  0.013 ( 0.016)\tLoss 5.5859e-01 (7.9066e-01)\tAcc@1  75.00 ( 63.51)\n",
      "[xla:2]Train:  Epoch: [23][120/129]\tTime  0.202 ( 0.207)\tData  0.012 ( 0.015)\tLoss 7.5000e-01 (7.9789e-01)\tAcc@1  62.50 ( 64.06)\n",
      "[xla:1]Train:  Epoch: [23][120/129]\tTime  0.220 ( 0.209)\tData  0.020 ( 0.015)\tLoss 8.2031e-01 (7.9842e-01)\tAcc@1  62.50 ( 65.15)\n",
      "[xla:5]Validation: [0/6]\tTime  0.081 ( 0.081)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.153 ( 0.153)\tLoss 1.3047e+00 (1.3047e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Finished training epoch 23\n",
      "[xla:2]Validation: [0/6]\tTime  0.073 ( 0.073)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Validation: [0/6]\tTime  0.093 ( 0.093)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:4]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Validation: [0/6]\tTime  0.059 ( 0.059)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.139 ( 0.139)\tLoss 9.6484e-01 (9.6484e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.088 ( 0.088)\tLoss 7.1094e-01 (7.1094e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [24][  0/129]\tTime  0.237 ( 0.237)\tData  0.049 ( 0.049)\tLoss 7.7734e-01 (7.7734e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [24][  0/129]\tTime  0.238 ( 0.238)\tData  0.065 ( 0.065)\tLoss 9.4141e-01 (9.4141e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [24][  0/129]\tTime  0.231 ( 0.231)\tData  0.077 ( 0.077)\tLoss 9.1797e-01 (9.1797e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [24][  0/129]\tTime  0.245 ( 0.245)\tData  0.076 ( 0.076)\tLoss 7.2266e-01 (7.2266e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [24][  0/129]\tTime  0.265 ( 0.265)\tData  0.098 ( 0.098)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Train:  Epoch: [24][  0/129]\tTime  0.217 ( 0.217)\tData  0.078 ( 0.078)\tLoss 5.3516e-01 (5.3516e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [24][  0/129]\tTime  0.251 ( 0.251)\tData  0.056 ( 0.056)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [24][  0/129]\tTime  0.256 ( 0.256)\tData  0.104 ( 0.104)\tLoss 7.1875e-01 (7.1875e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [24][ 30/129]\tTime  0.177 ( 0.212)\tData  0.013 ( 0.018)\tLoss 6.9141e-01 (8.7399e-01)\tAcc@1  69.00 ( 62.51)\n",
      "[xla:4]Train:  Epoch: [24][ 30/129]\tTime  0.171 ( 0.213)\tData  0.011 ( 0.019)\tLoss 6.9922e-01 (8.3260e-01)\tAcc@1  81.00 ( 62.90)\n",
      "[xla:6]Train:  Epoch: [24][ 30/129]\tTime  0.207 ( 0.212)\tData  0.013 ( 0.017)\tLoss 8.8672e-01 (8.2176e-01)\tAcc@1  56.25 ( 61.50)\n",
      "[xla:0]Train:  Epoch: [24][ 30/129]\tTime  0.188 ( 0.205)\tData  0.011 ( 0.017)\tLoss 1.0547e+00 (8.9982e-01)\tAcc@1  56.25 ( 59.30)\n",
      "[xla:7]Train:  Epoch: [24][ 30/129]\tTime  0.208 ( 0.206)\tData  0.011 ( 0.017)\tLoss 9.5703e-01 (8.7387e-01)\tAcc@1  50.00 ( 60.11)\n",
      "[xla:1]Train:  Epoch: [24][ 30/129]\tTime  0.207 ( 0.212)\tData  0.012 ( 0.016)\tLoss 8.6328e-01 (8.0456e-01)\tAcc@1  62.50 ( 63.53)\n",
      "[xla:2]Train:  Epoch: [24][ 30/129]\tTime  0.193 ( 0.215)\tData  0.010 ( 0.016)\tLoss 8.3203e-01 (8.3109e-01)\tAcc@1  69.00 ( 61.54)\n",
      "[xla:3]Train:  Epoch: [24][ 30/129]\tTime  0.184 ( 0.205)\tData  0.010 ( 0.017)\tLoss 9.4922e-01 (8.4854e-01)\tAcc@1  43.75 ( 58.68)\n",
      "[xla:5]Train:  Epoch: [24][ 60/129]\tTime  0.176 ( 0.208)\tData  0.011 ( 0.017)\tLoss 8.8281e-01 (8.5765e-01)\tAcc@1  62.50 ( 62.64)\n",
      "[xla:7]Train:  Epoch: [24][ 60/129]\tTime  0.181 ( 0.205)\tData  0.009 ( 0.016)\tLoss 5.4297e-01 (8.4401e-01)\tAcc@1  69.00 ( 61.73)\n",
      "[xla:3]Train:  Epoch: [24][ 60/129]\tTime  0.188 ( 0.205)\tData  0.012 ( 0.016)\tLoss 7.1875e-01 (8.1910e-01)\tAcc@1  69.00 ( 61.73)\n",
      "[xla:0]Train:  Epoch: [24][ 60/129]\tTime  0.196 ( 0.205)\tData  0.010 ( 0.016)\tLoss 8.0078e-01 (8.3776e-01)\tAcc@1  69.00 ( 62.51)\n",
      "[xla:1]Train:  Epoch: [24][ 60/129]\tTime  0.184 ( 0.209)\tData  0.009 ( 0.017)\tLoss 8.9844e-01 (8.4183e-01)\tAcc@1  56.25 ( 61.50)\n",
      "[xla:6]Train:  Epoch: [24][ 60/129]\tTime  0.225 ( 0.209)\tData  0.011 ( 0.016)\tLoss 8.5156e-01 (8.2854e-01)\tAcc@1  75.00 ( 62.00)\n",
      "[xla:4]Train:  Epoch: [24][ 60/129]\tTime  0.192 ( 0.210)\tData  0.008 ( 0.017)\tLoss 9.1797e-01 (8.1842e-01)\tAcc@1  50.00 ( 63.97)\n",
      "[xla:2]Train:  Epoch: [24][ 60/129]\tTime  0.200 ( 0.210)\tData  0.009 ( 0.016)\tLoss 8.7109e-01 (8.2399e-01)\tAcc@1  56.25 ( 62.13)\n",
      "[xla:7]Train:  Epoch: [24][ 90/129]\tTime  0.179 ( 0.205)\tData  0.011 ( 0.015)\tLoss 6.9922e-01 (8.3727e-01)\tAcc@1  62.50 ( 62.01)\n",
      "[xla:1]Train:  Epoch: [24][ 90/129]\tTime  0.181 ( 0.208)\tData  0.010 ( 0.016)\tLoss 5.2344e-01 (8.2830e-01)\tAcc@1  75.00 ( 62.33)\n",
      "[xla:2]Train:  Epoch: [24][ 90/129]\tTime  0.188 ( 0.209)\tData  0.009 ( 0.015)\tLoss 6.0938e-01 (8.3991e-01)\tAcc@1  75.00 ( 61.38)\n",
      "[xla:3]Train:  Epoch: [24][ 90/129]\tTime  0.203 ( 0.205)\tData  0.012 ( 0.015)\tLoss 7.1484e-01 (8.0827e-01)\tAcc@1  75.00 ( 62.68)\n",
      "[xla:6]Train:  Epoch: [24][ 90/129]\tTime  0.204 ( 0.208)\tData  0.011 ( 0.015)\tLoss 5.7422e-01 (8.1857e-01)\tAcc@1  62.50 ( 63.21)\n",
      "[xla:0]Train:  Epoch: [24][ 90/129]\tTime  0.198 ( 0.206)\tData  0.010 ( 0.015)\tLoss 1.1406e+00 (8.2175e-01)\tAcc@1  43.75 ( 62.79)\n",
      "[xla:4]Train:  Epoch: [24][ 90/129]\tTime  0.194 ( 0.209)\tData  0.011 ( 0.017)\tLoss 8.7500e-01 (8.2033e-01)\tAcc@1  62.50 ( 64.12)\n",
      "[xla:5]Train:  Epoch: [24][ 90/129]\tTime  0.198 ( 0.208)\tData  0.011 ( 0.016)\tLoss 7.5391e-01 (8.4375e-01)\tAcc@1  69.00 ( 63.49)\n",
      "[xla:3]Train:  Epoch: [24][120/129]\tTime  0.184 ( 0.208)\tData  0.014 ( 0.015)\tLoss 4.6875e-01 (7.8487e-01)\tAcc@1  94.00 ( 64.29)\n",
      "[xla:2]Train:  Epoch: [24][120/129]\tTime  0.185 ( 0.211)\tData  0.008 ( 0.015)\tLoss 8.0859e-01 (8.0025e-01)\tAcc@1  69.00 ( 63.52)\n",
      "[xla:1]Train:  Epoch: [24][120/129]\tTime  0.231 ( 0.210)\tData  0.013 ( 0.016)\tLoss 7.1875e-01 (8.0725e-01)\tAcc@1  75.00 ( 63.46)\n",
      "[xla:7]Train:  Epoch: [24][120/129]\tTime  0.222 ( 0.209)\tData  0.012 ( 0.015)\tLoss 8.6719e-01 (8.0943e-01)\tAcc@1  69.00 ( 63.79)\n",
      "[xla:6]Train:  Epoch: [24][120/129]\tTime  0.264 ( 0.210)\tData  0.022 ( 0.015)\tLoss 5.7812e-01 (7.9468e-01)\tAcc@1  62.50 ( 64.71)\n",
      "[xla:5]Train:  Epoch: [24][120/129]\tTime  0.257 ( 0.210)\tData  0.019 ( 0.016)\tLoss 4.6289e-01 (8.2846e-01)\tAcc@1  94.00 ( 63.77)\n",
      "[xla:0]Train:  Epoch: [24][120/129]\tTime  0.273 ( 0.209)\tData  0.013 ( 0.015)\tLoss 7.5000e-01 (7.9000e-01)\tAcc@1  62.50 ( 64.99)\n",
      "[xla:4]Train:  Epoch: [24][120/129]\tTime  0.256 ( 0.211)\tData  0.012 ( 0.017)\tLoss 8.1641e-01 (8.0269e-01)\tAcc@1  69.00 ( 64.81)\n",
      "[xla:7]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 9.7656e-01 (9.7656e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 24\n",
      "[xla:1]Validation: [0/6]\tTime  0.152 ( 0.152)\tLoss 1.3750e+00 (1.3750e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:6]Validation: [0/6]\tTime  0.167 ( 0.167)\tLoss 1.0469e+00 (1.0469e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:3]Validation: [0/6]\tTime  0.134 ( 0.134)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.121 ( 0.121)\tLoss 5.5859e-01 (5.5859e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.104 ( 0.104)\tLoss 6.4062e-01 (6.4062e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.122 ( 0.122)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Train:  Epoch: [25][  0/129]\tTime  0.237 ( 0.237)\tData  0.072 ( 0.072)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Train:  Epoch: [25][  0/129]\tTime  0.234 ( 0.234)\tData  0.089 ( 0.089)\tLoss 8.5938e-01 (8.5938e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:1]Train:  Epoch: [25][  0/129]\tTime  0.230 ( 0.230)\tData  0.080 ( 0.080)\tLoss 7.4609e-01 (7.4609e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [25][  0/129]\tTime  0.188 ( 0.188)\tData  0.027 ( 0.027)\tLoss 8.4766e-01 (8.4766e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Train:  Epoch: [25][  0/129]\tTime  0.186 ( 0.186)\tData  0.048 ( 0.048)\tLoss 1.0703e+00 (1.0703e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [25][  0/129]\tTime  0.245 ( 0.245)\tData  0.064 ( 0.064)\tLoss 5.6250e-01 (5.6250e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [25][  0/129]\tTime  0.231 ( 0.231)\tData  0.083 ( 0.083)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [25][  0/129]\tTime  0.229 ( 0.229)\tData  0.094 ( 0.094)\tLoss 9.4531e-01 (9.4531e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Train:  Epoch: [25][ 30/129]\tTime  0.163 ( 0.209)\tData  0.011 ( 0.016)\tLoss 7.1484e-01 (8.3720e-01)\tAcc@1  69.00 ( 65.14)\n",
      "[xla:0]Train:  Epoch: [25][ 30/129]\tTime  0.192 ( 0.211)\tData  0.010 ( 0.015)\tLoss 9.9609e-01 (8.7122e-01)\tAcc@1  50.00 ( 60.90)\n",
      "[xla:4]Train:  Epoch: [25][ 30/129]\tTime  0.174 ( 0.208)\tData  0.020 ( 0.016)\tLoss 7.5391e-01 (8.3795e-01)\tAcc@1  75.00 ( 62.13)\n",
      "[xla:1]Train:  Epoch: [25][ 30/129]\tTime  0.186 ( 0.218)\tData  0.011 ( 0.016)\tLoss 9.4141e-01 (8.1565e-01)\tAcc@1  56.25 ( 63.79)\n",
      "[xla:7]Train:  Epoch: [25][ 30/129]\tTime  0.208 ( 0.230)\tData  0.010 ( 0.017)\tLoss 8.7500e-01 (8.6316e-01)\tAcc@1  62.50 ( 60.11)\n",
      "[xla:2]Train:  Epoch: [25][ 30/129]\tTime  0.196 ( 0.224)\tData  0.010 ( 0.018)\tLoss 8.1250e-01 (8.7865e-01)\tAcc@1  69.00 ( 56.89)\n",
      "[xla:3]Train:  Epoch: [25][ 30/129]\tTime  0.190 ( 0.213)\tData  0.011 ( 0.017)\tLoss 7.6562e-01 (8.0406e-01)\tAcc@1  69.00 ( 65.39)\n",
      "[xla:6]Train:  Epoch: [25][ 30/129]\tTime  0.205 ( 0.212)\tData  0.013 ( 0.015)\tLoss 9.6875e-01 (8.1426e-01)\tAcc@1  43.75 ( 64.75)\n",
      "[xla:2]Train:  Epoch: [25][ 60/129]\tTime  0.175 ( 0.215)\tData  0.012 ( 0.016)\tLoss 9.2188e-01 (8.3972e-01)\tAcc@1  56.25 ( 59.86)\n",
      "[xla:5]Train:  Epoch: [25][ 60/129]\tTime  0.187 ( 0.209)\tData  0.017 ( 0.015)\tLoss 8.8672e-01 (8.3703e-01)\tAcc@1  56.25 ( 63.54)\n",
      "[xla:0]Train:  Epoch: [25][ 60/129]\tTime  0.214 ( 0.209)\tData  0.011 ( 0.016)\tLoss 7.7734e-01 (8.3315e-01)\tAcc@1  69.00 ( 62.93)\n",
      "[xla:3]Train:  Epoch: [25][ 60/129]\tTime  0.198 ( 0.210)\tData  0.010 ( 0.016)\tLoss 5.8203e-01 (7.9790e-01)\tAcc@1  75.00 ( 65.32)\n",
      "[xla:1]Train:  Epoch: [25][ 60/129]\tTime  0.189 ( 0.213)\tData  0.013 ( 0.016)\tLoss 1.0234e+00 (8.2470e-01)\tAcc@1  50.00 ( 62.68)\n",
      "[xla:4]Train:  Epoch: [25][ 60/129]\tTime  0.188 ( 0.208)\tData  0.010 ( 0.016)\tLoss 7.8125e-01 (8.3197e-01)\tAcc@1  62.50 ( 62.73)\n",
      "[xla:6]Train:  Epoch: [25][ 60/129]\tTime  0.205 ( 0.210)\tData  0.013 ( 0.014)\tLoss 9.3359e-01 (8.3530e-01)\tAcc@1  50.00 ( 62.55)\n",
      "[xla:7]Train:  Epoch: [25][ 60/129]\tTime  0.196 ( 0.219)\tData  0.011 ( 0.016)\tLoss 5.2344e-01 (8.2691e-01)\tAcc@1  87.50 ( 62.23)\n",
      "[xla:2]Train:  Epoch: [25][ 90/129]\tTime  0.146 ( 0.212)\tData  0.010 ( 0.015)\tLoss 5.3906e-01 (8.4263e-01)\tAcc@1  75.00 ( 60.55)\n",
      "[xla:6]Train:  Epoch: [25][ 90/129]\tTime  0.183 ( 0.208)\tData  0.011 ( 0.015)\tLoss 6.9922e-01 (8.2081e-01)\tAcc@1  69.00 ( 63.17)\n",
      "[xla:5]Train:  Epoch: [25][ 90/129]\tTime  0.208 ( 0.207)\tData  0.014 ( 0.015)\tLoss 7.8516e-01 (8.3662e-01)\tAcc@1  75.00 ( 63.76)\n",
      "[xla:7]Train:  Epoch: [25][ 90/129]\tTime  0.190 ( 0.214)\tData  0.010 ( 0.016)\tLoss 5.8203e-01 (8.2538e-01)\tAcc@1  69.00 ( 62.20)\n",
      "[xla:0]Train:  Epoch: [25][ 90/129]\tTime  0.189 ( 0.208)\tData  0.011 ( 0.016)\tLoss 1.3516e+00 (8.1100e-01)\tAcc@1  31.25 ( 64.10)\n",
      "[xla:3]Train:  Epoch: [25][ 90/129]\tTime  0.200 ( 0.208)\tData  0.012 ( 0.015)\tLoss 6.1719e-01 (7.8629e-01)\tAcc@1  56.25 ( 64.95)\n",
      "[xla:4]Train:  Epoch: [25][ 90/129]\tTime  0.193 ( 0.207)\tData  0.011 ( 0.015)\tLoss 9.9219e-01 (8.2177e-01)\tAcc@1  56.25 ( 63.20)\n",
      "[xla:1]Train:  Epoch: [25][ 90/129]\tTime  0.210 ( 0.210)\tData  0.011 ( 0.015)\tLoss 6.8359e-01 (8.1690e-01)\tAcc@1  75.00 ( 62.84)\n",
      "[xla:0]Train:  Epoch: [25][120/129]\tTime  0.152 ( 0.207)\tData  0.011 ( 0.016)\tLoss 7.7734e-01 (7.8380e-01)\tAcc@1  69.00 ( 65.32)\n",
      "[xla:7]Train:  Epoch: [25][120/129]\tTime  0.196 ( 0.212)\tData  0.011 ( 0.016)\tLoss 8.2812e-01 (8.0661e-01)\tAcc@1  75.00 ( 63.47)\n",
      "[xla:2]Train:  Epoch: [25][120/129]\tTime  0.197 ( 0.210)\tData  0.010 ( 0.015)\tLoss 7.2656e-01 (8.0198e-01)\tAcc@1  75.00 ( 63.37)\n",
      "[xla:6]Train:  Epoch: [25][120/129]\tTime  0.204 ( 0.207)\tData  0.014 ( 0.015)\tLoss 5.5078e-01 (7.9523e-01)\tAcc@1  87.50 ( 64.71)\n",
      "[xla:3]Train:  Epoch: [25][120/129]\tTime  0.192 ( 0.207)\tData  0.010 ( 0.014)\tLoss 5.0391e-01 (7.7349e-01)\tAcc@1  75.00 ( 66.09)\n",
      "[xla:4]Train:  Epoch: [25][120/129]\tTime  0.227 ( 0.206)\tData  0.011 ( 0.015)\tLoss 7.4609e-01 (8.0172e-01)\tAcc@1  75.00 ( 64.26)\n",
      "[xla:5]Train:  Epoch: [25][120/129]\tTime  0.205 ( 0.207)\tData  0.011 ( 0.015)\tLoss 3.9258e-01 (8.1726e-01)\tAcc@1  87.50 ( 64.33)\n",
      "[xla:1]Train:  Epoch: [25][120/129]\tTime  0.244 ( 0.209)\tData  0.011 ( 0.015)\tLoss 8.6719e-01 (7.8727e-01)\tAcc@1  62.50 ( 64.73)\n",
      "[xla:4]Validation: [0/6]\tTime  0.061 ( 0.061)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Validation: [0/6]\tTime  0.059 ( 0.059)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.072 ( 0.072)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 25\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.065 ( 0.065)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 6.2109e-01 (6.2109e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 6.9922e-01 (6.9922e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.109 ( 0.109)\tLoss 1.3438e+00 (1.3438e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.080 ( 0.080)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Train:  Epoch: [26][  0/129]\tTime  0.169 ( 0.169)\tData  0.033 ( 0.033)\tLoss 1.1094e+00 (1.1094e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [26][  0/129]\tTime  0.193 ( 0.193)\tData  0.068 ( 0.068)\tLoss 8.0469e-01 (8.0469e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [26][  0/129]\tTime  0.224 ( 0.224)\tData  0.071 ( 0.071)\tLoss 8.0469e-01 (8.0469e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Train:  Epoch: [26][  0/129]\tTime  0.172 ( 0.172)\tData  0.035 ( 0.035)\tLoss 7.9297e-01 (7.9297e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [26][  0/129]\tTime  0.182 ( 0.182)\tData  0.038 ( 0.038)\tLoss 9.1016e-01 (9.1016e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Train:  Epoch: [26][  0/129]\tTime  0.201 ( 0.201)\tData  0.045 ( 0.045)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [26][  0/129]\tTime  0.217 ( 0.217)\tData  0.061 ( 0.061)\tLoss 8.5547e-01 (8.5547e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [26][  0/129]\tTime  0.177 ( 0.177)\tData  0.059 ( 0.059)\tLoss 8.5547e-01 (8.5547e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [26][ 30/129]\tTime  0.181 ( 0.209)\tData  0.012 ( 0.016)\tLoss 8.5156e-01 (8.6152e-01)\tAcc@1  62.50 ( 55.68)\n",
      "[xla:1]Train:  Epoch: [26][ 30/129]\tTime  0.190 ( 0.214)\tData  0.013 ( 0.018)\tLoss 8.5547e-01 (8.1748e-01)\tAcc@1  56.25 ( 65.15)\n",
      "[xla:5]Train:  Epoch: [26][ 30/129]\tTime  0.207 ( 0.212)\tData  0.011 ( 0.016)\tLoss 7.6953e-01 (8.4665e-01)\tAcc@1  75.00 ( 63.34)\n",
      "[xla:6]Train:  Epoch: [26][ 30/129]\tTime  0.181 ( 0.223)\tData  0.014 ( 0.016)\tLoss 9.1406e-01 (7.9284e-01)\tAcc@1  50.00 ( 64.54)\n",
      "[xla:4]Train:  Epoch: [26][ 30/129]\tTime  0.201 ( 0.219)\tData  0.021 ( 0.017)\tLoss 6.7969e-01 (8.2548e-01)\tAcc@1  75.00 ( 62.94)\n",
      "[xla:0]Train:  Epoch: [26][ 30/129]\tTime  0.207 ( 0.207)\tData  0.018 ( 0.015)\tLoss 1.0234e+00 (8.7046e-01)\tAcc@1  50.00 ( 59.71)\n",
      "[xla:7]Train:  Epoch: [26][ 30/129]\tTime  0.192 ( 0.222)\tData  0.011 ( 0.016)\tLoss 8.5938e-01 (8.5572e-01)\tAcc@1  62.50 ( 62.76)\n",
      "[xla:3]Train:  Epoch: [26][ 30/129]\tTime  0.215 ( 0.211)\tData  0.011 ( 0.015)\tLoss 9.2578e-01 (8.4942e-01)\tAcc@1  69.00 ( 62.55)\n",
      "[xla:0]Train:  Epoch: [26][ 60/129]\tTime  0.192 ( 0.206)\tData  0.011 ( 0.015)\tLoss 7.5781e-01 (8.3133e-01)\tAcc@1  62.50 ( 62.64)\n",
      "[xla:1]Train:  Epoch: [26][ 60/129]\tTime  0.164 ( 0.210)\tData  0.010 ( 0.016)\tLoss 9.8438e-01 (8.4817e-01)\tAcc@1  50.00 ( 64.28)\n",
      "[xla:7]Train:  Epoch: [26][ 60/129]\tTime  0.177 ( 0.214)\tData  0.012 ( 0.015)\tLoss 4.9219e-01 (8.3427e-01)\tAcc@1  87.50 ( 63.04)\n",
      "[xla:3]Train:  Epoch: [26][ 60/129]\tTime  0.189 ( 0.208)\tData  0.018 ( 0.016)\tLoss 6.9141e-01 (8.1029e-01)\tAcc@1  75.00 ( 64.89)\n",
      "[xla:2]Train:  Epoch: [26][ 60/129]\tTime  0.188 ( 0.208)\tData  0.014 ( 0.015)\tLoss 9.1016e-01 (8.2480e-01)\tAcc@1  56.25 ( 60.06)\n",
      "[xla:4]Train:  Epoch: [26][ 60/129]\tTime  0.190 ( 0.213)\tData  0.011 ( 0.015)\tLoss 7.5000e-01 (8.0783e-01)\tAcc@1  69.00 ( 65.39)\n",
      "[xla:6]Train:  Epoch: [26][ 60/129]\tTime  0.193 ( 0.215)\tData  0.011 ( 0.015)\tLoss 8.7891e-01 (8.1301e-01)\tAcc@1  56.25 ( 63.24)\n",
      "[xla:5]Train:  Epoch: [26][ 60/129]\tTime  0.212 ( 0.209)\tData  0.011 ( 0.016)\tLoss 1.1016e+00 (8.3511e-01)\tAcc@1  50.00 ( 63.17)\n",
      "[xla:2]Train:  Epoch: [26][ 90/129]\tTime  0.164 ( 0.206)\tData  0.012 ( 0.015)\tLoss 5.5469e-01 (8.3594e-01)\tAcc@1  75.00 ( 60.54)\n",
      "[xla:1]Train:  Epoch: [26][ 90/129]\tTime  0.190 ( 0.208)\tData  0.014 ( 0.016)\tLoss 7.3047e-01 (8.2463e-01)\tAcc@1  75.00 ( 64.94)\n",
      "[xla:4]Train:  Epoch: [26][ 90/129]\tTime  0.164 ( 0.210)\tData  0.008 ( 0.015)\tLoss 9.5703e-01 (8.0636e-01)\tAcc@1  43.75 ( 64.71)\n",
      "[xla:7]Train:  Epoch: [26][ 90/129]\tTime  0.203 ( 0.211)\tData  0.016 ( 0.015)\tLoss 5.9766e-01 (8.3336e-01)\tAcc@1  69.00 ( 61.71)\n",
      "[xla:5]Train:  Epoch: [26][ 90/129]\tTime  0.215 ( 0.208)\tData  0.011 ( 0.015)\tLoss 6.7969e-01 (8.3302e-01)\tAcc@1  69.00 ( 63.58)\n",
      "[xla:0]Train:  Epoch: [26][ 90/129]\tTime  0.247 ( 0.206)\tData  0.009 ( 0.014)\tLoss 1.3594e+00 (8.1405e-01)\tAcc@1  43.75 ( 63.77)\n",
      "[xla:6]Train:  Epoch: [26][ 90/129]\tTime  0.225 ( 0.212)\tData  0.015 ( 0.014)\tLoss 5.6641e-01 (8.0185e-01)\tAcc@1  81.00 ( 64.30)\n",
      "[xla:3]Train:  Epoch: [26][ 90/129]\tTime  0.230 ( 0.207)\tData  0.010 ( 0.016)\tLoss 5.0781e-01 (7.9569e-01)\tAcc@1  75.00 ( 65.55)\n",
      "[xla:1]Train:  Epoch: [26][120/129]\tTime  0.192 ( 0.209)\tData  0.011 ( 0.015)\tLoss 7.1484e-01 (8.0353e-01)\tAcc@1  75.00 ( 65.64)\n",
      "[xla:3]Train:  Epoch: [26][120/129]\tTime  0.174 ( 0.208)\tData  0.009 ( 0.015)\tLoss 4.9414e-01 (7.7763e-01)\tAcc@1  81.00 ( 66.56)\n",
      "[xla:4]Train:  Epoch: [26][120/129]\tTime  0.196 ( 0.210)\tData  0.013 ( 0.015)\tLoss 7.7734e-01 (7.8776e-01)\tAcc@1  69.00 ( 65.36)\n",
      "[xla:7]Train:  Epoch: [26][120/129]\tTime  0.226 ( 0.211)\tData  0.011 ( 0.015)\tLoss 9.1406e-01 (8.0950e-01)\tAcc@1  69.00 ( 63.36)\n",
      "[xla:0]Train:  Epoch: [26][120/129]\tTime  0.198 ( 0.207)\tData  0.011 ( 0.014)\tLoss 7.0703e-01 (7.8782e-01)\tAcc@1  62.50 ( 64.96)\n",
      "[xla:6]Train:  Epoch: [26][120/129]\tTime  0.218 ( 0.212)\tData  0.013 ( 0.015)\tLoss 5.7422e-01 (7.7689e-01)\tAcc@1  69.00 ( 65.61)\n",
      "[xla:2]Train:  Epoch: [26][120/129]\tTime  0.186 ( 0.208)\tData  0.008 ( 0.014)\tLoss 7.5000e-01 (7.9788e-01)\tAcc@1  69.00 ( 63.30)\n",
      "[xla:5]Train:  Epoch: [26][120/129]\tTime  0.215 ( 0.209)\tData  0.009 ( 0.014)\tLoss 4.4531e-01 (8.1938e-01)\tAcc@1  87.50 ( 64.35)\n",
      "[xla:1]Validation: [0/6]\tTime  0.089 ( 0.089)\tLoss 1.3203e+00 (1.3203e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.115 ( 0.115)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 26\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:6]Validation: [0/6]\tTime  0.085 ( 0.085)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Validation: [0/6]\tTime  0.108 ( 0.108)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.142 ( 0.142)\tLoss 8.5547e-01 (8.5547e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Validation: [0/6]\tTime  0.123 ( 0.123)\tLoss 5.8594e-01 (5.8594e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.118 ( 0.118)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.120 ( 0.120)\tLoss 9.6875e-01 (9.6875e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:1]Train:  Epoch: [27][  0/129]\tTime  0.312 ( 0.312)\tData  0.071 ( 0.071)\tLoss 8.1250e-01 (8.1250e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [27][  0/129]\tTime  0.290 ( 0.290)\tData  0.067 ( 0.067)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [27][  0/129]\tTime  0.233 ( 0.233)\tData  0.068 ( 0.068)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:0]Train:  Epoch: [27][  0/129]\tTime  0.220 ( 0.220)\tData  0.058 ( 0.058)\tLoss 6.9922e-01 (6.9922e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Train:  Epoch: [27][  0/129]\tTime  0.262 ( 0.262)\tData  0.039 ( 0.039)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [27][  0/129]\tTime  0.171 ( 0.171)\tData  0.019 ( 0.019)\tLoss 9.1016e-01 (9.1016e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [27][  0/129]\tTime  0.244 ( 0.244)\tData  0.073 ( 0.073)\tLoss 7.5000e-01 (7.5000e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Train:  Epoch: [27][  0/129]\tTime  0.223 ( 0.223)\tData  0.070 ( 0.070)\tLoss 8.2812e-01 (8.2812e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Train:  Epoch: [27][ 30/129]\tTime  0.178 ( 0.219)\tData  0.012 ( 0.017)\tLoss 7.6562e-01 (8.6013e-01)\tAcc@1  75.00 ( 59.73)\n",
      "[xla:5]Train:  Epoch: [27][ 30/129]\tTime  0.164 ( 0.210)\tData  0.009 ( 0.019)\tLoss 7.5391e-01 (8.7128e-01)\tAcc@1  62.50 ( 61.75)\n",
      "[xla:1]Train:  Epoch: [27][ 30/129]\tTime  0.205 ( 0.234)\tData  0.013 ( 0.018)\tLoss 8.4766e-01 (8.1641e-01)\tAcc@1  50.00 ( 63.92)\n",
      "[xla:4]Train:  Epoch: [27][ 30/129]\tTime  0.200 ( 0.213)\tData  0.012 ( 0.017)\tLoss 7.6172e-01 (8.2976e-01)\tAcc@1  81.00 ( 61.73)\n",
      "[xla:3]Train:  Epoch: [27][ 30/129]\tTime  0.192 ( 0.230)\tData  0.014 ( 0.016)\tLoss 7.8125e-01 (8.2762e-01)\tAcc@1  62.50 ( 62.12)\n",
      "[xla:6]Train:  Epoch: [27][ 30/129]\tTime  0.230 ( 0.217)\tData  0.014 ( 0.015)\tLoss 8.3203e-01 (7.9278e-01)\tAcc@1  56.25 ( 65.75)\n",
      "[xla:0]Train:  Epoch: [27][ 30/129]\tTime  0.214 ( 0.216)\tData  0.015 ( 0.018)\tLoss 9.4922e-01 (8.9441e-01)\tAcc@1  43.75 ( 57.48)\n",
      "[xla:7]Train:  Epoch: [27][ 30/129]\tTime  0.278 ( 0.211)\tData  0.010 ( 0.015)\tLoss 8.9844e-01 (8.7172e-01)\tAcc@1  50.00 ( 59.31)\n",
      "[xla:6]Train:  Epoch: [27][ 60/129]\tTime  0.185 ( 0.211)\tData  0.011 ( 0.016)\tLoss 9.8828e-01 (8.0946e-01)\tAcc@1  62.50 ( 64.57)\n",
      "[xla:7]Train:  Epoch: [27][ 60/129]\tTime  0.198 ( 0.207)\tData  0.010 ( 0.015)\tLoss 6.0547e-01 (8.4894e-01)\tAcc@1  75.00 ( 60.58)\n",
      "[xla:0]Train:  Epoch: [27][ 60/129]\tTime  0.175 ( 0.211)\tData  0.011 ( 0.017)\tLoss 6.6797e-01 (8.3363e-01)\tAcc@1  69.00 ( 61.61)\n",
      "[xla:5]Train:  Epoch: [27][ 60/129]\tTime  0.197 ( 0.208)\tData  0.011 ( 0.017)\tLoss 1.0000e+00 (8.4807e-01)\tAcc@1  62.50 ( 62.03)\n",
      "[xla:4]Train:  Epoch: [27][ 60/129]\tTime  0.224 ( 0.209)\tData  0.011 ( 0.016)\tLoss 9.0234e-01 (8.1535e-01)\tAcc@1  50.00 ( 63.35)\n",
      "[xla:3]Train:  Epoch: [27][ 60/129]\tTime  0.207 ( 0.218)\tData  0.012 ( 0.016)\tLoss 6.0938e-01 (8.0526e-01)\tAcc@1  75.00 ( 63.17)\n",
      "[xla:1]Train:  Epoch: [27][ 60/129]\tTime  0.209 ( 0.221)\tData  0.011 ( 0.017)\tLoss 9.2188e-01 (8.2204e-01)\tAcc@1  56.25 ( 63.56)\n",
      "[xla:2]Train:  Epoch: [27][ 60/129]\tTime  0.195 ( 0.213)\tData  0.011 ( 0.016)\tLoss 8.6719e-01 (8.2582e-01)\tAcc@1  62.50 ( 61.71)\n",
      "[xla:3]Train:  Epoch: [27][ 90/129]\tTime  0.194 ( 0.214)\tData  0.012 ( 0.015)\tLoss 5.0391e-01 (7.9404e-01)\tAcc@1  75.00 ( 63.64)\n",
      "[xla:1]Train:  Epoch: [27][ 90/129]\tTime  0.204 ( 0.215)\tData  0.010 ( 0.016)\tLoss 6.1328e-01 (8.0829e-01)\tAcc@1  75.00 ( 63.70)\n",
      "[xla:0]Train:  Epoch: [27][ 90/129]\tTime  0.183 ( 0.209)\tData  0.011 ( 0.017)\tLoss 1.2109e+00 (8.1520e-01)\tAcc@1  50.00 ( 62.60)\n",
      "[xla:2]Train:  Epoch: [27][ 90/129]\tTime  0.197 ( 0.210)\tData  0.012 ( 0.016)\tLoss 5.4688e-01 (8.2911e-01)\tAcc@1  75.00 ( 62.05)\n",
      "[xla:5]Train:  Epoch: [27][ 90/129]\tTime  0.190 ( 0.207)\tData  0.016 ( 0.017)\tLoss 8.0859e-01 (8.2965e-01)\tAcc@1  62.50 ( 62.81)\n",
      "[xla:6]Train:  Epoch: [27][ 90/129]\tTime  0.198 ( 0.210)\tData  0.018 ( 0.016)\tLoss 5.5469e-01 (8.0361e-01)\tAcc@1  81.00 ( 64.10)\n",
      "[xla:4]Train:  Epoch: [27][ 90/129]\tTime  0.195 ( 0.208)\tData  0.011 ( 0.016)\tLoss 9.1797e-01 (8.1420e-01)\tAcc@1  43.75 ( 63.90)\n",
      "[xla:7]Train:  Epoch: [27][ 90/129]\tTime  0.230 ( 0.207)\tData  0.021 ( 0.015)\tLoss 4.8242e-01 (8.4094e-01)\tAcc@1  75.00 ( 60.62)\n",
      "[xla:0]Train:  Epoch: [27][120/129]\tTime  0.188 ( 0.207)\tData  0.012 ( 0.016)\tLoss 7.9297e-01 (7.8478e-01)\tAcc@1  62.50 ( 64.27)\n",
      "[xla:5]Train:  Epoch: [27][120/129]\tTime  0.197 ( 0.206)\tData  0.013 ( 0.016)\tLoss 5.0781e-01 (8.1970e-01)\tAcc@1  81.00 ( 63.66)\n",
      "[xla:6]Train:  Epoch: [27][120/129]\tTime  0.180 ( 0.208)\tData  0.013 ( 0.015)\tLoss 5.4297e-01 (7.8220e-01)\tAcc@1  69.00 ( 65.21)\n",
      "[xla:2]Train:  Epoch: [27][120/129]\tTime  0.183 ( 0.209)\tData  0.010 ( 0.015)\tLoss 9.0625e-01 (7.9558e-01)\tAcc@1  50.00 ( 64.18)\n",
      "[xla:7]Train:  Epoch: [27][120/129]\tTime  0.210 ( 0.206)\tData  0.013 ( 0.015)\tLoss 7.5391e-01 (8.1310e-01)\tAcc@1  81.00 ( 63.15)\n",
      "[xla:4]Train:  Epoch: [27][120/129]\tTime  0.187 ( 0.207)\tData  0.009 ( 0.016)\tLoss 8.3594e-01 (7.9328e-01)\tAcc@1  75.00 ( 65.15)\n",
      "[xla:1]Train:  Epoch: [27][120/129]\tTime  0.197 ( 0.212)\tData  0.010 ( 0.015)\tLoss 8.8281e-01 (7.8830e-01)\tAcc@1  62.50 ( 64.90)\n",
      "[xla:3]Train:  Epoch: [27][120/129]\tTime  0.212 ( 0.212)\tData  0.011 ( 0.015)\tLoss 5.8203e-01 (7.7474e-01)\tAcc@1  69.00 ( 65.37)\n",
      "[xla:6]Validation: [0/6]\tTime  0.070 ( 0.070)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Validation: [0/6]\tTime  0.075 ( 0.075)\tLoss 9.6875e-01 (9.6875e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.130 ( 0.130)\tLoss 6.9922e-01 (6.9922e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Finished training epoch 27\n",
      "[xla:4]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:5]Validation: [0/6]\tTime  0.087 ( 0.087)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Validation: [0/6]\tTime  0.067 ( 0.067)\tLoss 5.9375e-01 (5.9375e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.110 ( 0.110)\tLoss 8.6328e-01 (8.6328e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 1.3125e+00 (1.3125e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [28][  0/129]\tTime  0.185 ( 0.185)\tData  0.056 ( 0.056)\tLoss 8.7891e-01 (8.7891e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Train:  Epoch: [28][  0/129]\tTime  0.192 ( 0.192)\tData  0.050 ( 0.050)\tLoss 1.1094e+00 (1.1094e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:3]Train:  Epoch: [28][  0/129]\tTime  0.249 ( 0.249)\tData  0.087 ( 0.087)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [28][  0/129]\tTime  0.166 ( 0.166)\tData  0.012 ( 0.012)\tLoss 8.2031e-01 (8.2031e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Train:  Epoch: [28][  0/129]\tTime  0.201 ( 0.201)\tData  0.073 ( 0.073)\tLoss 9.9219e-01 (9.9219e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [28][  0/129]\tTime  0.185 ( 0.185)\tData  0.044 ( 0.044)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Train:  Epoch: [28][  0/129]\tTime  0.189 ( 0.189)\tData  0.029 ( 0.029)\tLoss 9.3359e-01 (9.3359e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [28][  0/129]\tTime  0.262 ( 0.262)\tData  0.100 ( 0.100)\tLoss 7.8516e-01 (7.8516e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Train:  Epoch: [28][ 30/129]\tTime  0.178 ( 0.220)\tData  0.012 ( 0.017)\tLoss 9.1797e-01 (8.4740e-01)\tAcc@1  50.00 ( 60.13)\n",
      "[xla:1]Train:  Epoch: [28][ 30/129]\tTime  0.183 ( 0.208)\tData  0.012 ( 0.018)\tLoss 7.5781e-01 (8.0733e-01)\tAcc@1  62.50 ( 65.10)\n",
      "[xla:6]Train:  Epoch: [28][ 30/129]\tTime  0.189 ( 0.220)\tData  0.013 ( 0.015)\tLoss 8.5156e-01 (7.9688e-01)\tAcc@1  62.50 ( 61.91)\n",
      "[xla:4]Train:  Epoch: [28][ 30/129]\tTime  0.182 ( 0.209)\tData  0.011 ( 0.015)\tLoss 6.4844e-01 (8.1893e-01)\tAcc@1  81.00 ( 62.34)\n",
      "[xla:5]Train:  Epoch: [28][ 30/129]\tTime  0.209 ( 0.210)\tData  0.012 ( 0.015)\tLoss 7.0312e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.90)\n",
      "[xla:7]Train:  Epoch: [28][ 30/129]\tTime  0.213 ( 0.224)\tData  0.016 ( 0.016)\tLoss 8.3984e-01 (8.6341e-01)\tAcc@1  62.50 ( 60.70)\n",
      "[xla:0]Train:  Epoch: [28][ 30/129]\tTime  0.212 ( 0.209)\tData  0.011 ( 0.016)\tLoss 9.5703e-01 (8.9289e-01)\tAcc@1  50.00 ( 57.28)\n",
      "[xla:2]Train:  Epoch: [28][ 30/129]\tTime  0.183 ( 0.209)\tData  0.009 ( 0.015)\tLoss 9.2578e-01 (8.5761e-01)\tAcc@1  62.50 ( 58.69)\n",
      "[xla:1]Train:  Epoch: [28][ 60/129]\tTime  0.167 ( 0.209)\tData  0.011 ( 0.017)\tLoss 8.7109e-01 (8.2198e-01)\tAcc@1  50.00 ( 63.64)\n",
      "[xla:7]Train:  Epoch: [28][ 60/129]\tTime  0.185 ( 0.218)\tData  0.012 ( 0.015)\tLoss 5.0000e-01 (8.4817e-01)\tAcc@1  87.50 ( 62.94)\n",
      "[xla:6]Train:  Epoch: [28][ 60/129]\tTime  0.215 ( 0.216)\tData  0.012 ( 0.015)\tLoss 9.4141e-01 (8.1186e-01)\tAcc@1  43.75 ( 61.72)\n",
      "[xla:5]Train:  Epoch: [28][ 60/129]\tTime  0.185 ( 0.210)\tData  0.010 ( 0.015)\tLoss 1.1172e+00 (8.3888e-01)\tAcc@1  50.00 ( 61.91)\n",
      "[xla:3]Train:  Epoch: [28][ 60/129]\tTime  0.177 ( 0.216)\tData  0.010 ( 0.016)\tLoss 5.4688e-01 (8.1554e-01)\tAcc@1  81.00 ( 62.55)\n",
      "[xla:4]Train:  Epoch: [28][ 60/129]\tTime  0.210 ( 0.210)\tData  0.010 ( 0.017)\tLoss 8.2812e-01 (8.0520e-01)\tAcc@1  62.50 ( 64.28)\n",
      "[xla:2]Train:  Epoch: [28][ 60/129]\tTime  0.236 ( 0.210)\tData  0.011 ( 0.015)\tLoss 9.7266e-01 (8.4711e-01)\tAcc@1  56.25 ( 59.86)\n",
      "[xla:0]Train:  Epoch: [28][ 60/129]\tTime  0.193 ( 0.210)\tData  0.012 ( 0.016)\tLoss 6.0156e-01 (8.2787e-01)\tAcc@1  81.00 ( 62.01)\n",
      "[xla:2]Train:  Epoch: [28][ 90/129]\tTime  0.181 ( 0.209)\tData  0.012 ( 0.015)\tLoss 6.0547e-01 (8.5115e-01)\tAcc@1  69.00 ( 59.85)\n",
      "[xla:0]Train:  Epoch: [28][ 90/129]\tTime  0.181 ( 0.209)\tData  0.011 ( 0.015)\tLoss 1.1406e+00 (8.0992e-01)\tAcc@1  50.00 ( 63.14)\n",
      "[xla:4]Train:  Epoch: [28][ 90/129]\tTime  0.172 ( 0.209)\tData  0.008 ( 0.016)\tLoss 8.6719e-01 (8.0812e-01)\tAcc@1  56.25 ( 64.02)\n",
      "[xla:7]Train:  Epoch: [28][ 90/129]\tTime  0.190 ( 0.214)\tData  0.011 ( 0.015)\tLoss 5.7031e-01 (8.3160e-01)\tAcc@1  75.00 ( 63.76)\n",
      "[xla:3]Train:  Epoch: [28][ 90/129]\tTime  0.236 ( 0.213)\tData  0.012 ( 0.016)\tLoss 5.2344e-01 (8.0027e-01)\tAcc@1  81.00 ( 63.57)\n",
      "[xla:5]Train:  Epoch: [28][ 90/129]\tTime  0.226 ( 0.209)\tData  0.013 ( 0.015)\tLoss 7.0703e-01 (8.3117e-01)\tAcc@1  62.50 ( 62.32)\n",
      "[xla:6]Train:  Epoch: [28][ 90/129]\tTime  0.221 ( 0.213)\tData  0.013 ( 0.015)\tLoss 6.1328e-01 (8.0694e-01)\tAcc@1  69.00 ( 62.13)\n",
      "[xla:1]Train:  Epoch: [28][ 90/129]\tTime  0.214 ( 0.209)\tData  0.014 ( 0.016)\tLoss 5.9766e-01 (8.1027e-01)\tAcc@1  81.00 ( 63.74)\n",
      "[xla:7]Train:  Epoch: [28][120/129]\tTime  0.230 ( 0.212)\tData  0.010 ( 0.015)\tLoss 8.7891e-01 (8.0788e-01)\tAcc@1  69.00 ( 64.48)\n",
      "[xla:1]Train:  Epoch: [28][120/129]\tTime  0.253 ( 0.208)\tData  0.011 ( 0.016)\tLoss 6.8750e-01 (7.8052e-01)\tAcc@1  69.00 ( 64.64)\n",
      "[xla:0]Train:  Epoch: [28][120/129]\tTime  0.263 ( 0.209)\tData  0.013 ( 0.015)\tLoss 7.1094e-01 (7.7544e-01)\tAcc@1  69.00 ( 64.85)\n",
      "[xla:5]Train:  Epoch: [28][120/129]\tTime  0.229 ( 0.209)\tData  0.009 ( 0.016)\tLoss 3.8281e-01 (8.1491e-01)\tAcc@1  87.50 ( 62.78)\n",
      "[xla:6]Train:  Epoch: [28][120/129]\tTime  0.246 ( 0.212)\tData  0.011 ( 0.015)\tLoss 6.0547e-01 (7.8617e-01)\tAcc@1  75.00 ( 64.03)\n",
      "[xla:3]Train:  Epoch: [28][120/129]\tTime  0.254 ( 0.212)\tData  0.012 ( 0.016)\tLoss 5.8203e-01 (7.8096e-01)\tAcc@1  75.00 ( 65.23)\n",
      "[xla:4]Train:  Epoch: [28][120/129]\tTime  0.257 ( 0.209)\tData  0.013 ( 0.015)\tLoss 8.0469e-01 (7.8811e-01)\tAcc@1  69.00 ( 65.16)\n",
      "[xla:2]Train:  Epoch: [28][120/129]\tTime  0.187 ( 0.209)\tData  0.008 ( 0.015)\tLoss 7.6172e-01 (8.0512e-01)\tAcc@1  81.00 ( 62.83)\n",
      "Finished training epoch 28\n",
      "[xla:2]Validation: [0/6]\tTime  0.062 ( 0.062)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/6]\tTime  0.056 ( 0.056)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 1.2891e+00 (1.2891e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.076 ( 0.076)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 5.8594e-01 (5.8594e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.093 ( 0.093)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.076 ( 0.076)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.107 ( 0.107)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [29][  0/129]\tTime  0.180 ( 0.180)\tData  0.053 ( 0.053)\tLoss 9.1797e-01 (9.1797e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [29][  0/129]\tTime  0.316 ( 0.316)\tData  0.076 ( 0.076)\tLoss 8.4375e-01 (8.4375e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [29][  0/129]\tTime  0.268 ( 0.268)\tData  0.060 ( 0.060)\tLoss 7.3047e-01 (7.3047e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [29][  0/129]\tTime  0.254 ( 0.254)\tData  0.055 ( 0.055)\tLoss 7.7344e-01 (7.7344e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:7]Train:  Epoch: [29][  0/129]\tTime  0.322 ( 0.322)\tData  0.077 ( 0.077)\tLoss 9.4141e-01 (9.4141e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Train:  Epoch: [29][  0/129]\tTime  0.158 ( 0.158)\tData  0.035 ( 0.035)\tLoss 9.7656e-01 (9.7656e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [29][  0/129]\tTime  0.274 ( 0.274)\tData  0.073 ( 0.073)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [29][  0/129]\tTime  0.273 ( 0.273)\tData  0.083 ( 0.083)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Train:  Epoch: [29][ 30/129]\tTime  0.180 ( 0.219)\tData  0.012 ( 0.017)\tLoss 1.0234e+00 (8.7954e-01)\tAcc@1  62.50 ( 59.54)\n",
      "[xla:2]Train:  Epoch: [29][ 30/129]\tTime  0.192 ( 0.229)\tData  0.011 ( 0.017)\tLoss 8.9844e-01 (8.7109e-01)\tAcc@1  62.50 ( 60.55)\n",
      "[xla:5]Train:  Epoch: [29][ 30/129]\tTime  0.197 ( 0.223)\tData  0.013 ( 0.016)\tLoss 7.1094e-01 (8.3707e-01)\tAcc@1  75.00 ( 62.94)\n",
      "[xla:4]Train:  Epoch: [29][ 30/129]\tTime  0.208 ( 0.218)\tData  0.010 ( 0.019)\tLoss 7.5391e-01 (8.1502e-01)\tAcc@1  69.00 ( 62.37)\n",
      "[xla:1]Train:  Epoch: [29][ 30/129]\tTime  0.183 ( 0.221)\tData  0.008 ( 0.017)\tLoss 7.8516e-01 (7.9738e-01)\tAcc@1  50.00 ( 64.40)\n",
      "[xla:3]Train:  Epoch: [29][ 30/129]\tTime  0.240 ( 0.219)\tData  0.017 ( 0.018)\tLoss 8.2422e-01 (8.2586e-01)\tAcc@1  56.25 ( 60.35)\n",
      "[xla:7]Train:  Epoch: [29][ 30/129]\tTime  0.215 ( 0.223)\tData  0.008 ( 0.019)\tLoss 9.1797e-01 (8.5698e-01)\tAcc@1  56.25 ( 61.12)\n",
      "[xla:6]Train:  Epoch: [29][ 30/129]\tTime  0.244 ( 0.217)\tData  0.011 ( 0.017)\tLoss 8.8672e-01 (7.9410e-01)\tAcc@1  50.00 ( 62.72)\n",
      "[xla:5]Train:  Epoch: [29][ 60/129]\tTime  0.163 ( 0.218)\tData  0.011 ( 0.016)\tLoss 1.1328e+00 (8.4010e-01)\tAcc@1  37.50 ( 61.84)\n",
      "[xla:1]Train:  Epoch: [29][ 60/129]\tTime  0.181 ( 0.217)\tData  0.010 ( 0.015)\tLoss 8.3594e-01 (8.0706e-01)\tAcc@1  56.25 ( 64.20)\n",
      "[xla:4]Train:  Epoch: [29][ 60/129]\tTime  0.262 ( 0.216)\tData  0.024 ( 0.018)\tLoss 9.2969e-01 (7.9944e-01)\tAcc@1  56.25 ( 64.50)\n",
      "[xla:2]Train:  Epoch: [29][ 60/129]\tTime  0.204 ( 0.222)\tData  0.014 ( 0.016)\tLoss 8.9453e-01 (8.4225e-01)\tAcc@1  56.25 ( 61.32)\n",
      "[xla:6]Train:  Epoch: [29][ 60/129]\tTime  0.248 ( 0.215)\tData  0.013 ( 0.017)\tLoss 9.9609e-01 (8.0546e-01)\tAcc@1  62.50 ( 62.43)\n",
      "[xla:3]Train:  Epoch: [29][ 60/129]\tTime  0.212 ( 0.217)\tData  0.011 ( 0.016)\tLoss 5.1562e-01 (8.0488e-01)\tAcc@1  87.50 ( 63.86)\n",
      "[xla:7]Train:  Epoch: [29][ 60/129]\tTime  0.211 ( 0.219)\tData  0.010 ( 0.017)\tLoss 5.3516e-01 (8.3331e-01)\tAcc@1  75.00 ( 62.24)\n",
      "[xla:0]Train:  Epoch: [29][ 60/129]\tTime  0.195 ( 0.218)\tData  0.008 ( 0.016)\tLoss 6.7578e-01 (8.3581e-01)\tAcc@1  69.00 ( 62.25)\n",
      "[xla:3]Train:  Epoch: [29][ 90/129]\tTime  0.199 ( 0.213)\tData  0.012 ( 0.015)\tLoss 5.8203e-01 (7.8666e-01)\tAcc@1  69.00 ( 64.18)\n",
      "[xla:2]Train:  Epoch: [29][ 90/129]\tTime  0.211 ( 0.217)\tData  0.015 ( 0.016)\tLoss 5.9766e-01 (8.4763e-01)\tAcc@1  69.00 ( 61.81)\n",
      "[xla:6]Train:  Epoch: [29][ 90/129]\tTime  0.201 ( 0.213)\tData  0.011 ( 0.016)\tLoss 6.0547e-01 (7.9679e-01)\tAcc@1  69.00 ( 63.83)\n",
      "[xla:5]Train:  Epoch: [29][ 90/129]\tTime  0.213 ( 0.215)\tData  0.012 ( 0.015)\tLoss 7.0312e-01 (8.2735e-01)\tAcc@1  75.00 ( 62.74)\n",
      "[xla:0]Train:  Epoch: [29][ 90/129]\tTime  0.187 ( 0.214)\tData  0.008 ( 0.015)\tLoss 1.2422e+00 (8.1538e-01)\tAcc@1  37.50 ( 63.38)\n",
      "[xla:1]Train:  Epoch: [29][ 90/129]\tTime  0.204 ( 0.214)\tData  0.009 ( 0.015)\tLoss 5.7031e-01 (7.9338e-01)\tAcc@1  62.50 ( 64.83)\n",
      "[xla:7]Train:  Epoch: [29][ 90/129]\tTime  0.210 ( 0.215)\tData  0.011 ( 0.016)\tLoss 6.6406e-01 (8.3362e-01)\tAcc@1  69.00 ( 60.97)\n",
      "[xla:4]Train:  Epoch: [29][ 90/129]\tTime  0.245 ( 0.214)\tData  0.012 ( 0.017)\tLoss 8.5547e-01 (8.0168e-01)\tAcc@1  50.00 ( 64.81)\n",
      "[xla:1]Train:  Epoch: [29][120/129]\tTime  0.161 ( 0.212)\tData  0.012 ( 0.015)\tLoss 8.7109e-01 (7.7671e-01)\tAcc@1  62.50 ( 65.24)\n",
      "[xla:6]Train:  Epoch: [29][120/129]\tTime  0.165 ( 0.210)\tData  0.014 ( 0.016)\tLoss 4.4531e-01 (7.7366e-01)\tAcc@1  87.50 ( 65.37)\n",
      "[xla:2]Train:  Epoch: [29][120/129]\tTime  0.185 ( 0.214)\tData  0.014 ( 0.015)\tLoss 8.1250e-01 (8.0653e-01)\tAcc@1  50.00 ( 64.10)\n",
      "[xla:4]Train:  Epoch: [29][120/129]\tTime  0.215 ( 0.211)\tData  0.015 ( 0.017)\tLoss 8.1641e-01 (7.8366e-01)\tAcc@1  62.50 ( 65.79)\n",
      "[xla:7]Train:  Epoch: [29][120/129]\tTime  0.190 ( 0.212)\tData  0.012 ( 0.015)\tLoss 9.2578e-01 (8.0477e-01)\tAcc@1  62.50 ( 63.11)\n",
      "[xla:3]Train:  Epoch: [29][120/129]\tTime  0.229 ( 0.211)\tData  0.020 ( 0.015)\tLoss 5.3516e-01 (7.7027e-01)\tAcc@1  81.00 ( 65.33)\n",
      "[xla:0]Train:  Epoch: [29][120/129]\tTime  0.212 ( 0.212)\tData  0.010 ( 0.015)\tLoss 8.4766e-01 (7.8680e-01)\tAcc@1  56.25 ( 65.33)\n",
      "[xla:5]Train:  Epoch: [29][120/129]\tTime  0.199 ( 0.213)\tData  0.010 ( 0.015)\tLoss 4.1406e-01 (8.1311e-01)\tAcc@1  75.00 ( 63.27)\n",
      "Finished training epoch 29\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/6]\tTime  0.096 ( 0.096)\tLoss 9.7656e-01 (9.7656e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.072 ( 0.072)\tLoss 5.7812e-01 (5.7812e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.130 ( 0.130)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.118 ( 0.118)\tLoss 1.2891e+00 (1.2891e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Validation: [0/6]\tTime  0.075 ( 0.075)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.116 ( 0.116)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [30][  0/129]\tTime  0.142 ( 0.142)\tData  0.038 ( 0.038)\tLoss 9.2969e-01 (9.2969e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [30][  0/129]\tTime  0.197 ( 0.197)\tData  0.035 ( 0.035)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [30][  0/129]\tTime  0.165 ( 0.165)\tData  0.041 ( 0.041)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [30][  0/129]\tTime  0.221 ( 0.221)\tData  0.078 ( 0.078)\tLoss 7.5000e-01 (7.5000e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Train:  Epoch: [30][  0/129]\tTime  0.203 ( 0.203)\tData  0.047 ( 0.047)\tLoss 9.0625e-01 (9.0625e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [30][  0/129]\tTime  0.183 ( 0.183)\tData  0.039 ( 0.039)\tLoss 6.3672e-01 (6.3672e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [30][  0/129]\tTime  0.221 ( 0.221)\tData  0.093 ( 0.093)\tLoss 9.0234e-01 (9.0234e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [30][  0/129]\tTime  0.244 ( 0.244)\tData  0.094 ( 0.094)\tLoss 1.1484e+00 (1.1484e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [30][ 30/129]\tTime  0.170 ( 0.220)\tData  0.011 ( 0.016)\tLoss 9.8438e-01 (8.9163e-01)\tAcc@1  69.00 ( 57.10)\n",
      "[xla:1]Train:  Epoch: [30][ 30/129]\tTime  0.182 ( 0.209)\tData  0.013 ( 0.016)\tLoss 8.8281e-01 (7.9889e-01)\tAcc@1  50.00 ( 64.35)\n",
      "[xla:4]Train:  Epoch: [30][ 30/129]\tTime  0.199 ( 0.211)\tData  0.014 ( 0.017)\tLoss 7.0312e-01 (8.4488e-01)\tAcc@1  75.00 ( 59.33)\n",
      "[xla:0]Train:  Epoch: [30][ 30/129]\tTime  0.202 ( 0.215)\tData  0.011 ( 0.015)\tLoss 1.0156e+00 (8.8319e-01)\tAcc@1  56.25 ( 58.31)\n",
      "[xla:3]Train:  Epoch: [30][ 30/129]\tTime  0.219 ( 0.206)\tData  0.011 ( 0.014)\tLoss 9.5703e-01 (8.4740e-01)\tAcc@1  43.75 ( 62.34)\n",
      "[xla:2]Train:  Epoch: [30][ 30/129]\tTime  0.225 ( 0.207)\tData  0.012 ( 0.016)\tLoss 8.8281e-01 (8.6731e-01)\tAcc@1  56.25 ( 60.70)\n",
      "[xla:5]Train:  Epoch: [30][ 30/129]\tTime  0.189 ( 0.207)\tData  0.008 ( 0.016)\tLoss 7.3438e-01 (8.4299e-01)\tAcc@1  69.00 ( 62.34)\n",
      "[xla:6]Train:  Epoch: [30][ 30/129]\tTime  0.202 ( 0.206)\tData  0.009 ( 0.017)\tLoss 8.6719e-01 (7.9385e-01)\tAcc@1  50.00 ( 64.55)\n",
      "[xla:2]Train:  Epoch: [30][ 60/129]\tTime  0.179 ( 0.206)\tData  0.012 ( 0.016)\tLoss 9.1016e-01 (8.2768e-01)\tAcc@1  56.25 ( 62.93)\n",
      "[xla:3]Train:  Epoch: [30][ 60/129]\tTime  0.198 ( 0.206)\tData  0.010 ( 0.015)\tLoss 6.9922e-01 (8.2351e-01)\tAcc@1  62.50 ( 63.07)\n",
      "[xla:1]Train:  Epoch: [30][ 60/129]\tTime  0.202 ( 0.208)\tData  0.012 ( 0.014)\tLoss 9.4922e-01 (8.1548e-01)\tAcc@1  56.25 ( 62.44)\n",
      "[xla:5]Train:  Epoch: [30][ 60/129]\tTime  0.230 ( 0.206)\tData  0.010 ( 0.015)\tLoss 9.8047e-01 (8.4209e-01)\tAcc@1  56.25 ( 63.06)\n",
      "[xla:6]Train:  Epoch: [30][ 60/129]\tTime  0.200 ( 0.206)\tData  0.012 ( 0.016)\tLoss 7.3047e-01 (8.1628e-01)\tAcc@1  69.00 ( 63.05)\n",
      "[xla:7]Train:  Epoch: [30][ 60/129]\tTime  0.198 ( 0.214)\tData  0.012 ( 0.014)\tLoss 4.3945e-01 (8.3783e-01)\tAcc@1  87.50 ( 60.36)\n",
      "[xla:4]Train:  Epoch: [30][ 60/129]\tTime  0.235 ( 0.209)\tData  0.014 ( 0.017)\tLoss 8.7109e-01 (8.1961e-01)\tAcc@1  56.25 ( 63.05)\n",
      "[xla:0]Train:  Epoch: [30][ 60/129]\tTime  0.185 ( 0.211)\tData  0.008 ( 0.014)\tLoss 7.8516e-01 (8.3405e-01)\tAcc@1  62.50 ( 62.21)\n",
      "[xla:2]Train:  Epoch: [30][ 90/129]\tTime  0.166 ( 0.206)\tData  0.011 ( 0.015)\tLoss 6.2109e-01 (8.3864e-01)\tAcc@1  81.00 ( 61.97)\n",
      "[xla:3]Train:  Epoch: [30][ 90/129]\tTime  0.166 ( 0.206)\tData  0.012 ( 0.014)\tLoss 6.0156e-01 (8.0185e-01)\tAcc@1  69.00 ( 64.19)\n",
      "[xla:5]Train:  Epoch: [30][ 90/129]\tTime  0.202 ( 0.206)\tData  0.011 ( 0.014)\tLoss 7.6953e-01 (8.2718e-01)\tAcc@1  69.00 ( 63.86)\n",
      "[xla:6]Train:  Epoch: [30][ 90/129]\tTime  0.194 ( 0.206)\tData  0.011 ( 0.016)\tLoss 6.2109e-01 (8.1555e-01)\tAcc@1  69.00 ( 62.60)\n",
      "[xla:7]Train:  Epoch: [30][ 90/129]\tTime  0.200 ( 0.211)\tData  0.011 ( 0.014)\tLoss 6.4453e-01 (8.3257e-01)\tAcc@1  69.00 ( 60.46)\n",
      "[xla:1]Train:  Epoch: [30][ 90/129]\tTime  0.191 ( 0.207)\tData  0.011 ( 0.014)\tLoss 6.4844e-01 (8.0042e-01)\tAcc@1  81.00 ( 64.32)\n",
      "[xla:4]Train:  Epoch: [30][ 90/129]\tTime  0.188 ( 0.208)\tData  0.011 ( 0.016)\tLoss 9.2969e-01 (8.0728e-01)\tAcc@1  43.75 ( 64.18)\n",
      "[xla:0]Train:  Epoch: [30][ 90/129]\tTime  0.216 ( 0.209)\tData  0.011 ( 0.014)\tLoss 1.1172e+00 (8.1231e-01)\tAcc@1  37.50 ( 62.87)\n",
      "[xla:7]Train:  Epoch: [30][120/129]\tTime  0.164 ( 0.210)\tData  0.013 ( 0.014)\tLoss 8.1250e-01 (8.0616e-01)\tAcc@1  62.50 ( 62.37)\n",
      "[xla:2]Train:  Epoch: [30][120/129]\tTime  0.189 ( 0.206)\tData  0.011 ( 0.015)\tLoss 8.9062e-01 (8.0038e-01)\tAcc@1  50.00 ( 64.18)\n",
      "[xla:1]Train:  Epoch: [30][120/129]\tTime  0.207 ( 0.207)\tData  0.012 ( 0.014)\tLoss 6.5625e-01 (7.7536e-01)\tAcc@1  69.00 ( 65.63)\n",
      "[xla:5]Train:  Epoch: [30][120/129]\tTime  0.179 ( 0.206)\tData  0.019 ( 0.015)\tLoss 2.8711e-01 (8.1290e-01)\tAcc@1 100.00 ( 64.51)\n",
      "[xla:0]Train:  Epoch: [30][120/129]\tTime  0.188 ( 0.208)\tData  0.010 ( 0.014)\tLoss 8.7891e-01 (7.9011e-01)\tAcc@1  56.25 ( 64.30)\n",
      "[xla:3]Train:  Epoch: [30][120/129]\tTime  0.186 ( 0.206)\tData  0.012 ( 0.014)\tLoss 4.8633e-01 (7.8427e-01)\tAcc@1  87.50 ( 65.58)\n",
      "[xla:4]Train:  Epoch: [30][120/129]\tTime  0.190 ( 0.207)\tData  0.011 ( 0.016)\tLoss 7.7734e-01 (7.8462e-01)\tAcc@1  75.00 ( 65.94)\n",
      "[xla:6]Train:  Epoch: [30][120/129]\tTime  0.201 ( 0.206)\tData  0.015 ( 0.015)\tLoss 5.1172e-01 (7.8822e-01)\tAcc@1  81.00 ( 63.92)\n",
      "Finished training epoch 30\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.073 ( 0.073)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 1.2812e+00 (1.2812e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.053 ( 0.053)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.121 ( 0.121)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.119 ( 0.119)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.153 ( 0.153)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.136 ( 0.136)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [31][  0/129]\tTime  0.146 ( 0.146)\tData  0.030 ( 0.030)\tLoss 5.4297e-01 (5.4297e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [31][  0/129]\tTime  0.180 ( 0.180)\tData  0.045 ( 0.045)\tLoss 9.6094e-01 (9.6094e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [31][  0/129]\tTime  0.178 ( 0.178)\tData  0.053 ( 0.053)\tLoss 7.6172e-01 (7.6172e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [31][  0/129]\tTime  0.252 ( 0.252)\tData  0.076 ( 0.076)\tLoss 9.1016e-01 (9.1016e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Train:  Epoch: [31][  0/129]\tTime  0.201 ( 0.201)\tData  0.049 ( 0.049)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:2]Train:  Epoch: [31][  0/129]\tTime  0.224 ( 0.224)\tData  0.101 ( 0.101)\tLoss 8.7109e-01 (8.7109e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [31][  0/129]\tTime  0.188 ( 0.188)\tData  0.039 ( 0.039)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Train:  Epoch: [31][  0/129]\tTime  0.214 ( 0.214)\tData  0.071 ( 0.071)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Train:  Epoch: [31][ 30/129]\tTime  0.197 ( 0.227)\tData  0.012 ( 0.017)\tLoss 8.4375e-01 (8.1760e-01)\tAcc@1  50.00 ( 62.76)\n",
      "[xla:7]Train:  Epoch: [31][ 30/129]\tTime  0.260 ( 0.229)\tData  0.014 ( 0.021)\tLoss 8.9453e-01 (8.6996e-01)\tAcc@1  62.50 ( 59.48)\n",
      "[xla:6]Train:  Epoch: [31][ 30/129]\tTime  0.190 ( 0.224)\tData  0.010 ( 0.018)\tLoss 8.5156e-01 (7.7949e-01)\tAcc@1  56.25 ( 66.38)\n",
      "[xla:4]Train:  Epoch: [31][ 30/129]\tTime  0.245 ( 0.220)\tData  0.026 ( 0.015)\tLoss 5.4688e-01 (8.2044e-01)\tAcc@1  87.50 ( 61.12)\n",
      "[xla:0]Train:  Epoch: [31][ 30/129]\tTime  0.241 ( 0.221)\tData  0.018 ( 0.016)\tLoss 9.2578e-01 (8.7021e-01)\tAcc@1  69.00 ( 61.33)\n",
      "[xla:5]Train:  Epoch: [31][ 30/129]\tTime  0.236 ( 0.228)\tData  0.012 ( 0.018)\tLoss 6.7188e-01 (8.3354e-01)\tAcc@1  62.50 ( 63.53)\n",
      "[xla:3]Train:  Epoch: [31][ 30/129]\tTime  0.239 ( 0.234)\tData  0.011 ( 0.018)\tLoss 9.2969e-01 (8.1515e-01)\tAcc@1  43.75 ( 60.13)\n",
      "[xla:2]Train:  Epoch: [31][ 30/129]\tTime  0.286 ( 0.226)\tData  0.011 ( 0.017)\tLoss 8.8281e-01 (8.5874e-01)\tAcc@1  62.50 ( 59.90)\n",
      "[xla:6]Train:  Epoch: [31][ 60/129]\tTime  0.167 ( 0.217)\tData  0.010 ( 0.017)\tLoss 9.8438e-01 (8.0757e-01)\tAcc@1  56.25 ( 63.35)\n",
      "[xla:1]Train:  Epoch: [31][ 60/129]\tTime  0.170 ( 0.219)\tData  0.010 ( 0.016)\tLoss 9.3359e-01 (8.2508e-01)\tAcc@1  50.00 ( 62.74)\n",
      "[xla:2]Train:  Epoch: [31][ 60/129]\tTime  0.196 ( 0.217)\tData  0.010 ( 0.016)\tLoss 1.0859e+00 (8.3997e-01)\tAcc@1  56.25 ( 62.21)\n",
      "[xla:7]Train:  Epoch: [31][ 60/129]\tTime  0.197 ( 0.220)\tData  0.010 ( 0.018)\tLoss 4.9219e-01 (8.3511e-01)\tAcc@1  87.50 ( 62.18)\n",
      "[xla:0]Train:  Epoch: [31][ 60/129]\tTime  0.217 ( 0.215)\tData  0.012 ( 0.016)\tLoss 6.2500e-01 (8.1849e-01)\tAcc@1  62.50 ( 63.27)\n",
      "[xla:3]Train:  Epoch: [31][ 60/129]\tTime  0.224 ( 0.222)\tData  0.013 ( 0.016)\tLoss 5.5859e-01 (8.0357e-01)\tAcc@1  81.00 ( 62.34)\n",
      "[xla:5]Train:  Epoch: [31][ 60/129]\tTime  0.179 ( 0.219)\tData  0.008 ( 0.017)\tLoss 9.2188e-01 (8.3446e-01)\tAcc@1  50.00 ( 62.83)\n",
      "[xla:4]Train:  Epoch: [31][ 60/129]\tTime  0.198 ( 0.215)\tData  0.011 ( 0.015)\tLoss 7.2656e-01 (8.0757e-01)\tAcc@1  69.00 ( 63.25)\n",
      "[xla:6]Train:  Epoch: [31][ 90/129]\tTime  0.178 ( 0.214)\tData  0.010 ( 0.016)\tLoss 4.8828e-01 (7.9846e-01)\tAcc@1  81.00 ( 63.77)\n",
      "[xla:2]Train:  Epoch: [31][ 90/129]\tTime  0.185 ( 0.214)\tData  0.010 ( 0.016)\tLoss 4.2969e-01 (8.4431e-01)\tAcc@1  81.00 ( 62.18)\n",
      "[xla:0]Train:  Epoch: [31][ 90/129]\tTime  0.206 ( 0.213)\tData  0.014 ( 0.016)\tLoss 1.0234e+00 (8.0428e-01)\tAcc@1  50.00 ( 63.77)\n",
      "[xla:5]Train:  Epoch: [31][ 90/129]\tTime  0.184 ( 0.215)\tData  0.014 ( 0.016)\tLoss 5.9375e-01 (8.2302e-01)\tAcc@1  69.00 ( 63.23)\n",
      "[xla:7]Train:  Epoch: [31][ 90/129]\tTime  0.182 ( 0.216)\tData  0.009 ( 0.016)\tLoss 6.0156e-01 (8.3100e-01)\tAcc@1  69.00 ( 62.93)\n",
      "[xla:3]Train:  Epoch: [31][ 90/129]\tTime  0.214 ( 0.217)\tData  0.010 ( 0.015)\tLoss 5.4297e-01 (7.9451e-01)\tAcc@1  69.00 ( 63.04)\n",
      "[xla:4]Train:  Epoch: [31][ 90/129]\tTime  0.188 ( 0.213)\tData  0.011 ( 0.015)\tLoss 9.4531e-01 (8.0617e-01)\tAcc@1  43.75 ( 63.96)\n",
      "[xla:1]Train:  Epoch: [31][ 90/129]\tTime  0.214 ( 0.216)\tData  0.011 ( 0.015)\tLoss 5.8984e-01 (8.0424e-01)\tAcc@1  69.00 ( 63.98)\n",
      "[xla:2]Train:  Epoch: [31][120/129]\tTime  0.203 ( 0.212)\tData  0.010 ( 0.016)\tLoss 8.5938e-01 (8.0252e-01)\tAcc@1  50.00 ( 64.54)\n",
      "[xla:5]Train:  Epoch: [31][120/129]\tTime  0.203 ( 0.213)\tData  0.013 ( 0.016)\tLoss 4.6875e-01 (8.1177e-01)\tAcc@1  75.00 ( 63.77)\n",
      "[xla:7]Train:  Epoch: [31][120/129]\tTime  0.192 ( 0.213)\tData  0.010 ( 0.016)\tLoss 8.8672e-01 (8.0798e-01)\tAcc@1  56.25 ( 64.18)\n",
      "[xla:4]Train:  Epoch: [31][120/129]\tTime  0.233 ( 0.211)\tData  0.012 ( 0.015)\tLoss 8.8281e-01 (7.8924e-01)\tAcc@1  56.25 ( 65.67)\n",
      "[xla:3]Train:  Epoch: [31][120/129]\tTime  0.224 ( 0.214)\tData  0.014 ( 0.015)\tLoss 5.1953e-01 (7.7983e-01)\tAcc@1  81.00 ( 64.30)\n",
      "[xla:0]Train:  Epoch: [31][120/129]\tTime  0.212 ( 0.211)\tData  0.010 ( 0.015)\tLoss 8.1250e-01 (7.7976e-01)\tAcc@1  69.00 ( 65.50)\n",
      "[xla:6]Train:  Epoch: [31][120/129]\tTime  0.220 ( 0.212)\tData  0.014 ( 0.016)\tLoss 5.4297e-01 (7.7780e-01)\tAcc@1  81.00 ( 64.97)\n",
      "[xla:1]Train:  Epoch: [31][120/129]\tTime  0.228 ( 0.213)\tData  0.011 ( 0.015)\tLoss 7.3828e-01 (7.8551e-01)\tAcc@1  75.00 ( 65.32)\n",
      "[xla:4]Validation: [0/6]\tTime  0.089 ( 0.089)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:5]Validation: [0/6]\tTime  0.102 ( 0.102)\tLoss 6.1719e-01 (6.1719e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Finished training epoch 31\n",
      "[xla:6]Validation: [0/6]\tTime  0.117 ( 0.117)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/6]\tTime  0.078 ( 0.078)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/6]\tTime  0.077 ( 0.077)\tLoss 5.7031e-01 (5.7031e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.094 ( 0.094)\tLoss 1.3047e+00 (1.3047e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.113 ( 0.113)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.120 ( 0.120)\tLoss 8.3203e-01 (8.3203e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [32][  0/129]\tTime  0.166 ( 0.166)\tData  0.047 ( 0.047)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [32][  0/129]\tTime  0.181 ( 0.181)\tData  0.050 ( 0.050)\tLoss 9.1016e-01 (9.1016e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [32][  0/129]\tTime  0.225 ( 0.225)\tData  0.093 ( 0.093)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:1]Train:  Epoch: [32][  0/129]\tTime  0.179 ( 0.179)\tData  0.058 ( 0.058)\tLoss 7.7344e-01 (7.7344e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Train:  Epoch: [32][  0/129]\tTime  0.231 ( 0.231)\tData  0.066 ( 0.066)\tLoss 9.8047e-01 (9.8047e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Train:  Epoch: [32][  0/129]\tTime  0.146 ( 0.146)\tData  0.027 ( 0.027)\tLoss 7.6562e-01 (7.6562e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [32][  0/129]\tTime  0.165 ( 0.165)\tData  0.019 ( 0.019)\tLoss 9.6484e-01 (9.6484e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [32][  0/129]\tTime  0.253 ( 0.253)\tData  0.084 ( 0.084)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [32][ 30/129]\tTime  0.154 ( 0.222)\tData  0.010 ( 0.018)\tLoss 7.0703e-01 (8.0456e-01)\tAcc@1  62.50 ( 63.33)\n",
      "[xla:1]Train:  Epoch: [32][ 30/129]\tTime  0.162 ( 0.208)\tData  0.009 ( 0.018)\tLoss 8.6328e-01 (7.9795e-01)\tAcc@1  62.50 ( 64.56)\n",
      "[xla:7]Train:  Epoch: [32][ 30/129]\tTime  0.185 ( 0.208)\tData  0.018 ( 0.016)\tLoss 9.8047e-01 (8.8659e-01)\tAcc@1  50.00 ( 60.12)\n",
      "[xla:3]Train:  Epoch: [32][ 30/129]\tTime  0.202 ( 0.204)\tData  0.012 ( 0.016)\tLoss 9.3750e-01 (8.5156e-01)\tAcc@1  43.75 ( 60.94)\n",
      "[xla:2]Train:  Epoch: [32][ 30/129]\tTime  0.201 ( 0.203)\tData  0.011 ( 0.014)\tLoss 9.0234e-01 (8.7475e-01)\tAcc@1  56.25 ( 59.31)\n",
      "[xla:5]Train:  Epoch: [32][ 30/129]\tTime  0.220 ( 0.219)\tData  0.011 ( 0.016)\tLoss 6.4453e-01 (8.3077e-01)\tAcc@1  75.00 ( 64.18)\n",
      "[xla:6]Train:  Epoch: [32][ 30/129]\tTime  0.201 ( 0.215)\tData  0.018 ( 0.018)\tLoss 9.2188e-01 (7.9309e-01)\tAcc@1  56.25 ( 66.52)\n",
      "[xla:0]Train:  Epoch: [32][ 30/129]\tTime  0.223 ( 0.206)\tData  0.013 ( 0.014)\tLoss 1.0156e+00 (8.9214e-01)\tAcc@1  50.00 ( 59.10)\n",
      "[xla:4]Train:  Epoch: [32][ 60/129]\tTime  0.194 ( 0.214)\tData  0.012 ( 0.016)\tLoss 8.5938e-01 (7.9988e-01)\tAcc@1  56.25 ( 65.50)\n",
      "[xla:2]Train:  Epoch: [32][ 60/129]\tTime  0.188 ( 0.203)\tData  0.010 ( 0.014)\tLoss 8.9453e-01 (8.3357e-01)\tAcc@1  56.25 ( 61.90)\n",
      "[xla:7]Train:  Epoch: [32][ 60/129]\tTime  0.195 ( 0.206)\tData  0.011 ( 0.015)\tLoss 5.0391e-01 (8.3671e-01)\tAcc@1  87.50 ( 61.82)\n",
      "[xla:0]Train:  Epoch: [32][ 60/129]\tTime  0.224 ( 0.205)\tData  0.011 ( 0.014)\tLoss 7.6172e-01 (8.3648e-01)\tAcc@1  69.00 ( 62.84)\n",
      "[xla:1]Train:  Epoch: [32][ 60/129]\tTime  0.208 ( 0.206)\tData  0.011 ( 0.016)\tLoss 9.6875e-01 (8.3226e-01)\tAcc@1  50.00 ( 62.33)\n",
      "[xla:3]Train:  Epoch: [32][ 60/129]\tTime  0.199 ( 0.205)\tData  0.009 ( 0.015)\tLoss 5.5078e-01 (8.1436e-01)\tAcc@1  81.00 ( 63.98)\n",
      "[xla:6]Train:  Epoch: [32][ 60/129]\tTime  0.190 ( 0.210)\tData  0.008 ( 0.017)\tLoss 9.1797e-01 (8.2191e-01)\tAcc@1  62.50 ( 64.56)\n",
      "[xla:5]Train:  Epoch: [32][ 60/129]\tTime  0.215 ( 0.212)\tData  0.011 ( 0.016)\tLoss 9.2969e-01 (8.2716e-01)\tAcc@1  56.25 ( 64.20)\n",
      "[xla:0]Train:  Epoch: [32][ 90/129]\tTime  0.185 ( 0.204)\tData  0.011 ( 0.014)\tLoss 1.0312e+00 (8.1711e-01)\tAcc@1  43.75 ( 63.90)\n",
      "[xla:2]Train:  Epoch: [32][ 90/129]\tTime  0.193 ( 0.203)\tData  0.012 ( 0.014)\tLoss 4.6484e-01 (8.2791e-01)\tAcc@1  81.00 ( 63.07)\n",
      "[xla:5]Train:  Epoch: [32][ 90/129]\tTime  0.238 ( 0.209)\tData  0.018 ( 0.015)\tLoss 7.5781e-01 (8.2418e-01)\tAcc@1  69.00 ( 63.87)\n",
      "[xla:1]Train:  Epoch: [32][ 90/129]\tTime  0.219 ( 0.205)\tData  0.013 ( 0.015)\tLoss 6.7188e-01 (8.1166e-01)\tAcc@1  69.00 ( 63.78)\n",
      "[xla:3]Train:  Epoch: [32][ 90/129]\tTime  0.176 ( 0.204)\tData  0.008 ( 0.015)\tLoss 5.1953e-01 (8.0005e-01)\tAcc@1  81.00 ( 64.12)\n",
      "[xla:7]Train:  Epoch: [32][ 90/129]\tTime  0.200 ( 0.205)\tData  0.009 ( 0.015)\tLoss 5.2344e-01 (8.3405e-01)\tAcc@1  81.00 ( 62.12)\n",
      "[xla:4]Train:  Epoch: [32][ 90/129]\tTime  0.221 ( 0.210)\tData  0.011 ( 0.016)\tLoss 8.7891e-01 (8.0097e-01)\tAcc@1  56.25 ( 65.61)\n",
      "[xla:6]Train:  Epoch: [32][ 90/129]\tTime  0.199 ( 0.207)\tData  0.009 ( 0.016)\tLoss 5.7031e-01 (8.0877e-01)\tAcc@1  81.00 ( 64.09)\n",
      "[xla:2]Train:  Epoch: [32][120/129]\tTime  0.186 ( 0.204)\tData  0.013 ( 0.014)\tLoss 8.6328e-01 (7.9119e-01)\tAcc@1  56.25 ( 64.85)\n",
      "[xla:5]Train:  Epoch: [32][120/129]\tTime  0.192 ( 0.208)\tData  0.010 ( 0.015)\tLoss 4.2578e-01 (8.0754e-01)\tAcc@1  81.00 ( 64.77)\n",
      "[xla:4]Train:  Epoch: [32][120/129]\tTime  0.196 ( 0.209)\tData  0.011 ( 0.016)\tLoss 7.4219e-01 (7.7896e-01)\tAcc@1  69.00 ( 66.60)\n",
      "[xla:1]Train:  Epoch: [32][120/129]\tTime  0.218 ( 0.206)\tData  0.011 ( 0.016)\tLoss 8.1250e-01 (7.8165e-01)\tAcc@1  62.50 ( 65.49)\n",
      "[xla:3]Train:  Epoch: [32][120/129]\tTime  0.231 ( 0.205)\tData  0.014 ( 0.015)\tLoss 4.5898e-01 (7.8254e-01)\tAcc@1  62.50 ( 65.18)\n",
      "[xla:0]Train:  Epoch: [32][120/129]\tTime  0.185 ( 0.205)\tData  0.008 ( 0.014)\tLoss 6.6797e-01 (7.9001e-01)\tAcc@1  75.00 ( 65.67)\n",
      "[xla:6]Train:  Epoch: [32][120/129]\tTime  0.203 ( 0.207)\tData  0.009 ( 0.016)\tLoss 4.7070e-01 (7.8018e-01)\tAcc@1  81.00 ( 65.30)\n",
      "[xla:7]Train:  Epoch: [32][120/129]\tTime  0.199 ( 0.206)\tData  0.008 ( 0.015)\tLoss 7.0703e-01 (8.1026e-01)\tAcc@1  69.00 ( 63.67)\n",
      "[xla:2]Validation: [0/6]\tTime  0.060 ( 0.060)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 32\n",
      "[xla:6]Validation: [0/6]\tTime  0.090 ( 0.090)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/6]\tTime  0.108 ( 0.108)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:5]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 6.2891e-01 (6.2891e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.088 ( 0.088)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Validation: [0/6]\tTime  0.132 ( 0.132)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.136 ( 0.136)\tLoss 1.3203e+00 (1.3203e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.161 ( 0.161)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [33][  0/129]\tTime  0.150 ( 0.150)\tData  0.043 ( 0.043)\tLoss 8.8281e-01 (8.8281e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Train:  Epoch: [33][  0/129]\tTime  0.227 ( 0.227)\tData  0.074 ( 0.074)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [33][  0/129]\tTime  0.195 ( 0.195)\tData  0.065 ( 0.065)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [33][  0/129]\tTime  0.211 ( 0.211)\tData  0.055 ( 0.055)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [33][  0/129]\tTime  0.219 ( 0.219)\tData  0.049 ( 0.049)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:7]Train:  Epoch: [33][  0/129]\tTime  0.285 ( 0.285)\tData  0.103 ( 0.103)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [33][  0/129]\tTime  0.272 ( 0.272)\tData  0.104 ( 0.104)\tLoss 7.3438e-01 (7.3438e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [33][  0/129]\tTime  0.251 ( 0.251)\tData  0.071 ( 0.071)\tLoss 7.6172e-01 (7.6172e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:3]Train:  Epoch: [33][ 30/129]\tTime  0.182 ( 0.219)\tData  0.011 ( 0.017)\tLoss 9.4531e-01 (8.3783e-01)\tAcc@1  43.75 ( 59.54)\n",
      "[xla:7]Train:  Epoch: [33][ 30/129]\tTime  0.181 ( 0.214)\tData  0.010 ( 0.017)\tLoss 9.6094e-01 (8.9163e-01)\tAcc@1  43.75 ( 56.87)\n",
      "[xla:5]Train:  Epoch: [33][ 30/129]\tTime  0.173 ( 0.218)\tData  0.010 ( 0.018)\tLoss 8.1641e-01 (8.5377e-01)\tAcc@1  56.25 ( 62.15)\n",
      "[xla:4]Train:  Epoch: [33][ 30/129]\tTime  0.208 ( 0.216)\tData  0.011 ( 0.017)\tLoss 7.1094e-01 (8.3669e-01)\tAcc@1  81.00 ( 62.52)\n",
      "[xla:0]Train:  Epoch: [33][ 30/129]\tTime  0.222 ( 0.213)\tData  0.011 ( 0.014)\tLoss 9.6875e-01 (8.9541e-01)\tAcc@1  56.25 ( 58.68)\n",
      "[xla:2]Train:  Epoch: [33][ 30/129]\tTime  0.199 ( 0.232)\tData  0.010 ( 0.016)\tLoss 9.1016e-01 (8.4438e-01)\tAcc@1  56.25 ( 61.74)\n",
      "[xla:6]Train:  Epoch: [33][ 30/129]\tTime  0.201 ( 0.221)\tData  0.009 ( 0.016)\tLoss 8.9062e-01 (7.7848e-01)\tAcc@1  43.75 ( 64.96)\n",
      "[xla:1]Train:  Epoch: [33][ 30/129]\tTime  0.201 ( 0.215)\tData  0.008 ( 0.016)\tLoss 9.3750e-01 (8.2012e-01)\tAcc@1  62.50 ( 63.95)\n",
      "[xla:7]Train:  Epoch: [33][ 60/129]\tTime  0.172 ( 0.214)\tData  0.012 ( 0.016)\tLoss 5.2734e-01 (8.3991e-01)\tAcc@1  81.00 ( 62.12)\n",
      "[xla:4]Train:  Epoch: [33][ 60/129]\tTime  0.189 ( 0.215)\tData  0.012 ( 0.015)\tLoss 8.0859e-01 (8.1769e-01)\tAcc@1  69.00 ( 64.38)\n",
      "[xla:0]Train:  Epoch: [33][ 60/129]\tTime  0.169 ( 0.213)\tData  0.011 ( 0.015)\tLoss 7.1094e-01 (8.4317e-01)\tAcc@1  75.00 ( 62.92)\n",
      "[xla:2]Train:  Epoch: [33][ 60/129]\tTime  0.211 ( 0.223)\tData  0.016 ( 0.016)\tLoss 7.9297e-01 (8.2489e-01)\tAcc@1  62.50 ( 61.61)\n",
      "[xla:5]Train:  Epoch: [33][ 60/129]\tTime  0.194 ( 0.216)\tData  0.011 ( 0.015)\tLoss 9.0625e-01 (8.3866e-01)\tAcc@1  43.75 ( 62.85)\n",
      "[xla:6]Train:  Epoch: [33][ 60/129]\tTime  0.202 ( 0.218)\tData  0.011 ( 0.015)\tLoss 7.8125e-01 (7.9143e-01)\tAcc@1  75.00 ( 64.49)\n",
      "[xla:1]Train:  Epoch: [33][ 60/129]\tTime  0.189 ( 0.214)\tData  0.009 ( 0.016)\tLoss 8.4375e-01 (8.2969e-01)\tAcc@1  50.00 ( 62.95)\n",
      "[xla:3]Train:  Epoch: [33][ 60/129]\tTime  0.202 ( 0.217)\tData  0.010 ( 0.015)\tLoss 5.6641e-01 (8.1218e-01)\tAcc@1  75.00 ( 62.95)\n",
      "[xla:0]Train:  Epoch: [33][ 90/129]\tTime  0.139 ( 0.211)\tData  0.010 ( 0.015)\tLoss 1.2266e+00 (8.2405e-01)\tAcc@1  37.50 ( 63.96)\n",
      "[xla:5]Train:  Epoch: [33][ 90/129]\tTime  0.181 ( 0.214)\tData  0.010 ( 0.015)\tLoss 7.3828e-01 (8.3609e-01)\tAcc@1  69.00 ( 62.48)\n",
      "[xla:2]Train:  Epoch: [33][ 90/129]\tTime  0.178 ( 0.218)\tData  0.010 ( 0.015)\tLoss 5.7812e-01 (8.3137e-01)\tAcc@1  75.00 ( 62.20)\n",
      "[xla:7]Train:  Epoch: [33][ 90/129]\tTime  0.233 ( 0.213)\tData  0.013 ( 0.015)\tLoss 5.5078e-01 (8.3499e-01)\tAcc@1  69.00 ( 62.19)\n",
      "[xla:4]Train:  Epoch: [33][ 90/129]\tTime  0.199 ( 0.213)\tData  0.012 ( 0.015)\tLoss 1.0547e+00 (8.1604e-01)\tAcc@1  43.75 ( 64.93)\n",
      "[xla:6]Train:  Epoch: [33][ 90/129]\tTime  0.251 ( 0.215)\tData  0.013 ( 0.015)\tLoss 6.0156e-01 (7.9383e-01)\tAcc@1  69.00 ( 64.27)\n",
      "[xla:1]Train:  Epoch: [33][ 90/129]\tTime  0.222 ( 0.213)\tData  0.011 ( 0.016)\tLoss 6.7578e-01 (8.1136e-01)\tAcc@1  81.00 ( 64.26)\n",
      "[xla:3]Train:  Epoch: [33][ 90/129]\tTime  0.210 ( 0.214)\tData  0.009 ( 0.015)\tLoss 6.0156e-01 (7.9451e-01)\tAcc@1  75.00 ( 63.63)\n",
      "[xla:6]Train:  Epoch: [33][120/129]\tTime  0.181 ( 0.213)\tData  0.010 ( 0.015)\tLoss 5.1953e-01 (7.7838e-01)\tAcc@1  81.00 ( 65.39)\n",
      "[xla:5]Train:  Epoch: [33][120/129]\tTime  0.173 ( 0.212)\tData  0.010 ( 0.015)\tLoss 3.5547e-01 (8.1634e-01)\tAcc@1  94.00 ( 63.68)\n",
      "[xla:4]Train:  Epoch: [33][120/129]\tTime  0.188 ( 0.211)\tData  0.011 ( 0.015)\tLoss 8.2031e-01 (7.9975e-01)\tAcc@1  75.00 ( 65.46)\n",
      "[xla:3]Train:  Epoch: [33][120/129]\tTime  0.217 ( 0.212)\tData  0.013 ( 0.015)\tLoss 4.6680e-01 (7.7182e-01)\tAcc@1  87.50 ( 64.86)\n",
      "[xla:0]Train:  Epoch: [33][120/129]\tTime  0.227 ( 0.211)\tData  0.012 ( 0.014)\tLoss 6.6406e-01 (7.9031e-01)\tAcc@1  69.00 ( 66.03)\n",
      "[xla:1]Train:  Epoch: [33][120/129]\tTime  0.193 ( 0.211)\tData  0.011 ( 0.015)\tLoss 8.2422e-01 (7.8564e-01)\tAcc@1  69.00 ( 65.44)\n",
      "[xla:2]Train:  Epoch: [33][120/129]\tTime  0.199 ( 0.216)\tData  0.010 ( 0.015)\tLoss 7.1484e-01 (7.9767e-01)\tAcc@1  62.50 ( 64.64)\n",
      "[xla:7]Train:  Epoch: [33][120/129]\tTime  0.213 ( 0.211)\tData  0.011 ( 0.016)\tLoss 7.6172e-01 (8.0834e-01)\tAcc@1  75.00 ( 63.41)\n",
      "Finished training epoch 33\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/6]\tTime  0.040 ( 0.040)\tLoss 5.7031e-01 (5.7031e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 6.2109e-01 (6.2109e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Validation: [0/6]\tTime  0.080 ( 0.080)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 1.3125e+00 (1.3125e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.111 ( 0.111)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.112 ( 0.112)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:6]Validation: [0/6]\tTime  0.087 ( 0.087)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.091 ( 0.091)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  37.50 ( 37.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [34][  0/129]\tTime  0.184 ( 0.184)\tData  0.033 ( 0.033)\tLoss 7.1094e-01 (7.1094e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [34][  0/129]\tTime  0.225 ( 0.225)\tData  0.085 ( 0.085)\tLoss 9.3750e-01 (9.3750e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Train:  Epoch: [34][  0/129]\tTime  0.203 ( 0.203)\tData  0.050 ( 0.050)\tLoss 8.7500e-01 (8.7500e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [34][  0/129]\tTime  0.169 ( 0.169)\tData  0.039 ( 0.039)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Train:  Epoch: [34][  0/129]\tTime  0.232 ( 0.232)\tData  0.078 ( 0.078)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Train:  Epoch: [34][  0/129]\tTime  0.200 ( 0.200)\tData  0.043 ( 0.043)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Train:  Epoch: [34][  0/129]\tTime  0.185 ( 0.185)\tData  0.039 ( 0.039)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Train:  Epoch: [34][  0/129]\tTime  0.179 ( 0.179)\tData  0.028 ( 0.028)\tLoss 9.3750e-01 (9.3750e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Train:  Epoch: [34][ 30/129]\tTime  0.174 ( 0.218)\tData  0.010 ( 0.014)\tLoss 1.0234e+00 (8.7374e-01)\tAcc@1  56.25 ( 60.51)\n",
      "[xla:2]Train:  Epoch: [34][ 30/129]\tTime  0.180 ( 0.209)\tData  0.011 ( 0.017)\tLoss 8.5938e-01 (8.6593e-01)\tAcc@1  62.50 ( 60.72)\n",
      "[xla:1]Train:  Epoch: [34][ 30/129]\tTime  0.202 ( 0.211)\tData  0.011 ( 0.016)\tLoss 8.1641e-01 (8.0916e-01)\tAcc@1  62.50 ( 66.40)\n",
      "[xla:6]Train:  Epoch: [34][ 30/129]\tTime  0.232 ( 0.201)\tData  0.011 ( 0.015)\tLoss 9.1797e-01 (7.9461e-01)\tAcc@1  50.00 ( 64.28)\n",
      "[xla:3]Train:  Epoch: [34][ 30/129]\tTime  0.200 ( 0.204)\tData  0.011 ( 0.014)\tLoss 9.2188e-01 (8.1288e-01)\tAcc@1  50.00 ( 60.93)\n",
      "[xla:7]Train:  Epoch: [34][ 30/129]\tTime  0.205 ( 0.201)\tData  0.012 ( 0.013)\tLoss 8.6719e-01 (8.9176e-01)\tAcc@1  69.00 ( 60.10)\n",
      "[xla:4]Train:  Epoch: [34][ 30/129]\tTime  0.205 ( 0.210)\tData  0.013 ( 0.014)\tLoss 6.7578e-01 (8.3039e-01)\tAcc@1  81.00 ( 60.31)\n",
      "[xla:5]Train:  Epoch: [34][ 30/129]\tTime  0.182 ( 0.216)\tData  0.009 ( 0.018)\tLoss 6.0156e-01 (8.4526e-01)\tAcc@1  69.00 ( 62.94)\n",
      "[xla:0]Train:  Epoch: [34][ 60/129]\tTime  0.187 ( 0.211)\tData  0.012 ( 0.014)\tLoss 7.0703e-01 (8.3258e-01)\tAcc@1  69.00 ( 63.36)\n",
      "[xla:5]Train:  Epoch: [34][ 60/129]\tTime  0.171 ( 0.210)\tData  0.008 ( 0.017)\tLoss 9.8828e-01 (8.4087e-01)\tAcc@1  56.25 ( 61.73)\n",
      "[xla:1]Train:  Epoch: [34][ 60/129]\tTime  0.199 ( 0.208)\tData  0.012 ( 0.015)\tLoss 8.6719e-01 (8.2521e-01)\tAcc@1  62.50 ( 63.47)\n",
      "[xla:2]Train:  Epoch: [34][ 60/129]\tTime  0.178 ( 0.207)\tData  0.008 ( 0.016)\tLoss 9.8828e-01 (8.3097e-01)\tAcc@1  56.25 ( 61.91)\n",
      "[xla:7]Train:  Epoch: [34][ 60/129]\tTime  0.215 ( 0.203)\tData  0.012 ( 0.014)\tLoss 5.6641e-01 (8.4375e-01)\tAcc@1  81.00 ( 63.02)\n",
      "[xla:3]Train:  Epoch: [34][ 60/129]\tTime  0.195 ( 0.204)\tData  0.009 ( 0.014)\tLoss 5.3125e-01 (7.9527e-01)\tAcc@1  87.50 ( 62.97)\n",
      "[xla:6]Train:  Epoch: [34][ 60/129]\tTime  0.201 ( 0.203)\tData  0.012 ( 0.016)\tLoss 9.9219e-01 (8.1487e-01)\tAcc@1  31.25 ( 64.03)\n",
      "[xla:4]Train:  Epoch: [34][ 60/129]\tTime  0.206 ( 0.208)\tData  0.013 ( 0.014)\tLoss 9.7266e-01 (8.0622e-01)\tAcc@1  50.00 ( 62.64)\n",
      "[xla:2]Train:  Epoch: [34][ 90/129]\tTime  0.170 ( 0.207)\tData  0.010 ( 0.015)\tLoss 5.5859e-01 (8.3124e-01)\tAcc@1  75.00 ( 62.20)\n",
      "[xla:1]Train:  Epoch: [34][ 90/129]\tTime  0.205 ( 0.207)\tData  0.014 ( 0.015)\tLoss 6.3281e-01 (8.1308e-01)\tAcc@1  69.00 ( 63.64)\n",
      "[xla:0]Train:  Epoch: [34][ 90/129]\tTime  0.196 ( 0.210)\tData  0.010 ( 0.014)\tLoss 1.1797e+00 (8.0930e-01)\tAcc@1  43.75 ( 64.32)\n",
      "[xla:6]Train:  Epoch: [34][ 90/129]\tTime  0.201 ( 0.204)\tData  0.012 ( 0.016)\tLoss 5.4297e-01 (8.0950e-01)\tAcc@1  75.00 ( 63.75)\n",
      "[xla:3]Train:  Epoch: [34][ 90/129]\tTime  0.187 ( 0.205)\tData  0.010 ( 0.014)\tLoss 4.8633e-01 (7.8318e-01)\tAcc@1  75.00 ( 63.18)\n",
      "[xla:4]Train:  Epoch: [34][ 90/129]\tTime  0.192 ( 0.207)\tData  0.011 ( 0.015)\tLoss 9.0234e-01 (8.0666e-01)\tAcc@1  50.00 ( 63.35)\n",
      "[xla:7]Train:  Epoch: [34][ 90/129]\tTime  0.181 ( 0.204)\tData  0.011 ( 0.015)\tLoss 6.1328e-01 (8.3486e-01)\tAcc@1  75.00 ( 63.13)\n",
      "[xla:5]Train:  Epoch: [34][ 90/129]\tTime  0.202 ( 0.209)\tData  0.013 ( 0.016)\tLoss 8.2031e-01 (8.3611e-01)\tAcc@1  81.00 ( 62.34)\n",
      "[xla:4]Train:  Epoch: [34][120/129]\tTime  0.179 ( 0.206)\tData  0.011 ( 0.014)\tLoss 8.1641e-01 (7.9108e-01)\tAcc@1  75.00 ( 64.75)\n",
      "[xla:7]Train:  Epoch: [34][120/129]\tTime  0.180 ( 0.203)\tData  0.022 ( 0.015)\tLoss 8.6328e-01 (8.0758e-01)\tAcc@1  56.25 ( 64.48)\n",
      "[xla:1]Train:  Epoch: [34][120/129]\tTime  0.181 ( 0.206)\tData  0.027 ( 0.015)\tLoss 8.6328e-01 (7.9247e-01)\tAcc@1  56.25 ( 65.02)\n",
      "[xla:6]Train:  Epoch: [34][120/129]\tTime  0.179 ( 0.204)\tData  0.016 ( 0.016)\tLoss 5.3906e-01 (7.8357e-01)\tAcc@1  75.00 ( 65.11)\n",
      "[xla:5]Train:  Epoch: [34][120/129]\tTime  0.184 ( 0.207)\tData  0.013 ( 0.015)\tLoss 4.3359e-01 (8.1963e-01)\tAcc@1  87.50 ( 63.62)\n",
      "[xla:2]Train:  Epoch: [34][120/129]\tTime  0.183 ( 0.206)\tData  0.011 ( 0.015)\tLoss 8.8281e-01 (7.9671e-01)\tAcc@1  56.25 ( 64.23)\n",
      "[xla:0]Train:  Epoch: [34][120/129]\tTime  0.209 ( 0.208)\tData  0.011 ( 0.014)\tLoss 7.3828e-01 (7.7391e-01)\tAcc@1  75.00 ( 66.10)\n",
      "[xla:3]Train:  Epoch: [34][120/129]\tTime  0.217 ( 0.204)\tData  0.016 ( 0.014)\tLoss 4.7656e-01 (7.6533e-01)\tAcc@1  87.50 ( 64.93)\n",
      "[xla:1]Validation: [0/6]\tTime  0.076 ( 0.076)\tLoss 1.2891e+00 (1.2891e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Finished training epoch 34\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/6]\tTime  0.077 ( 0.077)\tLoss 5.8203e-01 (5.8203e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.101 ( 0.101)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.098 ( 0.098)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/6]\tTime  0.098 ( 0.098)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:2]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 8.2812e-01 (8.2812e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Validation: [0/6]\tTime  0.110 ( 0.110)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Validation: [0/6]\tTime  0.142 ( 0.142)\tLoss 6.9922e-01 (6.9922e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [35][  0/129]\tTime  0.196 ( 0.196)\tData  0.048 ( 0.048)\tLoss 8.1641e-01 (8.1641e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:6]Train:  Epoch: [35][  0/129]\tTime  0.264 ( 0.264)\tData  0.062 ( 0.062)\tLoss 1.2109e+00 (1.2109e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Train:  Epoch: [35][  0/129]\tTime  0.336 ( 0.336)\tData  0.118 ( 0.118)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [35][  0/129]\tTime  0.307 ( 0.307)\tData  0.137 ( 0.137)\tLoss 8.5156e-01 (8.5156e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [35][  0/129]\tTime  0.312 ( 0.312)\tData  0.119 ( 0.119)\tLoss 6.3672e-01 (6.3672e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [35][  0/129]\tTime  0.267 ( 0.267)\tData  0.109 ( 0.109)\tLoss 6.7188e-01 (6.7188e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [35][  0/129]\tTime  0.199 ( 0.199)\tData  0.049 ( 0.049)\tLoss 8.3594e-01 (8.3594e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Train:  Epoch: [35][  0/129]\tTime  0.190 ( 0.190)\tData  0.049 ( 0.049)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [35][ 30/129]\tTime  0.216 ( 0.212)\tData  0.012 ( 0.020)\tLoss 8.6328e-01 (9.0600e-01)\tAcc@1  62.50 ( 57.69)\n",
      "[xla:6]Train:  Epoch: [35][ 30/129]\tTime  0.211 ( 0.217)\tData  0.010 ( 0.015)\tLoss 7.6562e-01 (7.7646e-01)\tAcc@1  56.25 ( 65.15)\n",
      "[xla:4]Train:  Epoch: [35][ 30/129]\tTime  0.217 ( 0.205)\tData  0.011 ( 0.016)\tLoss 6.9922e-01 (8.2245e-01)\tAcc@1  75.00 ( 61.31)\n",
      "[xla:2]Train:  Epoch: [35][ 30/129]\tTime  0.245 ( 0.206)\tData  0.011 ( 0.017)\tLoss 7.8125e-01 (8.6643e-01)\tAcc@1  69.00 ( 59.32)\n",
      "[xla:3]Train:  Epoch: [35][ 30/129]\tTime  0.241 ( 0.215)\tData  0.011 ( 0.018)\tLoss 8.4375e-01 (8.3556e-01)\tAcc@1  56.25 ( 61.13)\n",
      "[xla:1]Train:  Epoch: [35][ 30/129]\tTime  0.196 ( 0.232)\tData  0.008 ( 0.017)\tLoss 8.3203e-01 (8.1930e-01)\tAcc@1  62.50 ( 63.35)\n",
      "[xla:7]Train:  Epoch: [35][ 30/129]\tTime  0.185 ( 0.218)\tData  0.008 ( 0.019)\tLoss 9.8047e-01 (8.6668e-01)\tAcc@1  56.25 ( 60.31)\n",
      "[xla:5]Train:  Epoch: [35][ 30/129]\tTime  0.232 ( 0.216)\tData  0.011 ( 0.019)\tLoss 7.1484e-01 (8.4350e-01)\tAcc@1  81.00 ( 62.52)\n",
      "[xla:1]Train:  Epoch: [35][ 60/129]\tTime  0.177 ( 0.222)\tData  0.012 ( 0.016)\tLoss 9.6875e-01 (8.2419e-01)\tAcc@1  50.00 ( 62.64)\n",
      "[xla:7]Train:  Epoch: [35][ 60/129]\tTime  0.181 ( 0.216)\tData  0.013 ( 0.016)\tLoss 5.2344e-01 (8.3424e-01)\tAcc@1  81.00 ( 61.10)\n",
      "[xla:0]Train:  Epoch: [35][ 60/129]\tTime  0.170 ( 0.213)\tData  0.008 ( 0.017)\tLoss 8.5547e-01 (8.3978e-01)\tAcc@1  62.50 ( 60.69)\n",
      "[xla:6]Train:  Epoch: [35][ 60/129]\tTime  0.204 ( 0.215)\tData  0.018 ( 0.015)\tLoss 7.3828e-01 (7.9944e-01)\tAcc@1  75.00 ( 64.18)\n",
      "[xla:5]Train:  Epoch: [35][ 60/129]\tTime  0.196 ( 0.215)\tData  0.011 ( 0.018)\tLoss 8.8672e-01 (8.4132e-01)\tAcc@1  50.00 ( 61.81)\n",
      "[xla:4]Train:  Epoch: [35][ 60/129]\tTime  0.223 ( 0.210)\tData  0.011 ( 0.015)\tLoss 8.5938e-01 (7.9617e-01)\tAcc@1  62.50 ( 64.48)\n",
      "[xla:3]Train:  Epoch: [35][ 60/129]\tTime  0.194 ( 0.214)\tData  0.010 ( 0.016)\tLoss 6.2500e-01 (8.1794e-01)\tAcc@1  81.00 ( 62.74)\n",
      "[xla:2]Train:  Epoch: [35][ 60/129]\tTime  0.218 ( 0.210)\tData  0.010 ( 0.016)\tLoss 8.7891e-01 (8.3456e-01)\tAcc@1  62.50 ( 61.63)\n",
      "[xla:3]Train:  Epoch: [35][ 90/129]\tTime  0.180 ( 0.213)\tData  0.012 ( 0.015)\tLoss 4.8828e-01 (7.9688e-01)\tAcc@1  81.00 ( 63.84)\n",
      "[xla:7]Train:  Epoch: [35][ 90/129]\tTime  0.163 ( 0.215)\tData  0.012 ( 0.016)\tLoss 5.5469e-01 (8.3690e-01)\tAcc@1  75.00 ( 61.37)\n",
      "[xla:5]Train:  Epoch: [35][ 90/129]\tTime  0.185 ( 0.214)\tData  0.014 ( 0.017)\tLoss 8.3594e-01 (8.3328e-01)\tAcc@1  69.00 ( 62.88)\n",
      "[xla:1]Train:  Epoch: [35][ 90/129]\tTime  0.200 ( 0.219)\tData  0.009 ( 0.015)\tLoss 5.0000e-01 (8.1050e-01)\tAcc@1  75.00 ( 62.73)\n",
      "[xla:0]Train:  Epoch: [35][ 90/129]\tTime  0.184 ( 0.213)\tData  0.019 ( 0.016)\tLoss 1.1562e+00 (8.1018e-01)\tAcc@1  56.25 ( 63.01)\n",
      "[xla:2]Train:  Epoch: [35][ 90/129]\tTime  0.187 ( 0.211)\tData  0.012 ( 0.015)\tLoss 5.7031e-01 (8.3956e-01)\tAcc@1  69.00 ( 61.25)\n",
      "[xla:4]Train:  Epoch: [35][ 90/129]\tTime  0.192 ( 0.211)\tData  0.012 ( 0.015)\tLoss 9.4922e-01 (8.0741e-01)\tAcc@1  31.25 ( 63.98)\n",
      "[xla:6]Train:  Epoch: [35][ 90/129]\tTime  0.206 ( 0.215)\tData  0.011 ( 0.015)\tLoss 5.1953e-01 (7.9344e-01)\tAcc@1  75.00 ( 64.31)\n",
      "[xla:0]Train:  Epoch: [35][120/129]\tTime  0.209 ( 0.211)\tData  0.011 ( 0.015)\tLoss 7.4609e-01 (7.8514e-01)\tAcc@1  87.50 ( 64.81)\n",
      "[xla:2]Train:  Epoch: [35][120/129]\tTime  0.208 ( 0.209)\tData  0.012 ( 0.015)\tLoss 6.7188e-01 (8.0296e-01)\tAcc@1  75.00 ( 63.32)\n",
      "[xla:3]Train:  Epoch: [35][120/129]\tTime  0.196 ( 0.212)\tData  0.010 ( 0.015)\tLoss 4.4531e-01 (7.7864e-01)\tAcc@1  87.50 ( 64.92)\n",
      "[xla:6]Train:  Epoch: [35][120/129]\tTime  0.205 ( 0.212)\tData  0.009 ( 0.014)\tLoss 5.6250e-01 (7.7647e-01)\tAcc@1  75.00 ( 64.74)\n",
      "[xla:1]Train:  Epoch: [35][120/129]\tTime  0.194 ( 0.216)\tData  0.009 ( 0.015)\tLoss 8.8281e-01 (7.8640e-01)\tAcc@1  62.50 ( 64.29)\n",
      "[xla:7]Train:  Epoch: [35][120/129]\tTime  0.181 ( 0.212)\tData  0.009 ( 0.016)\tLoss 8.1250e-01 (8.1306e-01)\tAcc@1  62.50 ( 63.20)\n",
      "[xla:5]Train:  Epoch: [35][120/129]\tTime  0.238 ( 0.212)\tData  0.013 ( 0.016)\tLoss 3.8867e-01 (8.2698e-01)\tAcc@1  87.50 ( 63.31)\n",
      "[xla:4]Train:  Epoch: [35][120/129]\tTime  0.190 ( 0.209)\tData  0.009 ( 0.015)\tLoss 9.8828e-01 (7.9931e-01)\tAcc@1  62.50 ( 64.18)\n",
      "[xla:2]Validation: [0/6]\tTime  0.080 ( 0.080)\tLoss 8.3594e-01 (8.3594e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Validation: [0/6]\tTime  0.086 ( 0.086)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "Finished training epoch 35\n",
      "[xla:7]Validation: [0/6]\tTime  0.085 ( 0.085)\tLoss 9.9219e-01 (9.9219e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.102 ( 0.102)\tLoss 6.9531e-01 (6.9531e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:5]Validation: [0/6]\tTime  0.134 ( 0.134)\tLoss 5.8594e-01 (5.8594e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Validation: [0/6]\tTime  0.108 ( 0.108)\tLoss 1.2500e+00 (1.2500e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.113 ( 0.113)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.100 ( 0.100)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [36][  0/129]\tTime  0.196 ( 0.196)\tData  0.035 ( 0.035)\tLoss 8.0078e-01 (8.0078e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [36][  0/129]\tTime  0.160 ( 0.160)\tData  0.037 ( 0.037)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:6]Train:  Epoch: [36][  0/129]\tTime  0.197 ( 0.197)\tData  0.039 ( 0.039)\tLoss 9.1016e-01 (9.1016e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:7]Train:  Epoch: [36][  0/129]\tTime  0.165 ( 0.165)\tData  0.037 ( 0.037)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [36][  0/129]\tTime  0.173 ( 0.173)\tData  0.034 ( 0.034)\tLoss 9.8438e-01 (9.8438e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [36][  0/129]\tTime  0.251 ( 0.251)\tData  0.100 ( 0.100)\tLoss 7.1484e-01 (7.1484e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:0]Train:  Epoch: [36][  0/129]\tTime  0.218 ( 0.218)\tData  0.076 ( 0.076)\tLoss 8.0469e-01 (8.0469e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [36][  0/129]\tTime  0.235 ( 0.235)\tData  0.061 ( 0.061)\tLoss 7.6953e-01 (7.6953e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Train:  Epoch: [36][ 30/129]\tTime  0.186 ( 0.212)\tData  0.011 ( 0.016)\tLoss 1.0000e+00 (8.5950e-01)\tAcc@1  56.25 ( 62.37)\n",
      "[xla:1]Train:  Epoch: [36][ 30/129]\tTime  0.181 ( 0.212)\tData  0.014 ( 0.016)\tLoss 8.9453e-01 (7.9958e-01)\tAcc@1  56.25 ( 65.94)\n",
      "[xla:3]Train:  Epoch: [36][ 30/129]\tTime  0.175 ( 0.220)\tData  0.012 ( 0.016)\tLoss 6.7578e-01 (8.2699e-01)\tAcc@1  81.00 ( 60.90)\n",
      "[xla:6]Train:  Epoch: [36][ 30/129]\tTime  0.204 ( 0.215)\tData  0.012 ( 0.017)\tLoss 9.4531e-01 (7.9435e-01)\tAcc@1  50.00 ( 63.93)\n",
      "[xla:0]Train:  Epoch: [36][ 30/129]\tTime  0.192 ( 0.209)\tData  0.012 ( 0.017)\tLoss 9.0234e-01 (8.8873e-01)\tAcc@1  62.50 ( 59.73)\n",
      "[xla:4]Train:  Epoch: [36][ 30/129]\tTime  0.206 ( 0.209)\tData  0.013 ( 0.017)\tLoss 5.0000e-01 (8.1729e-01)\tAcc@1  87.50 ( 65.77)\n",
      "[xla:5]Train:  Epoch: [36][ 30/129]\tTime  0.198 ( 0.211)\tData  0.012 ( 0.015)\tLoss 6.7188e-01 (8.4904e-01)\tAcc@1  75.00 ( 63.32)\n",
      "[xla:2]Train:  Epoch: [36][ 30/129]\tTime  0.262 ( 0.226)\tData  0.019 ( 0.017)\tLoss 7.9297e-01 (8.3758e-01)\tAcc@1  69.00 ( 61.11)\n",
      "[xla:3]Train:  Epoch: [36][ 60/129]\tTime  0.183 ( 0.213)\tData  0.010 ( 0.016)\tLoss 5.4297e-01 (8.1647e-01)\tAcc@1  81.00 ( 62.11)\n",
      "[xla:1]Train:  Epoch: [36][ 60/129]\tTime  0.171 ( 0.209)\tData  0.012 ( 0.015)\tLoss 1.0000e+00 (8.2105e-01)\tAcc@1  43.75 ( 63.04)\n",
      "[xla:6]Train:  Epoch: [36][ 60/129]\tTime  0.177 ( 0.210)\tData  0.011 ( 0.016)\tLoss 8.3203e-01 (8.0446e-01)\tAcc@1  56.25 ( 64.24)\n",
      "[xla:4]Train:  Epoch: [36][ 60/129]\tTime  0.235 ( 0.207)\tData  0.021 ( 0.016)\tLoss 8.9844e-01 (8.0546e-01)\tAcc@1  56.25 ( 65.62)\n",
      "[xla:5]Train:  Epoch: [36][ 60/129]\tTime  0.181 ( 0.209)\tData  0.010 ( 0.015)\tLoss 1.0156e+00 (8.3395e-01)\tAcc@1  43.75 ( 64.27)\n",
      "[xla:7]Train:  Epoch: [36][ 60/129]\tTime  0.183 ( 0.210)\tData  0.011 ( 0.016)\tLoss 4.5898e-01 (8.3373e-01)\tAcc@1  87.50 ( 62.75)\n",
      "[xla:2]Train:  Epoch: [36][ 60/129]\tTime  0.239 ( 0.216)\tData  0.012 ( 0.016)\tLoss 9.1797e-01 (8.3107e-01)\tAcc@1  56.25 ( 60.98)\n",
      "[xla:0]Train:  Epoch: [36][ 60/129]\tTime  0.222 ( 0.208)\tData  0.012 ( 0.017)\tLoss 6.9141e-01 (8.3242e-01)\tAcc@1  75.00 ( 62.22)\n",
      "[xla:2]Train:  Epoch: [36][ 90/129]\tTime  0.193 ( 0.212)\tData  0.012 ( 0.016)\tLoss 6.4062e-01 (8.3701e-01)\tAcc@1  75.00 ( 60.81)\n",
      "[xla:6]Train:  Epoch: [36][ 90/129]\tTime  0.178 ( 0.209)\tData  0.010 ( 0.016)\tLoss 5.5078e-01 (7.9857e-01)\tAcc@1  81.00 ( 64.23)\n",
      "[xla:0]Train:  Epoch: [36][ 90/129]\tTime  0.199 ( 0.207)\tData  0.012 ( 0.016)\tLoss 1.1328e+00 (8.0868e-01)\tAcc@1  37.50 ( 63.76)\n",
      "[xla:4]Train:  Epoch: [36][ 90/129]\tTime  0.195 ( 0.207)\tData  0.010 ( 0.016)\tLoss 6.7188e-01 (8.0310e-01)\tAcc@1  69.00 ( 65.44)\n",
      "[xla:3]Train:  Epoch: [36][ 90/129]\tTime  0.179 ( 0.211)\tData  0.008 ( 0.015)\tLoss 6.1719e-01 (7.9606e-01)\tAcc@1  62.50 ( 63.07)\n",
      "[xla:1]Train:  Epoch: [36][ 90/129]\tTime  0.204 ( 0.209)\tData  0.012 ( 0.014)\tLoss 5.1172e-01 (7.9426e-01)\tAcc@1  75.00 ( 65.26)\n",
      "[xla:5]Train:  Epoch: [36][ 90/129]\tTime  0.184 ( 0.208)\tData  0.010 ( 0.015)\tLoss 7.3047e-01 (8.2679e-01)\tAcc@1  69.00 ( 63.49)\n",
      "[xla:7]Train:  Epoch: [36][ 90/129]\tTime  0.200 ( 0.209)\tData  0.011 ( 0.015)\tLoss 6.6406e-01 (8.2737e-01)\tAcc@1  62.50 ( 62.27)\n",
      "[xla:3]Train:  Epoch: [36][120/129]\tTime  0.173 ( 0.210)\tData  0.011 ( 0.015)\tLoss 5.8984e-01 (7.7747e-01)\tAcc@1  81.00 ( 64.85)\n",
      "[xla:7]Train:  Epoch: [36][120/129]\tTime  0.189 ( 0.208)\tData  0.011 ( 0.015)\tLoss 8.4766e-01 (8.0516e-01)\tAcc@1  69.00 ( 63.79)\n",
      "[xla:4]Train:  Epoch: [36][120/129]\tTime  0.212 ( 0.207)\tData  0.011 ( 0.015)\tLoss 8.3203e-01 (7.8541e-01)\tAcc@1  62.50 ( 66.48)\n",
      "[xla:1]Train:  Epoch: [36][120/129]\tTime  0.196 ( 0.208)\tData  0.013 ( 0.014)\tLoss 7.2656e-01 (7.7258e-01)\tAcc@1  62.50 ( 66.54)\n",
      "[xla:6]Train:  Epoch: [36][120/129]\tTime  0.183 ( 0.208)\tData  0.011 ( 0.015)\tLoss 6.7188e-01 (7.8422e-01)\tAcc@1  75.00 ( 65.05)\n",
      "[xla:0]Train:  Epoch: [36][120/129]\tTime  0.225 ( 0.207)\tData  0.013 ( 0.016)\tLoss 8.5156e-01 (7.8131e-01)\tAcc@1  69.00 ( 65.89)\n",
      "[xla:5]Train:  Epoch: [36][120/129]\tTime  0.203 ( 0.207)\tData  0.012 ( 0.015)\tLoss 4.8242e-01 (8.1871e-01)\tAcc@1  81.00 ( 63.71)\n",
      "[xla:2]Train:  Epoch: [36][120/129]\tTime  0.195 ( 0.211)\tData  0.009 ( 0.015)\tLoss 8.2422e-01 (8.0143e-01)\tAcc@1  75.00 ( 62.94)\n",
      "[xla:1]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Finished training epoch 36\n",
      "[xla:3]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/6]\tTime  0.093 ( 0.093)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Validation: [0/6]\tTime  0.128 ( 0.128)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:5]Validation: [0/6]\tTime  0.091 ( 0.091)\tLoss 5.7812e-01 (5.7812e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Validation: [0/6]\tTime  0.098 ( 0.098)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Validation: [0/6]\tTime  0.075 ( 0.075)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/6]\tTime  0.103 ( 0.103)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Train:  Epoch: [37][  0/129]\tTime  0.263 ( 0.263)\tData  0.064 ( 0.064)\tLoss 7.2656e-01 (7.2656e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Train:  Epoch: [37][  0/129]\tTime  0.192 ( 0.192)\tData  0.044 ( 0.044)\tLoss 6.2109e-01 (6.2109e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [37][  0/129]\tTime  0.185 ( 0.185)\tData  0.067 ( 0.067)\tLoss 7.3828e-01 (7.3828e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:6]Train:  Epoch: [37][  0/129]\tTime  0.188 ( 0.188)\tData  0.058 ( 0.058)\tLoss 1.0625e+00 (1.0625e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [37][  0/129]\tTime  0.146 ( 0.146)\tData  0.035 ( 0.035)\tLoss 1.1484e+00 (1.1484e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:7]Train:  Epoch: [37][  0/129]\tTime  0.227 ( 0.227)\tData  0.056 ( 0.056)\tLoss 7.7734e-01 (7.7734e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:4]Train:  Epoch: [37][  0/129]\tTime  0.183 ( 0.183)\tData  0.039 ( 0.039)\tLoss 8.4375e-01 (8.4375e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:0]Train:  Epoch: [37][  0/129]\tTime  0.180 ( 0.180)\tData  0.051 ( 0.051)\tLoss 7.1875e-01 (7.1875e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [37][ 30/129]\tTime  0.176 ( 0.210)\tData  0.011 ( 0.016)\tLoss 8.1250e-01 (8.5673e-01)\tAcc@1  62.50 ( 61.76)\n",
      "[xla:2]Train:  Epoch: [37][ 30/129]\tTime  0.179 ( 0.217)\tData  0.013 ( 0.017)\tLoss 8.1250e-01 (8.5295e-01)\tAcc@1  62.50 ( 61.71)\n",
      "[xla:7]Train:  Epoch: [37][ 30/129]\tTime  0.187 ( 0.207)\tData  0.010 ( 0.016)\tLoss 9.5703e-01 (8.3808e-01)\tAcc@1  56.25 ( 62.71)\n",
      "[xla:0]Train:  Epoch: [37][ 30/129]\tTime  0.189 ( 0.204)\tData  0.017 ( 0.015)\tLoss 9.8438e-01 (9.0071e-01)\tAcc@1  43.75 ( 58.06)\n",
      "[xla:4]Train:  Epoch: [37][ 30/129]\tTime  0.202 ( 0.205)\tData  0.011 ( 0.014)\tLoss 6.3672e-01 (7.9877e-01)\tAcc@1  81.00 ( 63.94)\n",
      "[xla:3]Train:  Epoch: [37][ 30/129]\tTime  0.189 ( 0.218)\tData  0.024 ( 0.016)\tLoss 8.5156e-01 (8.3241e-01)\tAcc@1  62.50 ( 60.35)\n",
      "[xla:6]Train:  Epoch: [37][ 30/129]\tTime  0.213 ( 0.212)\tData  0.010 ( 0.015)\tLoss 1.0078e+00 (7.7974e-01)\tAcc@1  50.00 ( 65.13)\n",
      "[xla:1]Train:  Epoch: [37][ 30/129]\tTime  0.200 ( 0.225)\tData  0.020 ( 0.016)\tLoss 8.4766e-01 (8.1420e-01)\tAcc@1  56.25 ( 66.74)\n",
      "[xla:2]Train:  Epoch: [37][ 60/129]\tTime  0.186 ( 0.217)\tData  0.013 ( 0.016)\tLoss 9.3750e-01 (8.2742e-01)\tAcc@1  56.25 ( 62.73)\n",
      "[xla:6]Train:  Epoch: [37][ 60/129]\tTime  0.183 ( 0.214)\tData  0.012 ( 0.016)\tLoss 8.5547e-01 (8.0078e-01)\tAcc@1  56.25 ( 63.87)\n",
      "[xla:4]Train:  Epoch: [37][ 60/129]\tTime  0.209 ( 0.211)\tData  0.016 ( 0.014)\tLoss 8.1641e-01 (7.9931e-01)\tAcc@1  62.50 ( 63.74)\n",
      "[xla:1]Train:  Epoch: [37][ 60/129]\tTime  0.199 ( 0.221)\tData  0.012 ( 0.017)\tLoss 8.2812e-01 (8.2553e-01)\tAcc@1  75.00 ( 63.84)\n",
      "[xla:7]Train:  Epoch: [37][ 60/129]\tTime  0.216 ( 0.212)\tData  0.013 ( 0.016)\tLoss 4.4336e-01 (8.2604e-01)\tAcc@1 100.00 ( 63.64)\n",
      "[xla:0]Train:  Epoch: [37][ 60/129]\tTime  0.226 ( 0.211)\tData  0.012 ( 0.016)\tLoss 6.8359e-01 (8.4490e-01)\tAcc@1  75.00 ( 60.35)\n",
      "[xla:3]Train:  Epoch: [37][ 60/129]\tTime  0.209 ( 0.218)\tData  0.012 ( 0.016)\tLoss 7.1094e-01 (8.1663e-01)\tAcc@1  75.00 ( 62.56)\n",
      "[xla:5]Train:  Epoch: [37][ 60/129]\tTime  0.184 ( 0.214)\tData  0.008 ( 0.016)\tLoss 9.4141e-01 (8.4119e-01)\tAcc@1  50.00 ( 61.32)\n",
      "[xla:5]Train:  Epoch: [37][ 90/129]\tTime  0.182 ( 0.214)\tData  0.011 ( 0.016)\tLoss 6.4062e-01 (8.3246e-01)\tAcc@1  75.00 ( 61.80)\n",
      "[xla:6]Train:  Epoch: [37][ 90/129]\tTime  0.203 ( 0.215)\tData  0.010 ( 0.016)\tLoss 6.4844e-01 (7.9979e-01)\tAcc@1  75.00 ( 63.55)\n",
      "[xla:0]Train:  Epoch: [37][ 90/129]\tTime  0.209 ( 0.213)\tData  0.011 ( 0.015)\tLoss 1.2812e+00 (8.1604e-01)\tAcc@1  31.25 ( 62.25)\n",
      "[xla:7]Train:  Epoch: [37][ 90/129]\tTime  0.213 ( 0.214)\tData  0.013 ( 0.015)\tLoss 6.4844e-01 (8.2360e-01)\tAcc@1  69.00 ( 63.00)\n",
      "[xla:1]Train:  Epoch: [37][ 90/129]\tTime  0.212 ( 0.220)\tData  0.010 ( 0.016)\tLoss 5.5859e-01 (8.0686e-01)\tAcc@1  87.50 ( 65.19)\n",
      "[xla:3]Train:  Epoch: [37][ 90/129]\tTime  0.232 ( 0.218)\tData  0.011 ( 0.015)\tLoss 5.1953e-01 (7.9827e-01)\tAcc@1  75.00 ( 63.58)\n",
      "[xla:4]Train:  Epoch: [37][ 90/129]\tTime  0.220 ( 0.213)\tData  0.013 ( 0.015)\tLoss 7.2266e-01 (7.9681e-01)\tAcc@1  69.00 ( 64.72)\n",
      "[xla:2]Train:  Epoch: [37][ 90/129]\tTime  0.227 ( 0.218)\tData  0.009 ( 0.016)\tLoss 4.8047e-01 (8.2916e-01)\tAcc@1  87.50 ( 62.40)\n",
      "[xla:6]Train:  Epoch: [37][120/129]\tTime  0.148 ( 0.213)\tData  0.010 ( 0.016)\tLoss 6.5234e-01 (7.7546e-01)\tAcc@1  69.00 ( 64.95)\n",
      "[xla:2]Train:  Epoch: [37][120/129]\tTime  0.178 ( 0.215)\tData  0.011 ( 0.015)\tLoss 8.4766e-01 (7.9607e-01)\tAcc@1  56.25 ( 64.25)\n",
      "[xla:3]Train:  Epoch: [37][120/129]\tTime  0.177 ( 0.215)\tData  0.012 ( 0.015)\tLoss 5.1172e-01 (7.7621e-01)\tAcc@1  81.00 ( 64.72)\n",
      "[xla:0]Train:  Epoch: [37][120/129]\tTime  0.176 ( 0.212)\tData  0.011 ( 0.015)\tLoss 7.0312e-01 (7.8161e-01)\tAcc@1  81.00 ( 64.59)\n",
      "[xla:1]Train:  Epoch: [37][120/129]\tTime  0.183 ( 0.217)\tData  0.009 ( 0.015)\tLoss 1.0625e+00 (7.9269e-01)\tAcc@1  56.25 ( 65.56)\n",
      "[xla:7]Train:  Epoch: [37][120/129]\tTime  0.207 ( 0.212)\tData  0.011 ( 0.015)\tLoss 8.2422e-01 (8.0180e-01)\tAcc@1  62.50 ( 64.38)\n",
      "[xla:5]Train:  Epoch: [37][120/129]\tTime  0.221 ( 0.213)\tData  0.010 ( 0.015)\tLoss 4.6875e-01 (8.1457e-01)\tAcc@1  81.00 ( 62.85)\n",
      "[xla:4]Train:  Epoch: [37][120/129]\tTime  0.212 ( 0.212)\tData  0.013 ( 0.016)\tLoss 9.2578e-01 (7.8438e-01)\tAcc@1  69.00 ( 65.77)\n",
      "Finished training epoch 37\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:5]Validation: [0/6]\tTime  0.076 ( 0.076)\tLoss 5.7812e-01 (5.7812e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:4]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 1.2500e+00 (1.2500e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Validation: [0/6]\tTime  0.105 ( 0.105)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:1]Validation: [0/6]\tTime  0.095 ( 0.095)\tLoss 1.2344e+00 (1.2344e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.093 ( 0.093)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.133 ( 0.133)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/6]\tTime  0.119 ( 0.119)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Validation: [0/6]\tTime  0.131 ( 0.131)\tLoss 8.4375e-01 (8.4375e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Train:  Epoch: [38][  0/129]\tTime  0.175 ( 0.175)\tData  0.031 ( 0.031)\tLoss 9.3750e-01 (9.3750e-01)\tAcc@1  62.50 ( 62.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:7]Train:  Epoch: [38][  0/129]\tTime  0.160 ( 0.160)\tData  0.053 ( 0.053)\tLoss 9.3359e-01 (9.3359e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [38][  0/129]\tTime  0.266 ( 0.266)\tData  0.086 ( 0.086)\tLoss 6.9141e-01 (6.9141e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [38][  0/129]\tTime  0.195 ( 0.195)\tData  0.043 ( 0.043)\tLoss 6.6797e-01 (6.6797e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [38][  0/129]\tTime  0.201 ( 0.201)\tData  0.058 ( 0.058)\tLoss 9.3359e-01 (9.3359e-01)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Train:  Epoch: [38][  0/129]\tTime  0.274 ( 0.274)\tData  0.092 ( 0.092)\tLoss 6.2500e-01 (6.2500e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Train:  Epoch: [38][  0/129]\tTime  0.292 ( 0.292)\tData  0.120 ( 0.120)\tLoss 9.3359e-01 (9.3359e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Train:  Epoch: [38][  0/129]\tTime  0.253 ( 0.253)\tData  0.077 ( 0.077)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [38][ 30/129]\tTime  0.153 ( 0.220)\tData  0.011 ( 0.014)\tLoss 6.7969e-01 (8.4539e-01)\tAcc@1  62.50 ( 63.56)\n",
      "[xla:7]Train:  Epoch: [38][ 30/129]\tTime  0.192 ( 0.212)\tData  0.012 ( 0.015)\tLoss 8.8281e-01 (8.6202e-01)\tAcc@1  62.50 ( 60.91)\n",
      "[xla:2]Train:  Epoch: [38][ 30/129]\tTime  0.179 ( 0.209)\tData  0.009 ( 0.015)\tLoss 7.6562e-01 (8.6076e-01)\tAcc@1  62.50 ( 60.30)\n",
      "[xla:1]Train:  Epoch: [38][ 30/129]\tTime  0.180 ( 0.205)\tData  0.011 ( 0.015)\tLoss 8.6719e-01 (8.0192e-01)\tAcc@1  62.50 ( 65.20)\n",
      "[xla:0]Train:  Epoch: [38][ 30/129]\tTime  0.202 ( 0.210)\tData  0.009 ( 0.015)\tLoss 9.6484e-01 (8.7311e-01)\tAcc@1  62.50 ( 63.14)\n",
      "[xla:4]Train:  Epoch: [38][ 30/129]\tTime  0.195 ( 0.214)\tData  0.011 ( 0.018)\tLoss 6.9141e-01 (8.1615e-01)\tAcc@1  62.50 ( 62.94)\n",
      "[xla:3]Train:  Epoch: [38][ 30/129]\tTime  0.204 ( 0.208)\tData  0.011 ( 0.017)\tLoss 9.8438e-01 (8.3165e-01)\tAcc@1  56.25 ( 61.57)\n",
      "[xla:6]Train:  Epoch: [38][ 30/129]\tTime  0.219 ( 0.208)\tData  0.011 ( 0.017)\tLoss 9.2188e-01 (7.7205e-01)\tAcc@1  62.50 ( 65.91)\n",
      "[xla:4]Train:  Epoch: [38][ 60/129]\tTime  0.192 ( 0.210)\tData  0.012 ( 0.015)\tLoss 8.5547e-01 (7.9812e-01)\tAcc@1  43.75 ( 64.07)\n",
      "[xla:0]Train:  Epoch: [38][ 60/129]\tTime  0.205 ( 0.208)\tData  0.013 ( 0.014)\tLoss 7.2266e-01 (8.2361e-01)\tAcc@1  62.50 ( 64.99)\n",
      "[xla:3]Train:  Epoch: [38][ 60/129]\tTime  0.173 ( 0.207)\tData  0.010 ( 0.015)\tLoss 6.0938e-01 (7.9387e-01)\tAcc@1  75.00 ( 65.62)\n",
      "[xla:5]Train:  Epoch: [38][ 60/129]\tTime  0.193 ( 0.214)\tData  0.009 ( 0.014)\tLoss 8.9062e-01 (8.2294e-01)\tAcc@1  56.25 ( 62.56)\n",
      "[xla:6]Train:  Epoch: [38][ 60/129]\tTime  0.224 ( 0.207)\tData  0.017 ( 0.015)\tLoss 9.2578e-01 (7.9678e-01)\tAcc@1  50.00 ( 65.07)\n",
      "[xla:2]Train:  Epoch: [38][ 60/129]\tTime  0.186 ( 0.208)\tData  0.009 ( 0.014)\tLoss 7.9688e-01 (8.3158e-01)\tAcc@1  69.00 ( 62.10)\n",
      "[xla:1]Train:  Epoch: [38][ 60/129]\tTime  0.197 ( 0.207)\tData  0.010 ( 0.015)\tLoss 7.5000e-01 (8.1954e-01)\tAcc@1  56.25 ( 63.90)\n",
      "[xla:7]Train:  Epoch: [38][ 60/129]\tTime  0.196 ( 0.210)\tData  0.010 ( 0.014)\tLoss 5.7031e-01 (8.3171e-01)\tAcc@1  75.00 ( 62.43)\n",
      "[xla:2]Train:  Epoch: [38][ 90/129]\tTime  0.173 ( 0.206)\tData  0.021 ( 0.015)\tLoss 6.4062e-01 (8.3512e-01)\tAcc@1  75.00 ( 62.78)\n",
      "[xla:4]Train:  Epoch: [38][ 90/129]\tTime  0.181 ( 0.208)\tData  0.022 ( 0.015)\tLoss 9.8047e-01 (7.9887e-01)\tAcc@1  56.25 ( 64.52)\n",
      "[xla:5]Train:  Epoch: [38][ 90/129]\tTime  0.177 ( 0.210)\tData  0.024 ( 0.015)\tLoss 6.6797e-01 (8.1817e-01)\tAcc@1  75.00 ( 63.04)\n",
      "[xla:7]Train:  Epoch: [38][ 90/129]\tTime  0.189 ( 0.207)\tData  0.011 ( 0.014)\tLoss 7.1484e-01 (8.3169e-01)\tAcc@1  62.50 ( 61.37)\n",
      "[xla:6]Train:  Epoch: [38][ 90/129]\tTime  0.193 ( 0.206)\tData  0.011 ( 0.015)\tLoss 6.5625e-01 (7.9054e-01)\tAcc@1  69.00 ( 64.23)\n",
      "[xla:0]Train:  Epoch: [38][ 90/129]\tTime  0.193 ( 0.206)\tData  0.015 ( 0.014)\tLoss 1.2500e+00 (8.1018e-01)\tAcc@1  37.50 ( 65.41)\n",
      "[xla:3]Train:  Epoch: [38][ 90/129]\tTime  0.190 ( 0.205)\tData  0.012 ( 0.015)\tLoss 4.7266e-01 (7.8816e-01)\tAcc@1  81.00 ( 65.35)\n",
      "[xla:1]Train:  Epoch: [38][ 90/129]\tTime  0.194 ( 0.205)\tData  0.026 ( 0.014)\tLoss 5.9375e-01 (8.0143e-01)\tAcc@1  87.50 ( 64.84)\n",
      "[xla:4]Train:  Epoch: [38][120/129]\tTime  0.169 ( 0.206)\tData  0.010 ( 0.015)\tLoss 8.9844e-01 (7.7776e-01)\tAcc@1  69.00 ( 65.42)\n",
      "[xla:7]Train:  Epoch: [38][120/129]\tTime  0.168 ( 0.206)\tData  0.014 ( 0.014)\tLoss 8.5156e-01 (8.0751e-01)\tAcc@1  75.00 ( 63.21)\n",
      "[xla:2]Train:  Epoch: [38][120/129]\tTime  0.170 ( 0.205)\tData  0.011 ( 0.015)\tLoss 7.1875e-01 (8.0419e-01)\tAcc@1  81.00 ( 64.17)\n",
      "[xla:6]Train:  Epoch: [38][120/129]\tTime  0.193 ( 0.205)\tData  0.010 ( 0.015)\tLoss 6.0547e-01 (7.6542e-01)\tAcc@1  75.00 ( 65.62)\n",
      "[xla:3]Train:  Epoch: [38][120/129]\tTime  0.210 ( 0.205)\tData  0.012 ( 0.015)\tLoss 4.8047e-01 (7.7005e-01)\tAcc@1  87.50 ( 66.21)\n",
      "[xla:5]Train:  Epoch: [38][120/129]\tTime  0.210 ( 0.208)\tData  0.011 ( 0.015)\tLoss 4.8242e-01 (8.1084e-01)\tAcc@1  81.00 ( 63.44)\n",
      "[xla:0]Train:  Epoch: [38][120/129]\tTime  0.190 ( 0.205)\tData  0.009 ( 0.015)\tLoss 7.3047e-01 (7.8395e-01)\tAcc@1  81.00 ( 66.87)\n",
      "[xla:1]Train:  Epoch: [38][120/129]\tTime  0.212 ( 0.205)\tData  0.011 ( 0.014)\tLoss 7.4609e-01 (7.7768e-01)\tAcc@1  69.00 ( 65.93)\n",
      "[xla:4]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.113 ( 0.113)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  56.25 ( 56.25)\n",
      "Finished training epoch 38\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/6]\tTime  0.097 ( 0.097)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:0]Validation: [0/6]\tTime  0.115 ( 0.115)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/6]\tTime  0.077 ( 0.077)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.119 ( 0.119)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:2]Validation: [0/6]\tTime  0.092 ( 0.092)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Validation: [0/6]\tTime  0.123 ( 0.123)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Train:  Epoch: [39][  0/129]\tTime  0.160 ( 0.160)\tData  0.049 ( 0.049)\tLoss 9.9609e-01 (9.9609e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:4]Train:  Epoch: [39][  0/129]\tTime  0.260 ( 0.260)\tData  0.065 ( 0.065)\tLoss 8.6328e-01 (8.6328e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [39][  0/129]\tTime  0.169 ( 0.169)\tData  0.032 ( 0.032)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [39][  0/129]\tTime  0.252 ( 0.252)\tData  0.056 ( 0.056)\tLoss 8.2031e-01 (8.2031e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Train:  Epoch: [39][  0/129]\tTime  0.200 ( 0.200)\tData  0.046 ( 0.046)\tLoss 6.9922e-01 (6.9922e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Train:  Epoch: [39][  0/129]\tTime  0.268 ( 0.268)\tData  0.092 ( 0.092)\tLoss 8.6328e-01 (8.6328e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Train:  Epoch: [39][  0/129]\tTime  0.262 ( 0.262)\tData  0.093 ( 0.093)\tLoss 1.0312e+00 (1.0312e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Train:  Epoch: [39][  0/129]\tTime  0.198 ( 0.198)\tData  0.038 ( 0.038)\tLoss 9.5703e-01 (9.5703e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Train:  Epoch: [39][ 30/129]\tTime  0.176 ( 0.207)\tData  0.011 ( 0.018)\tLoss 8.0859e-01 (8.2170e-01)\tAcc@1  69.00 ( 61.35)\n",
      "[xla:6]Train:  Epoch: [39][ 30/129]\tTime  0.201 ( 0.204)\tData  0.011 ( 0.018)\tLoss 8.7500e-01 (8.0122e-01)\tAcc@1  62.50 ( 63.14)\n",
      "[xla:7]Train:  Epoch: [39][ 30/129]\tTime  0.189 ( 0.220)\tData  0.010 ( 0.015)\tLoss 9.2969e-01 (8.6114e-01)\tAcc@1  62.50 ( 61.73)\n",
      "[xla:5]Train:  Epoch: [39][ 30/129]\tTime  0.178 ( 0.202)\tData  0.012 ( 0.015)\tLoss 7.0703e-01 (8.3531e-01)\tAcc@1  69.00 ( 64.76)\n",
      "[xla:1]Train:  Epoch: [39][ 30/129]\tTime  0.209 ( 0.207)\tData  0.010 ( 0.017)\tLoss 7.7344e-01 (8.2901e-01)\tAcc@1  69.00 ( 64.75)\n",
      "[xla:2]Train:  Epoch: [39][ 30/129]\tTime  0.192 ( 0.208)\tData  0.018 ( 0.015)\tLoss 7.7734e-01 (8.4892e-01)\tAcc@1  69.00 ( 60.13)\n",
      "[xla:4]Train:  Epoch: [39][ 30/129]\tTime  0.190 ( 0.223)\tData  0.017 ( 0.016)\tLoss 7.4219e-01 (8.2195e-01)\tAcc@1  75.00 ( 62.91)\n",
      "[xla:0]Train:  Epoch: [39][ 30/129]\tTime  0.187 ( 0.209)\tData  0.011 ( 0.017)\tLoss 8.9062e-01 (8.7084e-01)\tAcc@1  50.00 ( 60.52)\n",
      "[xla:4]Train:  Epoch: [39][ 60/129]\tTime  0.206 ( 0.214)\tData  0.014 ( 0.015)\tLoss 8.0078e-01 (7.9547e-01)\tAcc@1  62.50 ( 63.95)\n",
      "[xla:1]Train:  Epoch: [39][ 60/129]\tTime  0.167 ( 0.207)\tData  0.009 ( 0.015)\tLoss 7.8125e-01 (8.2899e-01)\tAcc@1  62.50 ( 64.99)\n",
      "[xla:5]Train:  Epoch: [39][ 60/129]\tTime  0.195 ( 0.204)\tData  0.011 ( 0.015)\tLoss 1.0391e+00 (8.4311e-01)\tAcc@1  56.25 ( 62.96)\n",
      "[xla:3]Train:  Epoch: [39][ 60/129]\tTime  0.217 ( 0.207)\tData  0.012 ( 0.016)\tLoss 6.9531e-01 (8.1090e-01)\tAcc@1  75.00 ( 63.04)\n",
      "[xla:2]Train:  Epoch: [39][ 60/129]\tTime  0.199 ( 0.207)\tData  0.010 ( 0.016)\tLoss 9.1016e-01 (8.3229e-01)\tAcc@1  50.00 ( 61.39)\n",
      "[xla:6]Train:  Epoch: [39][ 60/129]\tTime  0.191 ( 0.206)\tData  0.010 ( 0.016)\tLoss 9.3750e-01 (8.0946e-01)\tAcc@1  43.75 ( 63.04)\n",
      "[xla:0]Train:  Epoch: [39][ 60/129]\tTime  0.225 ( 0.208)\tData  0.012 ( 0.016)\tLoss 7.3828e-01 (8.3162e-01)\tAcc@1  56.25 ( 62.74)\n",
      "[xla:7]Train:  Epoch: [39][ 60/129]\tTime  0.207 ( 0.214)\tData  0.009 ( 0.015)\tLoss 5.0391e-01 (8.3254e-01)\tAcc@1  81.00 ( 63.36)\n",
      "[xla:5]Train:  Epoch: [39][ 90/129]\tTime  0.215 ( 0.208)\tData  0.011 ( 0.015)\tLoss 6.7969e-01 (8.3624e-01)\tAcc@1  75.00 ( 63.17)\n",
      "[xla:4]Train:  Epoch: [39][ 90/129]\tTime  0.183 ( 0.215)\tData  0.009 ( 0.015)\tLoss 1.0078e+00 (7.9694e-01)\tAcc@1  50.00 ( 64.52)\n",
      "[xla:3]Train:  Epoch: [39][ 90/129]\tTime  0.205 ( 0.210)\tData  0.011 ( 0.016)\tLoss 4.6875e-01 (7.9943e-01)\tAcc@1  87.50 ( 63.97)\n",
      "[xla:0]Train:  Epoch: [39][ 90/129]\tTime  0.229 ( 0.211)\tData  0.011 ( 0.016)\tLoss 1.0547e+00 (8.0724e-01)\tAcc@1  50.00 ( 63.97)\n",
      "[xla:2]Train:  Epoch: [39][ 90/129]\tTime  0.230 ( 0.210)\tData  0.011 ( 0.016)\tLoss 6.2109e-01 (8.4362e-01)\tAcc@1  75.00 ( 61.01)\n",
      "[xla:7]Train:  Epoch: [39][ 90/129]\tTime  0.245 ( 0.215)\tData  0.012 ( 0.015)\tLoss 5.2344e-01 (8.2976e-01)\tAcc@1  69.00 ( 62.76)\n",
      "[xla:1]Train:  Epoch: [39][ 90/129]\tTime  0.283 ( 0.210)\tData  0.014 ( 0.015)\tLoss 4.9219e-01 (8.0540e-01)\tAcc@1  81.00 ( 65.35)\n",
      "[xla:6]Train:  Epoch: [39][ 90/129]\tTime  0.236 ( 0.209)\tData  0.009 ( 0.016)\tLoss 4.7266e-01 (8.0037e-01)\tAcc@1  81.00 ( 64.10)\n",
      "[xla:4]Train:  Epoch: [39][120/129]\tTime  0.175 ( 0.216)\tData  0.016 ( 0.015)\tLoss 7.7734e-01 (7.8351e-01)\tAcc@1  69.00 ( 65.20)\n",
      "[xla:1]Train:  Epoch: [39][120/129]\tTime  0.188 ( 0.212)\tData  0.016 ( 0.016)\tLoss 8.5156e-01 (7.8612e-01)\tAcc@1  56.25 ( 66.19)\n",
      "[xla:3]Train:  Epoch: [39][120/129]\tTime  0.196 ( 0.212)\tData  0.010 ( 0.016)\tLoss 4.8047e-01 (7.7169e-01)\tAcc@1  87.50 ( 65.74)\n",
      "[xla:5]Train:  Epoch: [39][120/129]\tTime  0.195 ( 0.210)\tData  0.011 ( 0.015)\tLoss 4.9414e-01 (8.2031e-01)\tAcc@1  75.00 ( 63.58)\n",
      "[xla:6]Train:  Epoch: [39][120/129]\tTime  0.180 ( 0.211)\tData  0.009 ( 0.015)\tLoss 5.2344e-01 (7.7918e-01)\tAcc@1  69.00 ( 65.21)\n",
      "[xla:0]Train:  Epoch: [39][120/129]\tTime  0.195 ( 0.212)\tData  0.014 ( 0.015)\tLoss 6.8359e-01 (7.8077e-01)\tAcc@1  81.00 ( 65.15)\n",
      "[xla:2]Train:  Epoch: [39][120/129]\tTime  0.208 ( 0.212)\tData  0.011 ( 0.016)\tLoss 7.6953e-01 (8.0859e-01)\tAcc@1  69.00 ( 63.62)\n",
      "[xla:7]Train:  Epoch: [39][120/129]\tTime  0.210 ( 0.215)\tData  0.016 ( 0.015)\tLoss 8.7891e-01 (8.1143e-01)\tAcc@1  69.00 ( 63.89)\n",
      "[xla:6]Validation: [0/6]\tTime  0.106 ( 0.106)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/6]\tTime  0.096 ( 0.096)\tLoss 7.0312e-01 (7.0312e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 39\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/6]\tTime  0.068 ( 0.068)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/6]\tTime  0.077 ( 0.077)\tLoss 1.0000e+00 (1.0000e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:1]Validation: [0/6]\tTime  0.082 ( 0.082)\tLoss 1.2266e+00 (1.2266e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:5]Validation: [0/6]\tTime  0.121 ( 0.121)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:2]Validation: [0/6]\tTime  0.087 ( 0.087)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:4]Validation: [0/6]\tTime  0.099 ( 0.099)\tLoss 1.2422e+00 (1.2422e+00)\tAcc@1  50.00 ( 50.00)\n",
      "Saving Model ..\n",
      "Model Saved.\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 12\n",
      "  Accumulator: 40s699ms363.174us\n",
      "  ValueRate: 034ms596.251us / second\n",
      "  Rate: 0.0101552 / second\n",
      "  Percentiles: 1%=009ms169.293us; 5%=009ms169.293us; 10%=021ms279.796us; 20%=022ms637.129us; 50%=03s505ms134.386us; 80%=05s613ms733.709us; 90%=11s481ms560.768us; 95%=12s561ms670.022us; 99%=12s561ms670.022us\n",
      "Metric: DeviceLockWait\n",
      "  TotalSamples: 16318\n",
      "  Accumulator: 179ms710.955us\n",
      "  ValueRate: 061.922us / second\n",
      "  Rate: 14.1874 / second\n",
      "  Percentiles: 1%=002.447us; 5%=002.855us; 10%=002.968us; 20%=003.117us; 50%=003.514us; 80%=004.027us; 90%=004.317us; 95%=004.638us; 99%=014.199us\n",
      "Metric: ExecuteTime\n",
      "  TotalSamples: 16238\n",
      "  Accumulator: 07m49s321ms305.183us\n",
      "  ValueRate: 341ms446.412us / second\n",
      "  Rate: 14.1059 / second\n",
      "  Percentiles: 1%=002ms177.291us; 5%=007ms700.240us; 10%=008ms584.331us; 20%=008ms808.722us; 50%=008ms162.091us; 80%=050ms889.296us; 90%=066ms092.423us; 95%=081ms589.718us; 99%=110ms730.528us\n",
      "Metric: InboundData\n",
      "  TotalSamples: 10904\n",
      "  Accumulator: 257.02MB\n",
      "  ValueRate: 2.57MB / second\n",
      "  Rate: 10.2222 / second\n",
      "  Percentiles: 1%=2.00B; 5%=2.00B; 10%=2.00B; 20%=2.00B; 50%=2.00B; 80%=2.00B; 90%=1.50KB; 95%=1.50KB; 99%=4.50MB\n",
      "Metric: InputOutputAliasCount\n",
      "  TotalSamples: 8\n",
      "  Accumulator: 954.00\n",
      "  ValueRate: 14.27 / second\n",
      "  Rate: 0.119651 / second\n",
      "  Percentiles: 1%=1.00; 5%=1.00; 10%=1.00; 20%=1.00; 50%=4.00; 80%=421.00; 90%=421.00; 95%=421.00; 99%=421.00\n",
      "Metric: IrValueTensorToXlaData\n",
      "  TotalSamples: 104\n",
      "  Accumulator: 02s070ms449.635us\n",
      "  ValueRate: 04s023ms932.417us / second\n",
      "  Rate: 202.074 / second\n",
      "  Percentiles: 1%=842.830us; 5%=905.405us; 10%=001ms002.961us; 20%=001ms074.530us; 50%=002ms516.551us; 80%=005ms348.224us; 90%=015ms725.326us; 95%=016ms731.302us; 99%=040ms583.935us\n",
      "Metric: OutboundData\n",
      "  TotalSamples: 27307\n",
      "  Accumulator: 363.21MB\n",
      "  ValueRate: 94.31KB / second\n",
      "  Rate: 24.2171 / second\n",
      "  Percentiles: 1%=2.00B; 5%=2.00B; 10%=2.00B; 20%=2.00B; 50%=2.00B; 80%=8.00B; 90%=8.00B; 95%=20.12KB; 99%=80.50KB\n",
      "Metric: ReleaseDataHandlesTime\n",
      "  TotalSamples: 68218\n",
      "  Accumulator: 01m06s394ms300.479us\n",
      "  ValueRate: 062ms772.238us / second\n",
      "  Rate: 62.0765 / second\n",
      "  Percentiles: 1%=446.964us; 5%=505.783us; 10%=538.858us; 20%=585.104us; 50%=718.713us; 80%=001ms024.285us; 90%=002ms706.868us; 95%=002ms333.258us; 99%=005ms443.264us\n",
      "Metric: TensorsGraphSize\n",
      "  TotalSamples: 16238\n",
      "  Accumulator: 42383145.00\n",
      "  ValueRate: 35330.68 / second\n",
      "  Rate: 14.1049 / second\n",
      "  Percentiles: 1%=1.00; 5%=980.00; 10%=1135.00; 20%=1135.00; 50%=1145.00; 80%=5796.00; 90%=5796.00; 95%=5796.00; 99%=5796.00\n",
      "Metric: TransferFromServerTime\n",
      "  TotalSamples: 10904\n",
      "  Accumulator: 19s018ms279.969us\n",
      "  ValueRate: 034ms656.430us / second\n",
      "  Rate: 10.2222 / second\n",
      "  Percentiles: 1%=884.515us; 5%=945.473us; 10%=988.288us; 20%=001ms051.048us; 50%=001ms222.714us; 80%=002ms722.850us; 90%=004ms732.222us; 95%=005ms036.527us; 99%=015ms643.318us\n",
      "Metric: TransferToServerTime\n",
      "  TotalSamples: 27307\n",
      "  Accumulator: 07m33s239ms989.519us\n",
      "  ValueRate: 336ms822.996us / second\n",
      "  Rate: 24.2185 / second\n",
      "  Percentiles: 1%=884.876us; 5%=001ms015.749us; 10%=001ms094.685us; 20%=001ms225.109us; 50%=002ms195.457us; 80%=030ms256.593us; 90%=052ms185.227us; 95%=064ms505.375us; 99%=084ms592.084us\n",
      "Metric: TransferToServerTransformTime\n",
      "  TotalSamples: 27307\n",
      "  Accumulator: 30s625ms311.032us\n",
      "  ValueRate: 021ms384.329us / second\n",
      "  Rate: 24.2171 / second\n",
      "  Percentiles: 1%=043.922us; 5%=047.001us; 10%=051.072us; 20%=055.749us; 50%=102.048us; 80%=252.902us; 90%=002ms711.410us; 95%=005ms086.093us; 99%=015ms603.438us\n",
      "Counter: CachedCompile\n",
      "  Value: 16226\n",
      "Counter: CreateCompileHandles\n",
      "  Value: 12\n",
      "Counter: CreateDataHandles\n",
      "  Value: 2226368\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 8883656\n",
      "Counter: DestroyDataHandles\n",
      "  Value: 2225787\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 8883240\n",
      "Counter: DeviceDataCacheMiss\n",
      "  Value: 20643\n",
      "Counter: MarkStep\n",
      "  Value: 5480\n",
      "Counter: ReleaseDataHandles\n",
      "  Value: 2225787\n",
      "Counter: UncachedCompile\n",
      "  Value: 12\n",
      "Counter: XRTAllocateFromTensor_Empty\n",
      "  Value: 38\n",
      "Counter: XrtCompile_Empty\n",
      "  Value: 128\n",
      "Counter: XrtExecuteChained_Empty\n",
      "  Value: 128\n",
      "Counter: XrtExecute_Empty\n",
      "  Value: 128\n",
      "Counter: XrtMemoryInfo_Empty\n",
      "  Value: 128\n",
      "Counter: XrtRead_Empty\n",
      "  Value: 128\n",
      "Counter: XrtReleaseAllocationHandle_Empty\n",
      "  Value: 128\n",
      "Counter: XrtReleaseCompileHandle_Empty\n",
      "  Value: 128\n",
      "Counter: XrtSessionCount\n",
      "  Value: 10\n",
      "Counter: XrtSubTuple_Empty\n",
      "  Value: 128\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 10800\n",
      "Counter: xla::_log_softmax\n",
      "  Value: 5400\n",
      "Counter: xla::_log_softmax_backward_data\n",
      "  Value: 5160\n",
      "Counter: xla::_softmax\n",
      "  Value: 32400\n",
      "Counter: xla::_softmax_backward_data\n",
      "  Value: 30960\n",
      "Counter: xla::_unsafe_view\n",
      "  Value: 259200\n",
      "Counter: xla::add\n",
      "  Value: 199200\n",
      "Counter: xla::add_\n",
      "  Value: 2340856\n",
      "Counter: xla::addcdiv_\n",
      "  Value: 536640\n",
      "Counter: xla::addcmul\n",
      "  Value: 75600\n",
      "Counter: xla::addcmul_\n",
      "  Value: 536640\n",
      "Counter: xla::addmm\n",
      "  Value: 5400\n",
      "Counter: xla::arange_out\n",
      "  Value: 5400\n",
      "Counter: xla::as_strided\n",
      "  Value: 104\n",
      "Counter: xla::bernoulli_\n",
      "  Value: 72240\n",
      "Counter: xla::bmm\n",
      "  Value: 188640\n",
      "Counter: xla::cat\n",
      "  Value: 5400\n",
      "Counter: xla::clone\n",
      "  Value: 30960\n",
      "Counter: xla::copy_\n",
      "  Value: 11008\n",
      "Counter: xla::div\n",
      "  Value: 68520\n",
      "Counter: xla::div_\n",
      "  Value: 72240\n",
      "Counter: xla::embedding\n",
      "  Value: 10800\n",
      "Counter: xla::embedding_dense_backward\n",
      "  Value: 10320\n",
      "Counter: xla::empty\n",
      "  Value: 99072\n",
      "Counter: xla::empty_strided\n",
      "  Value: 104\n",
      "Counter: xla::eq\n",
      "  Value: 37800\n",
      "Counter: xla::expand\n",
      "  Value: 177960\n",
      "Counter: xla::fill_\n",
      "  Value: 5160\n",
      "Counter: xla::gelu\n",
      "  Value: 32400\n",
      "Counter: xla::gelu_backward\n",
      "  Value: 30960\n",
      "Counter: xla::index_select\n",
      "  Value: 10800\n",
      "Counter: xla::masked_fill_\n",
      "  Value: 63360\n",
      "Counter: xla::max\n",
      "  Value: 5400\n",
      "Counter: xla::mean\n",
      "  Value: 5400\n",
      "Counter: xla::mm\n",
      "  Value: 576240\n",
      "Counter: xla::mul\n",
      "  Value: 361200\n",
      "Counter: xla::mul_\n",
      "  Value: 1078680\n",
      "Counter: xla::native_batch_norm\n",
      "  Value: 75600\n",
      "Counter: xla::native_batch_norm_backward\n",
      "  Value: 72240\n",
      "Counter: xla::native_layer_norm\n",
      "  Value: 75600\n",
      "Counter: xla::native_layer_norm_backward\n",
      "  Value: 72240\n",
      "Counter: xla::nll_loss_backward\n",
      "  Value: 5160\n",
      "Counter: xla::nll_loss_forward\n",
      "  Value: 10560\n",
      "Counter: xla::scatter_\n",
      "  Value: 5160\n",
      "Counter: xla::slice\n",
      "  Value: 15720\n",
      "Counter: xla::sqrt\n",
      "  Value: 536640\n",
      "Counter: xla::sub\n",
      "  Value: 72240\n",
      "Counter: xla::sum\n",
      "  Value: 340800\n",
      "Counter: xla::t\n",
      "  Value: 777960\n",
      "Counter: xla::topk\n",
      "  Value: 5400\n",
      "Counter: xla::transpose\n",
      "  Value: 440640\n",
      "Counter: xla::unsqueeze\n",
      "  Value: 20880\n",
      "Counter: xla::view\n",
      "  Value: 2378760\n",
      "Counter: xla::zero_\n",
      "  Value: 541904\n",
      "Metric: XrtAllocateFromTensor\n",
      "  TotalSamples: 336856\n",
      "  Accumulator: 07m19s552ms227.251us\n",
      "  Mean: 001ms174.281us\n",
      "  StdDev: 985.626us\n",
      "  Rate: 251.727 / second\n",
      "  Percentiles: 25%=374.932us; 50%=716.916us; 80%=002ms448.719us; 90%=003ms770.039us; 95%=003ms964.874us; 99%=003ms150.992us\n",
      "Metric: XrtCompile\n",
      "  TotalSamples: 68\n",
      "  Accumulator: 04m25s523ms143.516us\n",
      "  Mean: 04s890ms046.228us\n",
      "  StdDev: 04s209ms631.836us\n",
      "  Rate: 0.0575499 / second\n",
      "  Percentiles: 25%=020ms418.303us; 50%=02s499ms893.117us; 80%=11s265ms125.976us; 90%=11s351ms610.152us; 95%=11s368ms188.635us; 99%=11s445ms259.250us\n",
      "Metric: XrtExecute\n",
      "  TotalSamples: 129645\n",
      "  Accumulator: 51m52s860ms549.751us\n",
      "  Mean: 021ms900.844us\n",
      "  StdDev: 026ms442.913us\n",
      "  Rate: 81.4084 / second\n",
      "  Percentiles: 25%=007ms931.695us; 50%=007ms169.820us; 80%=043ms415.176us; 90%=061ms827.351us; 95%=073ms531.798us; 99%=104ms015.685us\n",
      "Metric: XrtExecutorEvict\n",
      "  TotalSamples: 0\n",
      "  Accumulator: nanB\n",
      "  Mean: nanB\n",
      "  StdDev: nanB\n",
      "  Percentiles: \n",
      "Metric: XrtReadLiteral\n",
      "  TotalSamples: 86504\n",
      "  Accumulator: 34s022ms401.424us\n",
      "  Mean: 925.553us\n",
      "  StdDev: 015ms943.079us\n",
      "  Rate: 63.7593 / second\n",
      "  Percentiles: 25%=313.417us; 50%=386.461us; 80%=482.580us; 90%=551.126us; 95%=643.432us; 99%=004ms684.548us\n",
      "Metric: XrtReleaseAllocation\n",
      "  TotalSamples: 541398\n",
      "  Accumulator: 48s041ms623.192us\n",
      "  Mean: 066.689us\n",
      "  StdDev: 149.674us\n",
      "  Rate: 155.702 / second\n",
      "  Percentiles: 25%=015.194us; 50%=021.824us; 80%=042.574us; 90%=111.265us; 95%=386.155us; 99%=832.243us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    # torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    _run()\n",
    "\n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetRetriever(Dataset):\n",
    "    def __init__(self, df, ids, mask):\n",
    "        self.df = df\n",
    "        self.ids = ids\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):   \n",
    "        ids = self.ids[index]\n",
    "        mask = self.mask[index]\n",
    "        return {\n",
    "            'ids':torch.tensor(ids),\n",
    "            'mask':torch.tensor(mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "test_ids, test_mask = fast_encode(test, fast_tokenizer)\n",
    "\n",
    "test_dataset = TestDatasetRetriever(test, test_ids, test_mask)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load Serialized Model\n",
    "device = xm.xla_device()\n",
    "model = WRAPPED_MODEL.to(device).eval()\n",
    "model.load_state_dict(xser.load(\"model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:10<00:00, 15.71it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "\n",
    "for i, data in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "    ids = data[\"ids\"]\n",
    "    mask = data[\"mask\"]\n",
    "    ids = ids.to(device, dtype=torch.long)\n",
    "    mask = mask.to(device, dtype=torch.long)\n",
    "    outputs = model(\n",
    "        input_ids = ids,\n",
    "        attention_mask = mask,\n",
    "    )\n",
    "    outputs_np = outputs.cpu().detach().numpy().tolist()\n",
    "    test_preds.extend(outputs_np)  \n",
    "    \n",
    "test_preds = torch.FloatTensor(test_preds)\n",
    "top1_prob, top1_label = torch.topk(test_preds, 1)\n",
    "y = top1_label.cpu().detach().numpy()\n",
    "sample_submission.prediction = y\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01f314f19d7e4a77bbda4abc8edb6969": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_63f247bf3f3647248a12a247f675ddad",
        "IPY_MODEL_6c644f254f38486789f85056637a527a"
       ],
       "layout": "IPY_MODEL_a5c4885324f042eaa8a96acfec1b9871"
      }
     },
     "0c5f553b382a4401ac656551a206ba1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21067d923f264c809402001784078e41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2315baa2fff646898e9c206d19deb628": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3f23637c0c8a4b018a303c6ae83a4b42",
       "placeholder": "​",
       "style": "IPY_MODEL_9b6da934ed2e4e66b5b75c9623339bc5",
       "value": " 996k/996k [00:00&lt;00:00, 1.39MB/s]"
      }
     },
     "2658a82dcbd44959bcb47d8f098738c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0c5f553b382a4401ac656551a206ba1b",
       "max": 995526.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7416770e38ce4be1bf10fdb684f9c867",
       "value": 995526.0
      }
     },
     "2c2cb6d2df094764ba096d631772d0ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f23637c0c8a4b018a303c6ae83a4b42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43930634d0e54dc19ad5c1ed0d556251": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63f247bf3f3647248a12a247f675ddad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d76c28cdf3a84b87b35268ef21057ae3",
       "max": 541808922.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e74e70322c8f461498809001ab8bf955",
       "value": 541808922.0
      }
     },
     "66ec96a5540849f8a4d6c1b99cb43097": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c2cb6d2df094764ba096d631772d0ca",
       "max": 466.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d18193db4f36443887789afc69d4fb8b",
       "value": 466.0
      }
     },
     "6c644f254f38486789f85056637a527a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_43930634d0e54dc19ad5c1ed0d556251",
       "placeholder": "​",
       "style": "IPY_MODEL_aa3c1326a7244360a271e856dab8518b",
       "value": " 542M/542M [00:17&lt;00:00, 31.4MB/s]"
      }
     },
     "714fae57be2a496a9dc15b6f9209f2e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66ec96a5540849f8a4d6c1b99cb43097",
        "IPY_MODEL_a120be8a73bb436293107189bccc072f"
       ],
       "layout": "IPY_MODEL_8810b0c421cb47e5b4a20c8d641c3721"
      }
     },
     "7416770e38ce4be1bf10fdb684f9c867": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "83edcd65105748fb8a001b8c92044c31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8810b0c421cb47e5b4a20c8d641c3721": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "936f4837670f4d249ce66fefd76f1e1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b6da934ed2e4e66b5b75c9623339bc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a120be8a73bb436293107189bccc072f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_936f4837670f4d249ce66fefd76f1e1c",
       "placeholder": "​",
       "style": "IPY_MODEL_21067d923f264c809402001784078e41",
       "value": " 466/466 [00:20&lt;00:00, 22.6B/s]"
      }
     },
     "a5c4885324f042eaa8a96acfec1b9871": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa3c1326a7244360a271e856dab8518b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ba8947199fc141679f2c920ace26486d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2658a82dcbd44959bcb47d8f098738c8",
        "IPY_MODEL_2315baa2fff646898e9c206d19deb628"
       ],
       "layout": "IPY_MODEL_83edcd65105748fb8a001b8c92044c31"
      }
     },
     "d18193db4f36443887789afc69d4fb8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d76c28cdf3a84b87b35268ef21057ae3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e74e70322c8f461498809001ab8bf955": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
