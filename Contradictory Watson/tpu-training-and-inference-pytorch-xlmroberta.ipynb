{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Check TPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  ['10.0.0.2:8470']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
    "   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "   tpu = None\n",
    "if tpu:\n",
    "   tf.config.experimental_connect_to_cluster(tpu)\n",
    "   tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "   strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "   strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-googletrans.git\r\n",
      "  Cloning https://github.com/ssut/py-googletrans.git to /tmp/pip-req-build-5xqzs6o8\r\n",
      "  Running command git clone -q https://github.com/ssut/py-googletrans.git /tmp/pip-req-build-5xqzs6o8\r\n",
      "Collecting httpx==0.13.3\r\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.0.0) (2020.6.20)\r\n",
      "Requirement already satisfied: chardet==3.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.0.0) (3.0.4)\r\n",
      "Collecting sniffio\r\n",
      "  Downloading sniffio-1.1.0-py3-none-any.whl (4.5 kB)\r\n",
      "Collecting rfc3986<2,>=1.3\r\n",
      "  Downloading rfc3986-1.4.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Requirement already satisfied: idna==2.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.0.0) (2.9)\r\n",
      "Collecting hstspreload\r\n",
      "  Downloading hstspreload-2020.7.29-py3-none-any.whl (926 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 926 kB 15.1 MB/s \r\n",
      "\u001b[?25hCollecting httpcore==0.9.*\r\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 42 kB 904 kB/s \r\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\r\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \r\n",
      "\u001b[?25hCollecting h2==3.*\r\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.9 MB/s \r\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0\r\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting hpack<4,>=3.0\r\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\r\n",
      "Building wheels for collected packages: googletrans\r\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=16448 sha256=30c097b4914072cf7458d4c1e381ab79b4430362c54f1373d2c41beda0b44e33\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t4p21qpz/wheels/eb/82/a2/f07ad36dbee6290000d9057df7e3c81a973db81913efd3b252\r\n",
      "Successfully built googletrans\r\n",
      "Installing collected packages: sniffio, rfc3986, hstspreload, h11, hyperframe, hpack, h2, httpcore, httpx, googletrans\r\n",
      "Successfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.7.29 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.1.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  5115  100  5115    0     0  26096      0 --:--:-- --:--:-- --:--:-- 26096\r\n",
      "Updating... This may take around 2 minutes.\r\n",
      "Updating TPU runtime to pytorch-nightly ...\r\n",
      "Found existing installation: torch 1.5.0\r\n",
      "Uninstalling torch-1.5.0:\r\n",
      "  Successfully uninstalled torch-1.5.0\r\n",
      "Found existing installation: torchvision 0.6.0a0+35d732a\r\n",
      "Uninstalling torchvision-0.6.0a0+35d732a:\r\n",
      "Done updating TPU runtime\r\n",
      "  Successfully uninstalled torchvision-0.6.0a0+35d732a\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/108.8 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/124.6 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/2.4 MiB.                                      \r\n",
      "Processing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (1.18.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (0.18.2)\r\n",
      "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0+2f840b1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0+2f840b1 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: torch\r\n",
      "Successfully installed torch-1.7.0a0+2f840b1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.6+c4f8873\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (7.2.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.7.0a0+2f840b1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.18.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly) (0.18.2)\r\n",
      "Installing collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.8.0a0+40333c5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  libgfortran4 libopenblas-base\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libgfortran4 libomp5 libopenblas-base libopenblas-dev\r\n",
      "0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\r\n",
      "Need to get 8550 kB of archives.\r\n",
      "After this operation, 97.6 MB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\r\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\r\n",
      "Fetched 8550 kB in 0s (40.7 MB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libgfortran4:amd64.\r\n",
      "(Reading database ... 107745 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\r\n",
      "Unpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\r\n",
      "Selecting previously unselected package libopenblas-base:amd64.\r\n",
      "Preparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\r\n",
      "Unpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\r\n",
      "Selecting previously unselected package libopenblas-dev:amd64.\r\n",
      "Preparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\r\n",
      "Unpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (5.0.1-1) ...\r\n",
      "Setting up libomp5:amd64 (5.0.1-1) ...\r\n",
      "Setting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\r\n",
      "Setting up libopenblas-base:amd64 (0.2.20+ds-4) ...\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\r\n",
      "Setting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\r\n",
      "update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-googletrans.git\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH: 2f840b1662b487d5551d7230f8eb4d57645cfff5\n",
      "XLA: c4f8873d791e36e9819c102bac0e309d88b6ca8b\n",
      "CPU times: user 1.47 s, sys: 222 ms, total: 1.69 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%autosave 60\n",
    "\n",
    "import os\n",
    "os.environ['XLA_USE_BF16'] = \"1\"\n",
    "os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from googletrans import Translator\n",
    "from dask import bag, diagnostics\n",
    "\n",
    "import transformers\n",
    "from transformers import (AdamW, \n",
    "                          XLMRobertaTokenizer, \n",
    "                          XLMRobertaModel, \n",
    "                          get_cosine_schedule_with_warmup)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.utils.serialization as xser\n",
    "import torch_xla.version as xv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('PYTORCH:', xv.__torch_gitrev__)\n",
    "print('XLA:', xv.__xla_gitrev__)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\n",
    "test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\n",
    "sample_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Text Augmentation\n",
    "\n",
    "##### References - [JohnM's - Agmenting Data with Translations Kernel](https://www.kaggle.com/jpmiller/augmenting-data-with-translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 58.2s\n",
      "[########################################] | 100% Completed |  2min 57.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17370, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(words):\n",
    "    translator = Translator()\n",
    "    decoded = translator.translate(words, dest='en').text\n",
    "    return decoded\n",
    "\n",
    "other_langs = train.loc[train.lang_abv != \"en\"].copy()\n",
    "\n",
    "#TODO: use a dask dataframe instead of bags\n",
    "premise_bag = bag.from_sequence(other_langs.premise.tolist()).map(translate)\n",
    "hypo_bag =  bag.from_sequence(other_langs.hypothesis.tolist()).map(translate)\n",
    "with diagnostics.ProgressBar():\n",
    "    premises = premise_bag.compute()\n",
    "    hypos = hypo_bag.compute()\n",
    "    \n",
    "    \n",
    "other_langs[['premise', 'hypothesis']] = list(zip(premises, hypos))\n",
    "train = train.append(other_langs)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Dataset Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, df, encoded):\n",
    "        self.df = df\n",
    "        self.encoded = encoded\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):   \n",
    "        ids = self.encoded['input_ids'][index]\n",
    "        mask = self.encoded['attention_mask'][index]\n",
    "        targets = self.df.iloc[index].label\n",
    "        return {\n",
    "            'ids':torch.tensor(ids),\n",
    "            'mask':torch.tensor(mask),\n",
    "            'targets':targets\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRoberta(nn.Module):\n",
    "    def __init__(self, num_labels, multisample):\n",
    "        super(XLMRoberta, self).__init__()\n",
    "        output_hidden_states = False\n",
    "        self.num_labels = num_labels\n",
    "        self.multisample= multisample\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", \n",
    "                                                       output_hidden_states=output_hidden_states, \n",
    "                                                       num_labels=1)\n",
    "        self.layer_norm = nn.LayerNorm(1024*2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)        \n",
    "        self.classifier = nn.Linear(1024*2, self.num_labels)\n",
    "    \n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None):\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask,\n",
    "                               inputs_embeds=inputs_embeds)\n",
    "        average_pool = torch.mean(outputs[0], 1)\n",
    "        max_pool, _ = torch.max(outputs[0], 1)\n",
    "        concatenate_layer = torch.cat((average_pool, max_pool), 1)\n",
    "        normalization = self.layer_norm(concatenate_layer)\n",
    "        if self.multisample:\n",
    "            # Multisample Dropout\n",
    "            logits = torch.mean(\n",
    "                torch.stack(\n",
    "                    [self.classifier(self.dropout(normalization)) for _ in range(5)],\n",
    "                    dim=0,\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            logits = self.dropout(normalization)\n",
    "            logits = self.classifier(logits)       \n",
    "        outputs = F.log_softmax(logits, dim=1)\n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Metrics Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Optimizer Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(model):\n",
    "    # Differential Learning Rate\n",
    "    def is_backbone(name):\n",
    "        return \"roberta\" in name\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "       {'params': [param for name, param in model.named_parameters() if is_backbone(name)], 'lr': LR},\n",
    "       {'params': [param for name, param in model.named_parameters() if not is_backbone(name)], 'lr': 1e-3} \n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n",
    "    )\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Loss Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return nn.NLLLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n",
    "    # Train\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1],\n",
    "        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n",
    "    )\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data_time.update(time.time()-end)\n",
    "        ids = data[\"ids\"]\n",
    "        mask = data[\"mask\"]\n",
    "        targets = data[\"targets\"]\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids = ids,\n",
    "            attention_mask = mask\n",
    "        )\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        xm.optimizer_step(optimizer)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        acc1= accuracy(outputs, targets, topk=(1,))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        top1.update(acc1[0].item(), ids.size(0))\n",
    "        scheduler.step()\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % 30 == 0:\n",
    "            progress.display(i)\n",
    "    del loss\n",
    "    del outputs\n",
    "    del ids\n",
    "    del mask\n",
    "    del targets\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop_fn(validation_loader, model, device):\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    learning_rate = AverageMeter('LR',':2.8f')\n",
    "    progress = ProgressMeter(\n",
    "        len(validation_loader),\n",
    "        [batch_time, losses, top1],\n",
    "        prefix='[xla:{}]Validation: '.format(xm.get_ordinal()))\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            ids = data[\"ids\"]\n",
    "            mask = data[\"mask\"]\n",
    "            targets = data[\"targets\"]\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            outputs = model(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask\n",
    "            )\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            acc1= accuracy(outputs, targets, topk=(1,))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            top1.update(acc1[0].item(), ids.size(0))\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "    del loss\n",
    "    del outputs\n",
    "    del ids\n",
    "    del mask\n",
    "    del targets\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb0db4a191b47348356e15e1bbbc561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b8024340be4227ab9494532484fb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2244861551.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92586c4c595e43be98f6510f04e7e251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "MAX_LEN = 96\n",
    "# Scale learning rate to 8 TPU's\n",
    "LR = 2e-5 * xm.xrt_world_size() \n",
    "METRICS_DEBUG = True\n",
    "\n",
    "\n",
    "WRAPPED_MODEL = xmp.MpModelWrapper(XLMRoberta(num_labels=3, multisample=False))\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "mask = np.random.rand(len(train)) < 0.95\n",
    "train_df = train[mask]\n",
    "valid_df = train[~mask]\n",
    "\n",
    "train_text = train_df[['premise', 'hypothesis']].values.tolist()\n",
    "train_encoded = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "\n",
    "valid_text = valid_df[['premise', 'hypothesis']].values.tolist()\n",
    "valid_encoded = tokenizer.batch_encode_plus(\n",
    "    valid_text,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "\n",
    "train_dataset = DatasetRetriever(df=train_df, encoded=train_encoded)\n",
    "valid_dataset = DatasetRetriever(df=valid_df, encoded=valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run():\n",
    "    xm.master_print('Starting Run ...')\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    xm.master_print('Train Loader Created.')\n",
    "    \n",
    "    valid_sampler = DistributedSampler(\n",
    "        valid_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        sampler=valid_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    xm.master_print('Valid Loader Created.')\n",
    "    \n",
    "    num_train_steps = int(len(train_df) / TRAIN_BATCH_SIZE / xm.xrt_world_size())\n",
    "    device = xm.xla_device()\n",
    "    model = WRAPPED_MODEL.to(device)\n",
    "    xm.master_print('Done Model Loading.')\n",
    "    optimizer = get_model_optimizer(model)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = num_train_steps * EPOCHS\n",
    "    )\n",
    "    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
    "        xm.master_print('Parallel Loader Created. Training ...')\n",
    "        train_loop_fn(para_loader.per_device_loader(device),\n",
    "                      model,  \n",
    "                      optimizer, \n",
    "                      device, \n",
    "                      scheduler, \n",
    "                      epoch\n",
    "                     )\n",
    "        \n",
    "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "            \n",
    "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
    "        xm.master_print('Parallel Loader Created. Validating ...')\n",
    "        eval_loop_fn(para_loader.per_device_loader(device), \n",
    "                     model,  \n",
    "                     device\n",
    "                    )\n",
    "        \n",
    "        # Serialized and Memory Reduced Model Saving\n",
    "        if epoch == EPOCHS-1:\n",
    "            xm.master_print('Saving Model ..')\n",
    "            xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n",
    "            xm.master_print('Model Saved.')\n",
    "            \n",
    "    if METRICS_DEBUG:\n",
    "      xm.master_print(met.metrics_report(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run ...\n",
      "Train Loader Created.\n",
      "Valid Loader Created.\n",
      "Done Model Loading.\n",
      "Num Train Steps= 128, XRT World Size= 8.\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [0][  0/128]\tTime 24.179 (24.179)\tData  0.091 ( 0.091)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [0][  0/128]\tTime 12.723 (12.723)\tData  0.088 ( 0.088)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [0][  0/128]\tTime 18.487 (18.487)\tData  0.094 ( 0.094)\tLoss 1.0938e+00 (1.0938e+00)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:2]Train:  Epoch: [0][  0/128]\tTime  6.819 ( 6.819)\tData  0.088 ( 0.088)\tLoss 1.3906e+00 (1.3906e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:6]Train:  Epoch: [0][  0/128]\tTime 29.978 (29.978)\tData  0.087 ( 0.087)\tLoss 1.1719e+00 (1.1719e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:1]Train:  Epoch: [0][  0/128]\tTime 35.434 (35.434)\tData  0.099 ( 0.099)\tLoss 1.1328e+00 (1.1328e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Train:  Epoch: [0][  0/128]\tTime 41.361 (41.361)\tData  0.542 ( 0.542)\tLoss 1.4766e+00 (1.4766e+00)\tAcc@1  12.50 ( 12.50)\n",
      "[xla:5]Train:  Epoch: [0][  0/128]\tTime  1.040 ( 1.040)\tData  0.079 ( 0.079)\tLoss 1.3359e+00 (1.3359e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:6]Train:  Epoch: [0][ 30/128]\tTime  0.692 ( 6.426)\tData  0.047 ( 4.767)\tLoss 1.1875e+00 (1.1547e+00)\tAcc@1  25.00 ( 34.89)\n",
      "[xla:2]Train:  Epoch: [0][ 30/128]\tTime  0.720 ( 5.684)\tData  0.064 ( 4.767)\tLoss 1.1484e+00 (1.1447e+00)\tAcc@1  37.50 ( 36.09)\n",
      "[xla:0]Train:  Epoch: [0][ 30/128]\tTime  1.018 ( 6.797)\tData  0.113 ( 4.775)\tLoss 1.1406e+00 (1.1701e+00)\tAcc@1  37.50 ( 31.66)\n",
      "[xla:5]Train:  Epoch: [0][ 30/128]\tTime  0.743 ( 5.492)\tData  0.046 ( 4.761)\tLoss 1.1797e+00 (1.1471e+00)\tAcc@1  43.75 ( 36.69)\n",
      "[xla:7]Train:  Epoch: [0][ 30/128]\tTime  0.766 ( 5.876)\tData  0.071 ( 4.772)\tLoss 1.0781e+00 (1.1554e+00)\tAcc@1  43.75 ( 34.68)\n",
      "[xla:4]Train:  Epoch: [0][ 30/128]\tTime  0.801 ( 6.246)\tData  0.048 ( 4.766)\tLoss 1.1094e+00 (1.1612e+00)\tAcc@1  43.75 ( 31.85)\n",
      "[xla:3]Train:  Epoch: [0][ 30/128]\tTime  0.829 ( 6.063)\tData  0.050 ( 4.765)\tLoss 1.0859e+00 (1.1677e+00)\tAcc@1  56.25 ( 33.67)\n",
      "[xla:1]Train:  Epoch: [0][ 30/128]\tTime  0.841 ( 6.608)\tData  0.039 ( 4.765)\tLoss 1.1250e+00 (1.1680e+00)\tAcc@1  31.25 ( 34.07)\n",
      "[xla:5]Train:  Epoch: [0][ 60/128]\tTime  0.595 ( 3.180)\tData  0.045 ( 2.450)\tLoss 1.0469e+00 (1.1287e+00)\tAcc@1  43.75 ( 35.86)\n",
      "[xla:6]Train:  Epoch: [0][ 60/128]\tTime  0.844 ( 3.662)\tData  0.057 ( 2.455)\tLoss 1.0703e+00 (1.1387e+00)\tAcc@1  43.75 ( 35.15)\n",
      "[xla:2]Train:  Epoch: [0][ 60/128]\tTime  0.754 ( 3.283)\tData  0.061 ( 2.454)\tLoss 1.1172e+00 (1.1386e+00)\tAcc@1  43.75 ( 35.14)\n",
      "[xla:3]Train:  Epoch: [0][ 60/128]\tTime  0.791 ( 3.474)\tData  0.048 ( 2.451)\tLoss 1.0625e+00 (1.1467e+00)\tAcc@1  31.25 ( 33.40)\n",
      "[xla:4]Train:  Epoch: [0][ 60/128]\tTime  0.823 ( 3.568)\tData  0.044 ( 2.453)\tLoss 1.0703e+00 (1.1388e+00)\tAcc@1  50.00 ( 34.43)\n",
      "[xla:1]Train:  Epoch: [0][ 60/128]\tTime  0.854 ( 3.752)\tData  0.045 ( 2.454)\tLoss 1.1562e+00 (1.1435e+00)\tAcc@1  31.25 ( 34.53)\n",
      "[xla:7]Train:  Epoch: [0][ 60/128]\tTime  0.795 ( 3.380)\tData  0.049 ( 2.456)\tLoss 1.2578e+00 (1.1416e+00)\tAcc@1  31.25 ( 34.02)\n",
      "[xla:0]Train:  Epoch: [0][ 60/128]\tTime  0.795 ( 3.849)\tData  0.054 ( 2.456)\tLoss 1.1641e+00 (1.1438e+00)\tAcc@1  31.25 ( 33.41)\n",
      "[xla:1]Train:  Epoch: [0][ 90/128]\tTime  0.523 ( 2.774)\tData  0.058 ( 1.665)\tLoss 1.2188e+00 (1.1279e+00)\tAcc@1  18.75 ( 36.47)\n",
      "[xla:5]Train:  Epoch: [0][ 90/128]\tTime  0.626 ( 2.394)\tData  0.036 ( 1.663)\tLoss 1.2656e+00 (1.1235e+00)\tAcc@1  31.25 ( 36.26)\n",
      "[xla:6]Train:  Epoch: [0][ 90/128]\tTime  0.548 ( 2.714)\tData  0.069 ( 1.665)\tLoss 1.1797e+00 (1.1266e+00)\tAcc@1  50.00 ( 35.79)\n",
      "[xla:0]Train:  Epoch: [0][ 90/128]\tTime  0.666 ( 2.840)\tData  0.049 ( 1.666)\tLoss 1.2578e+00 (1.1357e+00)\tAcc@1  31.25 ( 34.07)\n",
      "[xla:4]Train:  Epoch: [0][ 90/128]\tTime  0.729 ( 2.652)\tData  0.075 ( 1.664)\tLoss 1.2734e+00 (1.1279e+00)\tAcc@1  18.75 ( 34.48)\n",
      "[xla:3]Train:  Epoch: [0][ 90/128]\tTime  0.771 ( 2.590)\tData  0.048 ( 1.663)\tLoss 1.0781e+00 (1.1295e+00)\tAcc@1  31.25 ( 34.69)\n",
      "[xla:7]Train:  Epoch: [0][ 90/128]\tTime  0.982 ( 2.527)\tData  0.071 ( 1.664)\tLoss 1.1172e+00 (1.1309e+00)\tAcc@1  31.25 ( 34.48)\n",
      "[xla:2]Train:  Epoch: [0][ 90/128]\tTime  1.075 ( 2.462)\tData  0.097 ( 1.666)\tLoss 1.1172e+00 (1.1294e+00)\tAcc@1  31.25 ( 35.92)\n",
      "[xla:7]Train:  Epoch: [0][120/128]\tTime  0.612 ( 2.091)\tData  0.038 ( 1.266)\tLoss 1.1094e+00 (1.1252e+00)\tAcc@1  31.25 ( 34.66)\n",
      "[xla:4]Train:  Epoch: [0][120/128]\tTime  0.664 ( 2.185)\tData  0.047 ( 1.266)\tLoss 1.1328e+00 (1.1257e+00)\tAcc@1  25.00 ( 34.61)\n",
      "[xla:6]Train:  Epoch: [0][120/128]\tTime  0.874 ( 2.235)\tData  0.078 ( 1.268)\tLoss 1.0000e+00 (1.1205e+00)\tAcc@1  56.25 ( 36.42)\n",
      "[xla:0]Train:  Epoch: [0][120/128]\tTime  0.750 ( 2.329)\tData  0.100 ( 1.269)\tLoss 1.0391e+00 (1.1342e+00)\tAcc@1  37.50 ( 33.89)\n",
      "[xla:5]Train:  Epoch: [0][120/128]\tTime  0.749 ( 1.994)\tData  0.082 ( 1.266)\tLoss 1.1172e+00 (1.1249e+00)\tAcc@1  37.50 ( 35.07)\n",
      "[xla:2]Train:  Epoch: [0][120/128]\tTime  0.767 ( 2.044)\tData  0.046 ( 1.268)\tLoss 1.0547e+00 (1.1215e+00)\tAcc@1  37.50 ( 36.37)\n",
      "[xla:1]Train:  Epoch: [0][120/128]\tTime  0.737 ( 2.280)\tData  0.050 ( 1.269)\tLoss 1.1641e+00 (1.1236e+00)\tAcc@1  25.00 ( 36.52)\n",
      "[xla:3]Train:  Epoch: [0][120/128]\tTime  0.800 ( 2.140)\tData  0.093 ( 1.266)\tLoss 1.1641e+00 (1.1271e+00)\tAcc@1  18.75 ( 34.40)\n",
      "Finished training epoch 0\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:5]Validation: [0/7]\tTime 17.765 (17.765)\tLoss 1.0781e+00 (1.0781e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:6]Validation: [0/7]\tTime 17.471 (17.471)\tLoss 1.0703e+00 (1.0703e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:3]Validation: [0/7]\tTime 17.591 (17.591)\tLoss 1.0938e+00 (1.0938e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:7]Validation: [0/7]\tTime 17.451 (17.451)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  31.25 ( 31.25)\n",
      "[xla:2]Validation: [0/7]\tTime 17.399 (17.399)\tLoss 1.0781e+00 (1.0781e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/7]\tTime 17.452 (17.452)\tLoss 1.1016e+00 (1.1016e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:1]Validation: [0/7]\tTime 17.833 (17.833)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:0]Validation: [0/7]\tTime 17.296 (17.296)\tLoss 1.0703e+00 (1.0703e+00)\tAcc@1  43.75 ( 43.75)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [1][  0/128]\tTime  0.685 ( 0.685)\tData  0.050 ( 0.050)\tLoss 1.0156e+00 (1.0156e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:3]Train:  Epoch: [1][  0/128]\tTime  0.724 ( 0.724)\tData  0.081 ( 0.081)\tLoss 1.0781e+00 (1.0781e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:7]Train:  Epoch: [1][  0/128]\tTime  0.721 ( 0.721)\tData  0.068 ( 0.068)\tLoss 1.1016e+00 (1.1016e+00)\tAcc@1  25.00 ( 25.00)\n",
      "[xla:1]Train:  Epoch: [1][  0/128]\tTime  0.693 ( 0.693)\tData  0.049 ( 0.049)\tLoss 1.1406e+00 (1.1406e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:6]Train:  Epoch: [1][  0/128]\tTime  0.622 ( 0.622)\tData  0.112 ( 0.112)\tLoss 1.1016e+00 (1.1016e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:5]Train:  Epoch: [1][  0/128]\tTime  0.673 ( 0.673)\tData  0.070 ( 0.070)\tLoss 9.7266e-01 (9.7266e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [1][  0/128]\tTime  0.495 ( 0.495)\tData  0.045 ( 0.045)\tLoss 1.0859e+00 (1.0859e+00)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Train:  Epoch: [1][  0/128]\tTime  0.674 ( 0.674)\tData  0.109 ( 0.109)\tLoss 1.1953e+00 (1.1953e+00)\tAcc@1  25.00 ( 25.00)\n",
      "[xla:6]Train:  Epoch: [1][ 30/128]\tTime  0.496 ( 0.755)\tData  0.044 ( 0.060)\tLoss 1.0312e+00 (1.1227e+00)\tAcc@1  43.75 ( 31.65)\n",
      "[xla:7]Train:  Epoch: [1][ 30/128]\tTime  0.575 ( 0.765)\tData  0.034 ( 0.061)\tLoss 1.0703e+00 (1.1109e+00)\tAcc@1  50.00 ( 37.70)\n",
      "[xla:4]Train:  Epoch: [1][ 30/128]\tTime  0.581 ( 0.756)\tData  0.041 ( 0.057)\tLoss 1.1484e+00 (1.1125e+00)\tAcc@1  25.00 ( 35.28)\n",
      "[xla:2]Train:  Epoch: [1][ 30/128]\tTime  0.890 ( 0.773)\tData  0.048 ( 0.065)\tLoss 1.0859e+00 (1.0819e+00)\tAcc@1  43.75 ( 39.92)\n",
      "[xla:5]Train:  Epoch: [1][ 30/128]\tTime  0.678 ( 0.761)\tData  0.043 ( 0.059)\tLoss 1.1250e+00 (1.1164e+00)\tAcc@1  50.00 ( 36.49)\n",
      "[xla:0]Train:  Epoch: [1][ 30/128]\tTime  0.844 ( 0.754)\tData  0.059 ( 0.054)\tLoss 1.0469e+00 (1.1042e+00)\tAcc@1  50.00 ( 37.71)\n",
      "[xla:3]Train:  Epoch: [1][ 30/128]\tTime  0.870 ( 0.772)\tData  0.060 ( 0.058)\tLoss 9.9219e-01 (1.1101e+00)\tAcc@1  56.25 ( 37.90)\n",
      "[xla:1]Train:  Epoch: [1][ 30/128]\tTime  0.771 ( 0.766)\tData  0.040 ( 0.057)\tLoss 1.1406e+00 (1.1024e+00)\tAcc@1  37.50 ( 36.49)\n",
      "[xla:7]Train:  Epoch: [1][ 60/128]\tTime  0.647 ( 0.773)\tData  0.057 ( 0.060)\tLoss 1.2969e+00 (1.1208e+00)\tAcc@1  31.25 ( 35.45)\n",
      "[xla:0]Train:  Epoch: [1][ 60/128]\tTime  0.504 ( 0.765)\tData  0.053 ( 0.055)\tLoss 1.0078e+00 (1.0940e+00)\tAcc@1  37.50 ( 37.71)\n",
      "[xla:2]Train:  Epoch: [1][ 60/128]\tTime  0.721 ( 0.777)\tData  0.048 ( 0.062)\tLoss 1.0547e+00 (1.0935e+00)\tAcc@1  50.00 ( 37.81)\n",
      "[xla:6]Train:  Epoch: [1][ 60/128]\tTime  0.766 ( 0.771)\tData  0.057 ( 0.062)\tLoss 1.0781e+00 (1.1119e+00)\tAcc@1  37.50 ( 33.40)\n",
      "[xla:4]Train:  Epoch: [1][ 60/128]\tTime  0.725 ( 0.771)\tData  0.050 ( 0.060)\tLoss 1.1172e+00 (1.1081e+00)\tAcc@1  37.50 ( 36.58)\n",
      "[xla:1]Train:  Epoch: [1][ 60/128]\tTime  0.739 ( 0.774)\tData  0.047 ( 0.060)\tLoss 1.1875e+00 (1.0989e+00)\tAcc@1  25.00 ( 37.09)\n",
      "[xla:3]Train:  Epoch: [1][ 60/128]\tTime  0.761 ( 0.777)\tData  0.062 ( 0.059)\tLoss 1.0938e+00 (1.1076e+00)\tAcc@1  31.25 ( 37.91)\n",
      "[xla:5]Train:  Epoch: [1][ 60/128]\tTime  0.814 ( 0.773)\tData  0.063 ( 0.059)\tLoss 9.7266e-01 (1.1040e+00)\tAcc@1  43.75 ( 38.52)\n",
      "[xla:0]Train:  Epoch: [1][ 90/128]\tTime  0.531 ( 0.766)\tData  0.060 ( 0.057)\tLoss 1.2188e+00 (1.1034e+00)\tAcc@1  31.25 ( 37.57)\n",
      "[xla:4]Train:  Epoch: [1][ 90/128]\tTime  0.685 ( 0.768)\tData  0.047 ( 0.059)\tLoss 1.3828e+00 (1.1058e+00)\tAcc@1  12.50 ( 37.50)\n",
      "[xla:2]Train:  Epoch: [1][ 90/128]\tTime  0.649 ( 0.774)\tData  0.041 ( 0.062)\tLoss 1.0938e+00 (1.0892e+00)\tAcc@1  37.50 ( 38.53)\n",
      "[xla:7]Train:  Epoch: [1][ 90/128]\tTime  0.648 ( 0.773)\tData  0.045 ( 0.059)\tLoss 1.0156e+00 (1.1017e+00)\tAcc@1  50.00 ( 37.91)\n",
      "[xla:1]Train:  Epoch: [1][ 90/128]\tTime  0.687 ( 0.771)\tData  0.052 ( 0.061)\tLoss 1.0859e+00 (1.0849e+00)\tAcc@1  43.75 ( 39.90)\n",
      "[xla:6]Train:  Epoch: [1][ 90/128]\tTime  0.709 ( 0.770)\tData  0.088 ( 0.060)\tLoss 9.8438e-01 (1.0939e+00)\tAcc@1  43.75 ( 36.54)\n",
      "[xla:5]Train:  Epoch: [1][ 90/128]\tTime  0.863 ( 0.771)\tData  0.067 ( 0.060)\tLoss 1.2109e+00 (1.0892e+00)\tAcc@1  25.00 ( 40.32)\n",
      "[xla:3]Train:  Epoch: [1][ 90/128]\tTime  0.851 ( 0.774)\tData  0.047 ( 0.057)\tLoss 9.8047e-01 (1.0946e+00)\tAcc@1  62.50 ( 38.67)\n",
      "[xla:6]Train:  Epoch: [1][120/128]\tTime  0.594 ( 0.770)\tData  0.050 ( 0.060)\tLoss 1.0078e+00 (1.0727e+00)\tAcc@1  50.00 ( 39.62)\n",
      "[xla:3]Train:  Epoch: [1][120/128]\tTime  0.671 ( 0.774)\tData  0.063 ( 0.058)\tLoss 9.4141e-01 (1.0721e+00)\tAcc@1  62.50 ( 41.94)\n",
      "[xla:1]Train:  Epoch: [1][120/128]\tTime  0.725 ( 0.773)\tData  0.057 ( 0.060)\tLoss 1.2266e+00 (1.0784e+00)\tAcc@1  12.50 ( 40.60)\n",
      "[xla:0]Train:  Epoch: [1][120/128]\tTime  0.815 ( 0.770)\tData  0.043 ( 0.059)\tLoss 8.5547e-01 (1.0818e+00)\tAcc@1  69.00 ( 39.99)\n",
      "[xla:7]Train:  Epoch: [1][120/128]\tTime  0.785 ( 0.775)\tData  0.046 ( 0.058)\tLoss 9.9609e-01 (1.0823e+00)\tAcc@1  50.00 ( 39.88)\n",
      "[xla:5]Train:  Epoch: [1][120/128]\tTime  0.785 ( 0.772)\tData  0.048 ( 0.060)\tLoss 9.3750e-01 (1.0802e+00)\tAcc@1  50.00 ( 41.43)\n",
      "[xla:2]Train:  Epoch: [1][120/128]\tTime  0.929 ( 0.776)\tData  0.072 ( 0.061)\tLoss 8.3594e-01 (1.0711e+00)\tAcc@1  69.00 ( 40.56)\n",
      "[xla:4]Train:  Epoch: [1][120/128]\tTime  0.767 ( 0.772)\tData  0.062 ( 0.058)\tLoss 1.1094e+00 (1.0933e+00)\tAcc@1  31.25 ( 39.20)\n",
      "Finished training epoch 1\n",
      "[xla:2]Validation: [0/7]\tTime  0.173 ( 0.173)\tLoss 9.3359e-01 (9.3359e-01)\tAcc@1  56.25 ( 56.25)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/7]\tTime  0.231 ( 0.231)\tLoss 9.8828e-01 (9.8828e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.294 ( 0.294)\tLoss 8.9844e-01 (8.9844e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:5]Validation: [0/7]\tTime  0.194 ( 0.194)\tLoss 9.9219e-01 (9.9219e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.232 ( 0.232)\tLoss 9.1406e-01 (9.1406e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.190 ( 0.190)\tLoss 9.9609e-01 (9.9609e-01)\tAcc@1  50.00 ( 50.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.207 ( 0.207)\tLoss 9.2969e-01 (9.2969e-01)\tAcc@1  56.25 ( 56.25)\n",
      "[xla:3]Validation: [0/7]\tTime  0.184 ( 0.184)\tLoss 9.3750e-01 (9.3750e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [2][  0/128]\tTime  0.601 ( 0.601)\tData  0.044 ( 0.044)\tLoss 1.0391e+00 (1.0391e+00)\tAcc@1  43.75 ( 43.75)\n",
      "[xla:4]Train:  Epoch: [2][  0/128]\tTime  0.629 ( 0.629)\tData  0.045 ( 0.045)\tLoss 7.3047e-01 (7.3047e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Train:  Epoch: [2][  0/128]\tTime  0.607 ( 0.607)\tData  0.068 ( 0.068)\tLoss 9.9219e-01 (9.9219e-01)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:7]Train:  Epoch: [2][  0/128]\tTime  0.740 ( 0.740)\tData  0.084 ( 0.084)\tLoss 8.8672e-01 (8.8672e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Train:  Epoch: [2][  0/128]\tTime  0.619 ( 0.619)\tData  0.038 ( 0.038)\tLoss 1.0078e+00 (1.0078e+00)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:0]Train:  Epoch: [2][  0/128]\tTime  0.588 ( 0.588)\tData  0.112 ( 0.112)\tLoss 1.0234e+00 (1.0234e+00)\tAcc@1  37.50 ( 37.50)\n",
      "[xla:5]Train:  Epoch: [2][  0/128]\tTime  0.574 ( 0.574)\tData  0.080 ( 0.080)\tLoss 7.7344e-01 (7.7344e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Train:  Epoch: [2][  0/128]\tTime  0.432 ( 0.432)\tData  0.062 ( 0.062)\tLoss 7.7344e-01 (7.7344e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Train:  Epoch: [2][ 30/128]\tTime  0.653 ( 0.765)\tData  0.048 ( 0.064)\tLoss 8.7891e-01 (8.0204e-01)\tAcc@1  56.25 ( 62.58)\n",
      "[xla:2]Train:  Epoch: [2][ 30/128]\tTime  0.640 ( 0.780)\tData  0.034 ( 0.060)\tLoss 6.3281e-01 (8.3216e-01)\tAcc@1  69.00 ( 61.32)\n",
      "[xla:3]Train:  Epoch: [2][ 30/128]\tTime  0.668 ( 0.755)\tData  0.040 ( 0.055)\tLoss 4.2188e-01 (8.2038e-01)\tAcc@1  87.50 ( 64.34)\n",
      "[xla:7]Train:  Epoch: [2][ 30/128]\tTime  0.847 ( 0.773)\tData  0.048 ( 0.060)\tLoss 7.1484e-01 (8.2422e-01)\tAcc@1  62.50 ( 64.19)\n",
      "[xla:0]Train:  Epoch: [2][ 30/128]\tTime  0.693 ( 0.765)\tData  0.046 ( 0.063)\tLoss 7.6953e-01 (8.3443e-01)\tAcc@1  62.50 ( 61.12)\n",
      "[xla:1]Train:  Epoch: [2][ 30/128]\tTime  0.727 ( 0.769)\tData  0.072 ( 0.065)\tLoss 6.7578e-01 (8.0418e-01)\tAcc@1  69.00 ( 63.93)\n",
      "[xla:6]Train:  Epoch: [2][ 30/128]\tTime  0.778 ( 0.771)\tData  0.071 ( 0.060)\tLoss 7.9688e-01 (8.5156e-01)\tAcc@1  56.25 ( 61.13)\n",
      "[xla:5]Train:  Epoch: [2][ 30/128]\tTime  0.972 ( 0.768)\tData  0.081 ( 0.059)\tLoss 7.0703e-01 (8.1666e-01)\tAcc@1  69.00 ( 64.55)\n",
      "[xla:5]Train:  Epoch: [2][ 60/128]\tTime  0.652 ( 0.766)\tData  0.040 ( 0.058)\tLoss 4.1797e-01 (7.5887e-01)\tAcc@1  75.00 ( 66.11)\n",
      "[xla:6]Train:  Epoch: [2][ 60/128]\tTime  0.594 ( 0.769)\tData  0.054 ( 0.059)\tLoss 6.1328e-01 (7.7606e-01)\tAcc@1  75.00 ( 64.78)\n",
      "[xla:4]Train:  Epoch: [2][ 60/128]\tTime  0.607 ( 0.770)\tData  0.069 ( 0.061)\tLoss 5.1172e-01 (7.5810e-01)\tAcc@1  81.00 ( 65.63)\n",
      "[xla:7]Train:  Epoch: [2][ 60/128]\tTime  0.676 ( 0.772)\tData  0.051 ( 0.060)\tLoss 7.2656e-01 (8.0799e-01)\tAcc@1  75.00 ( 65.32)\n",
      "[xla:3]Train:  Epoch: [2][ 60/128]\tTime  0.841 ( 0.764)\tData  0.052 ( 0.058)\tLoss 1.0078e+00 (7.7196e-01)\tAcc@1  56.25 ( 65.21)\n",
      "[xla:1]Train:  Epoch: [2][ 60/128]\tTime  0.745 ( 0.770)\tData  0.047 ( 0.057)\tLoss 7.5000e-01 (7.3799e-01)\tAcc@1  56.25 ( 67.92)\n",
      "[xla:0]Train:  Epoch: [2][ 60/128]\tTime  0.760 ( 0.769)\tData  0.048 ( 0.060)\tLoss 8.2422e-01 (7.8039e-01)\tAcc@1  62.50 ( 64.70)\n",
      "[xla:2]Train:  Epoch: [2][ 60/128]\tTime  0.741 ( 0.779)\tData  0.046 ( 0.060)\tLoss 4.0625e-01 (7.8797e-01)\tAcc@1  87.50 ( 64.18)\n",
      "[xla:3]Train:  Epoch: [2][ 90/128]\tTime  0.561 ( 0.771)\tData  0.062 ( 0.058)\tLoss 6.3672e-01 (7.3629e-01)\tAcc@1  75.00 ( 67.48)\n",
      "[xla:7]Train:  Epoch: [2][ 90/128]\tTime  0.592 ( 0.777)\tData  0.048 ( 0.061)\tLoss 5.3906e-01 (7.5801e-01)\tAcc@1  81.00 ( 67.69)\n",
      "[xla:2]Train:  Epoch: [2][ 90/128]\tTime  0.687 ( 0.781)\tData  0.058 ( 0.060)\tLoss 5.9766e-01 (7.4622e-01)\tAcc@1  69.00 ( 67.33)\n",
      "[xla:1]Train:  Epoch: [2][ 90/128]\tTime  0.879 ( 0.776)\tData  0.097 ( 0.059)\tLoss 7.7734e-01 (7.0068e-01)\tAcc@1  69.00 ( 69.77)\n",
      "[xla:4]Train:  Epoch: [2][ 90/128]\tTime  0.912 ( 0.777)\tData  0.060 ( 0.060)\tLoss 1.0547e+00 (7.3017e-01)\tAcc@1  50.00 ( 67.63)\n",
      "[xla:5]Train:  Epoch: [2][ 90/128]\tTime  0.766 ( 0.776)\tData  0.052 ( 0.057)\tLoss 9.4922e-01 (7.2100e-01)\tAcc@1  56.25 ( 67.75)\n",
      "[xla:6]Train:  Epoch: [2][ 90/128]\tTime  0.794 ( 0.778)\tData  0.045 ( 0.060)\tLoss 5.0781e-01 (7.4551e-01)\tAcc@1  81.00 ( 67.11)\n",
      "[xla:0]Train:  Epoch: [2][ 90/128]\tTime  0.802 ( 0.776)\tData  0.039 ( 0.063)\tLoss 8.2031e-01 (7.4343e-01)\tAcc@1  56.25 ( 66.84)\n",
      "[xla:5]Train:  Epoch: [2][120/128]\tTime  0.611 ( 0.771)\tData  0.035 ( 0.058)\tLoss 4.5898e-01 (7.0566e-01)\tAcc@1  87.50 ( 68.88)\n",
      "[xla:2]Train:  Epoch: [2][120/128]\tTime  0.529 ( 0.776)\tData  0.048 ( 0.059)\tLoss 3.9453e-01 (7.1977e-01)\tAcc@1  87.50 ( 68.98)\n",
      "[xla:1]Train:  Epoch: [2][120/128]\tTime  0.779 ( 0.774)\tData  0.044 ( 0.059)\tLoss 6.2891e-01 (6.8398e-01)\tAcc@1  75.00 ( 71.03)\n",
      "[xla:4]Train:  Epoch: [2][120/128]\tTime  1.020 ( 0.774)\tData  0.063 ( 0.059)\tLoss 7.9297e-01 (7.0598e-01)\tAcc@1  56.25 ( 69.50)\n",
      "[xla:3]Train:  Epoch: [2][120/128]\tTime  0.783 ( 0.771)\tData  0.074 ( 0.058)\tLoss 4.7656e-01 (7.0185e-01)\tAcc@1  81.00 ( 69.56)\n",
      "[xla:0]Train:  Epoch: [2][120/128]\tTime  0.771 ( 0.773)\tData  0.067 ( 0.063)\tLoss 4.8828e-01 (7.0940e-01)\tAcc@1  87.50 ( 68.61)\n",
      "[xla:6]Train:  Epoch: [2][120/128]\tTime  0.792 ( 0.774)\tData  0.046 ( 0.059)\tLoss 6.7969e-01 (6.9956e-01)\tAcc@1  75.00 ( 69.39)\n",
      "[xla:7]Train:  Epoch: [2][120/128]\tTime  0.984 ( 0.776)\tData  0.071 ( 0.060)\tLoss 7.4609e-01 (7.3555e-01)\tAcc@1  75.00 ( 68.93)\n",
      "[xla:2]Validation: [0/7]\tTime  0.243 ( 0.243)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.218 ( 0.218)\tLoss 6.5625e-01 (6.5625e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 2\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/7]\tTime  0.214 ( 0.214)\tLoss 6.0547e-01 (6.0547e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Validation: [0/7]\tTime  0.317 ( 0.317)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.188 ( 0.188)\tLoss 7.2656e-01 (7.2656e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.206 ( 0.206)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Validation: [0/7]\tTime  0.214 ( 0.214)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.218 ( 0.218)\tLoss 7.3047e-01 (7.3047e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [3][  0/128]\tTime  0.515 ( 0.515)\tData  0.066 ( 0.066)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  62.50 ( 62.50)\n",
      "[xla:6]Train:  Epoch: [3][  0/128]\tTime  0.511 ( 0.511)\tData  0.030 ( 0.030)\tLoss 5.0000e-01 (5.0000e-01)\tAcc@1  81.00 ( 81.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:3]Train:  Epoch: [3][  0/128]\tTime  0.585 ( 0.585)\tData  0.050 ( 0.050)\tLoss 3.1641e-01 (3.1641e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:7]Train:  Epoch: [3][  0/128]\tTime  0.701 ( 0.701)\tData  0.101 ( 0.101)\tLoss 5.7422e-01 (5.7422e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [3][  0/128]\tTime  0.580 ( 0.580)\tData  0.103 ( 0.103)\tLoss 5.3906e-01 (5.3906e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Train:  Epoch: [3][  0/128]\tTime  0.637 ( 0.637)\tData  0.061 ( 0.061)\tLoss 5.4297e-01 (5.4297e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:0]Train:  Epoch: [3][  0/128]\tTime  0.437 ( 0.437)\tData  0.006 ( 0.006)\tLoss 6.6016e-01 (6.6016e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [3][  0/128]\tTime  0.454 ( 0.454)\tData  0.043 ( 0.043)\tLoss 4.6875e-01 (4.6875e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Train:  Epoch: [3][ 30/128]\tTime  0.643 ( 0.777)\tData  0.035 ( 0.058)\tLoss 4.6094e-01 (5.2287e-01)\tAcc@1  87.50 ( 80.44)\n",
      "[xla:5]Train:  Epoch: [3][ 30/128]\tTime  0.655 ( 0.768)\tData  0.049 ( 0.059)\tLoss 4.1602e-01 (5.4650e-01)\tAcc@1  81.00 ( 77.44)\n",
      "[xla:4]Train:  Epoch: [3][ 30/128]\tTime  0.726 ( 0.780)\tData  0.040 ( 0.057)\tLoss 6.9141e-01 (5.6836e-01)\tAcc@1  69.00 ( 76.18)\n",
      "[xla:7]Train:  Epoch: [3][ 30/128]\tTime  0.843 ( 0.783)\tData  0.062 ( 0.062)\tLoss 4.7656e-01 (5.5913e-01)\tAcc@1  75.00 ( 75.23)\n",
      "[xla:2]Train:  Epoch: [3][ 30/128]\tTime  0.855 ( 0.797)\tData  0.051 ( 0.061)\tLoss 5.8984e-01 (5.8351e-01)\tAcc@1  75.00 ( 79.06)\n",
      "[xla:0]Train:  Epoch: [3][ 30/128]\tTime  0.895 ( 0.773)\tData  0.057 ( 0.059)\tLoss 5.1562e-01 (6.0692e-01)\tAcc@1  81.00 ( 77.87)\n",
      "[xla:1]Train:  Epoch: [3][ 30/128]\tTime  0.764 ( 0.781)\tData  0.049 ( 0.063)\tLoss 2.8711e-01 (5.2224e-01)\tAcc@1  94.00 ( 79.39)\n",
      "[xla:6]Train:  Epoch: [3][ 30/128]\tTime  0.783 ( 0.795)\tData  0.064 ( 0.060)\tLoss 3.1641e-01 (5.5336e-01)\tAcc@1  94.00 ( 77.40)\n",
      "[xla:3]Train:  Epoch: [3][ 60/128]\tTime  0.537 ( 0.771)\tData  0.067 ( 0.059)\tLoss 8.1250e-01 (5.2795e-01)\tAcc@1  56.25 ( 80.00)\n",
      "[xla:5]Train:  Epoch: [3][ 60/128]\tTime  0.693 ( 0.766)\tData  0.042 ( 0.059)\tLoss 2.3047e-01 (5.0711e-01)\tAcc@1  94.00 ( 78.09)\n",
      "[xla:4]Train:  Epoch: [3][ 60/128]\tTime  0.701 ( 0.773)\tData  0.046 ( 0.058)\tLoss 3.2617e-01 (5.4824e-01)\tAcc@1  94.00 ( 77.55)\n",
      "[xla:6]Train:  Epoch: [3][ 60/128]\tTime  0.750 ( 0.780)\tData  0.067 ( 0.058)\tLoss 5.9375e-01 (5.4569e-01)\tAcc@1  81.00 ( 78.38)\n",
      "[xla:0]Train:  Epoch: [3][ 60/128]\tTime  0.794 ( 0.769)\tData  0.041 ( 0.057)\tLoss 8.0078e-01 (5.8437e-01)\tAcc@1  75.00 ( 77.56)\n",
      "[xla:1]Train:  Epoch: [3][ 60/128]\tTime  0.818 ( 0.774)\tData  0.061 ( 0.062)\tLoss 8.1250e-01 (5.1604e-01)\tAcc@1  62.50 ( 79.80)\n",
      "[xla:7]Train:  Epoch: [3][ 60/128]\tTime  0.783 ( 0.777)\tData  0.065 ( 0.059)\tLoss 3.8867e-01 (5.5032e-01)\tAcc@1  87.50 ( 76.73)\n",
      "[xla:2]Train:  Epoch: [3][ 60/128]\tTime  0.790 ( 0.783)\tData  0.061 ( 0.060)\tLoss 3.5547e-01 (5.7254e-01)\tAcc@1  87.50 ( 79.20)\n",
      "[xla:7]Train:  Epoch: [3][ 90/128]\tTime  0.595 ( 0.771)\tData  0.036 ( 0.059)\tLoss 4.9805e-01 (5.4554e-01)\tAcc@1  87.50 ( 77.65)\n",
      "[xla:4]Train:  Epoch: [3][ 90/128]\tTime  0.547 ( 0.770)\tData  0.042 ( 0.058)\tLoss 7.3828e-01 (5.5736e-01)\tAcc@1  62.50 ( 77.26)\n",
      "[xla:2]Train:  Epoch: [3][ 90/128]\tTime  0.878 ( 0.776)\tData  0.090 ( 0.059)\tLoss 5.5469e-01 (5.5332e-01)\tAcc@1  81.00 ( 79.38)\n",
      "[xla:5]Train:  Epoch: [3][ 90/128]\tTime  0.696 ( 0.768)\tData  0.055 ( 0.059)\tLoss 5.2344e-01 (5.0011e-01)\tAcc@1  87.50 ( 79.05)\n",
      "[xla:1]Train:  Epoch: [3][ 90/128]\tTime  0.914 ( 0.772)\tData  0.079 ( 0.062)\tLoss 5.9375e-01 (5.0562e-01)\tAcc@1  81.00 ( 80.61)\n",
      "[xla:3]Train:  Epoch: [3][ 90/128]\tTime  0.777 ( 0.772)\tData  0.047 ( 0.058)\tLoss 4.3164e-01 (5.2807e-01)\tAcc@1  75.00 ( 79.60)\n",
      "[xla:0]Train:  Epoch: [3][ 90/128]\tTime  0.935 ( 0.769)\tData  0.047 ( 0.059)\tLoss 5.6641e-01 (5.7383e-01)\tAcc@1  69.00 ( 77.93)\n",
      "[xla:6]Train:  Epoch: [3][ 90/128]\tTime  0.799 ( 0.777)\tData  0.046 ( 0.059)\tLoss 2.4609e-01 (5.4662e-01)\tAcc@1  94.00 ( 78.23)\n",
      "[xla:1]Train:  Epoch: [3][120/128]\tTime  0.503 ( 0.769)\tData  0.054 ( 0.059)\tLoss 6.2891e-01 (5.1997e-01)\tAcc@1  75.00 ( 80.46)\n",
      "[xla:2]Train:  Epoch: [3][120/128]\tTime  0.575 ( 0.773)\tData  0.062 ( 0.058)\tLoss 4.1406e-01 (5.5537e-01)\tAcc@1  87.50 ( 78.90)\n",
      "[xla:7]Train:  Epoch: [3][120/128]\tTime  0.696 ( 0.770)\tData  0.044 ( 0.059)\tLoss 4.5703e-01 (5.5403e-01)\tAcc@1  81.00 ( 77.97)\n",
      "[xla:3]Train:  Epoch: [3][120/128]\tTime  0.716 ( 0.770)\tData  0.044 ( 0.057)\tLoss 3.7500e-01 (5.3547e-01)\tAcc@1  94.00 ( 78.77)\n",
      "[xla:4]Train:  Epoch: [3][120/128]\tTime  0.790 ( 0.770)\tData  0.059 ( 0.059)\tLoss 7.5391e-01 (5.5857e-01)\tAcc@1  69.00 ( 77.47)\n",
      "[xla:6]Train:  Epoch: [3][120/128]\tTime  0.926 ( 0.774)\tData  0.053 ( 0.058)\tLoss 6.3281e-01 (5.3411e-01)\tAcc@1  81.00 ( 79.39)\n",
      "[xla:0]Train:  Epoch: [3][120/128]\tTime  0.713 ( 0.768)\tData  0.045 ( 0.058)\tLoss 3.8281e-01 (5.6241e-01)\tAcc@1  94.00 ( 78.35)\n",
      "[xla:5]Train:  Epoch: [3][120/128]\tTime  0.759 ( 0.768)\tData  0.046 ( 0.058)\tLoss 4.1602e-01 (5.1742e-01)\tAcc@1  81.00 ( 78.48)\n",
      "[xla:3]Validation: [0/7]\tTime  0.266 ( 0.266)\tLoss 5.5469e-01 (5.5469e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.336 ( 0.336)\tLoss 7.1094e-01 (7.1094e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 3\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:5]Validation: [0/7]\tTime  0.227 ( 0.227)\tLoss 7.3047e-01 (7.3047e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.231 ( 0.231)\tLoss 6.1719e-01 (6.1719e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.227 ( 0.227)\tLoss 5.3125e-01 (5.3125e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.211 ( 0.211)\tLoss 5.9766e-01 (5.9766e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.227 ( 0.227)\tLoss 5.2344e-01 (5.2344e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.232 ( 0.232)\tLoss 5.1562e-01 (5.1562e-01)\tAcc@1  87.50 ( 87.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [4][  0/128]\tTime  0.528 ( 0.528)\tData  0.033 ( 0.033)\tLoss 3.9258e-01 (3.9258e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Train:  Epoch: [4][  0/128]\tTime  0.628 ( 0.628)\tData  0.067 ( 0.067)\tLoss 3.9844e-01 (3.9844e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Train:  Epoch: [4][  0/128]\tTime  0.670 ( 0.670)\tData  0.107 ( 0.107)\tLoss 5.2734e-01 (5.2734e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Train:  Epoch: [4][  0/128]\tTime  0.604 ( 0.604)\tData  0.075 ( 0.075)\tLoss 5.0391e-01 (5.0391e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [4][  0/128]\tTime  0.600 ( 0.600)\tData  0.071 ( 0.071)\tLoss 6.6797e-01 (6.6797e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Train:  Epoch: [4][  0/128]\tTime  0.479 ( 0.479)\tData  0.067 ( 0.067)\tLoss 6.4453e-01 (6.4453e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [4][  0/128]\tTime  0.459 ( 0.459)\tData  0.036 ( 0.036)\tLoss 4.3555e-01 (4.3555e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [4][  0/128]\tTime  0.418 ( 0.418)\tData  0.046 ( 0.046)\tLoss 3.4961e-01 (3.4961e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:4]Train:  Epoch: [4][ 30/128]\tTime  0.618 ( 0.749)\tData  0.036 ( 0.053)\tLoss 7.3828e-01 (4.9253e-01)\tAcc@1  81.00 ( 83.84)\n",
      "[xla:5]Train:  Epoch: [4][ 30/128]\tTime  0.694 ( 0.771)\tData  0.037 ( 0.060)\tLoss 3.0273e-01 (4.6135e-01)\tAcc@1  94.00 ( 81.90)\n",
      "[xla:7]Train:  Epoch: [4][ 30/128]\tTime  0.695 ( 0.751)\tData  0.076 ( 0.058)\tLoss 4.7852e-01 (4.9836e-01)\tAcc@1  75.00 ( 79.02)\n",
      "[xla:3]Train:  Epoch: [4][ 30/128]\tTime  0.690 ( 0.775)\tData  0.078 ( 0.061)\tLoss 4.4922e-01 (5.0526e-01)\tAcc@1  75.00 ( 79.66)\n",
      "[xla:1]Train:  Epoch: [4][ 30/128]\tTime  1.005 ( 0.773)\tData  0.095 ( 0.064)\tLoss 2.1973e-01 (4.3955e-01)\tAcc@1  94.00 ( 83.06)\n",
      "[xla:0]Train:  Epoch: [4][ 30/128]\tTime  0.917 ( 0.762)\tData  0.055 ( 0.055)\tLoss 4.4922e-01 (5.4029e-01)\tAcc@1  75.00 ( 79.48)\n",
      "[xla:2]Train:  Epoch: [4][ 30/128]\tTime  0.821 ( 0.759)\tData  0.063 ( 0.056)\tLoss 4.1602e-01 (4.6774e-01)\tAcc@1  81.00 ( 81.47)\n",
      "[xla:6]Train:  Epoch: [4][ 30/128]\tTime  0.856 ( 0.765)\tData  0.085 ( 0.059)\tLoss 2.8906e-01 (4.9096e-01)\tAcc@1  87.50 ( 81.66)\n",
      "[xla:5]Train:  Epoch: [4][ 60/128]\tTime  0.507 ( 0.780)\tData  0.053 ( 0.059)\tLoss 2.8711e-01 (4.4752e-01)\tAcc@1  87.50 ( 82.50)\n",
      "[xla:3]Train:  Epoch: [4][ 60/128]\tTime  0.537 ( 0.782)\tData  0.049 ( 0.059)\tLoss 8.3594e-01 (5.0003e-01)\tAcc@1  75.00 ( 81.16)\n",
      "[xla:7]Train:  Epoch: [4][ 60/128]\tTime  0.726 ( 0.772)\tData  0.046 ( 0.058)\tLoss 4.5703e-01 (5.0226e-01)\tAcc@1  75.00 ( 78.98)\n",
      "[xla:0]Train:  Epoch: [4][ 60/128]\tTime  0.711 ( 0.776)\tData  0.064 ( 0.058)\tLoss 7.5391e-01 (5.1460e-01)\tAcc@1  69.00 ( 80.24)\n",
      "[xla:4]Train:  Epoch: [4][ 60/128]\tTime  0.788 ( 0.774)\tData  0.081 ( 0.059)\tLoss 2.0410e-01 (4.9043e-01)\tAcc@1  94.00 ( 82.16)\n",
      "[xla:2]Train:  Epoch: [4][ 60/128]\tTime  0.807 ( 0.775)\tData  0.051 ( 0.059)\tLoss 2.7930e-01 (4.7426e-01)\tAcc@1  87.50 ( 81.12)\n",
      "[xla:6]Train:  Epoch: [4][ 60/128]\tTime  0.786 ( 0.778)\tData  0.052 ( 0.060)\tLoss 4.5703e-01 (4.7884e-01)\tAcc@1  87.50 ( 81.45)\n",
      "[xla:1]Train:  Epoch: [4][ 60/128]\tTime  0.824 ( 0.783)\tData  0.049 ( 0.062)\tLoss 6.8359e-01 (4.4235e-01)\tAcc@1  81.00 ( 82.87)\n",
      "[xla:2]Train:  Epoch: [4][ 90/128]\tTime  0.493 ( 0.769)\tData  0.045 ( 0.057)\tLoss 5.7031e-01 (4.7874e-01)\tAcc@1  81.00 ( 81.43)\n",
      "[xla:0]Train:  Epoch: [4][ 90/128]\tTime  0.648 ( 0.771)\tData  0.040 ( 0.059)\tLoss 7.6172e-01 (5.1414e-01)\tAcc@1  69.00 ( 79.76)\n",
      "[xla:4]Train:  Epoch: [4][ 90/128]\tTime  0.560 ( 0.770)\tData  0.039 ( 0.059)\tLoss 7.5391e-01 (4.8578e-01)\tAcc@1  69.00 ( 81.95)\n",
      "[xla:1]Train:  Epoch: [4][ 90/128]\tTime  0.770 ( 0.777)\tData  0.051 ( 0.061)\tLoss 5.2344e-01 (4.4226e-01)\tAcc@1  81.00 ( 83.37)\n",
      "[xla:3]Train:  Epoch: [4][ 90/128]\tTime  0.667 ( 0.778)\tData  0.042 ( 0.057)\tLoss 2.3828e-01 (5.0522e-01)\tAcc@1  94.00 ( 80.30)\n",
      "[xla:7]Train:  Epoch: [4][ 90/128]\tTime  0.715 ( 0.770)\tData  0.036 ( 0.058)\tLoss 4.6484e-01 (4.9070e-01)\tAcc@1  87.50 ( 79.86)\n",
      "[xla:6]Train:  Epoch: [4][ 90/128]\tTime  0.861 ( 0.773)\tData  0.046 ( 0.058)\tLoss 2.6953e-01 (4.9038e-01)\tAcc@1  87.50 ( 80.63)\n",
      "[xla:5]Train:  Epoch: [4][ 90/128]\tTime  1.002 ( 0.779)\tData  0.066 ( 0.058)\tLoss 6.1719e-01 (4.5138e-01)\tAcc@1  62.50 ( 81.88)\n",
      "[xla:5]Train:  Epoch: [4][120/128]\tTime  0.581 ( 0.778)\tData  0.046 ( 0.059)\tLoss 3.8867e-01 (4.6986e-01)\tAcc@1  81.00 ( 81.61)\n",
      "[xla:0]Train:  Epoch: [4][120/128]\tTime  0.657 ( 0.774)\tData  0.037 ( 0.059)\tLoss 2.9102e-01 (5.1411e-01)\tAcc@1  87.50 ( 79.50)\n",
      "[xla:2]Train:  Epoch: [4][120/128]\tTime  0.687 ( 0.774)\tData  0.056 ( 0.058)\tLoss 4.6289e-01 (4.8996e-01)\tAcc@1  81.00 ( 80.91)\n",
      "[xla:4]Train:  Epoch: [4][120/128]\tTime  0.701 ( 0.774)\tData  0.065 ( 0.061)\tLoss 8.8281e-01 (4.9163e-01)\tAcc@1  56.25 ( 81.56)\n",
      "[xla:1]Train:  Epoch: [4][120/128]\tTime  0.852 ( 0.779)\tData  0.044 ( 0.060)\tLoss 4.3555e-01 (4.5592e-01)\tAcc@1  81.00 ( 82.62)\n",
      "[xla:6]Train:  Epoch: [4][120/128]\tTime  0.828 ( 0.776)\tData  0.046 ( 0.060)\tLoss 9.1406e-01 (4.7592e-01)\tAcc@1  75.00 ( 81.67)\n",
      "[xla:3]Train:  Epoch: [4][120/128]\tTime  0.813 ( 0.780)\tData  0.063 ( 0.059)\tLoss 3.6328e-01 (5.0563e-01)\tAcc@1  87.50 ( 80.28)\n",
      "[xla:7]Train:  Epoch: [4][120/128]\tTime  0.826 ( 0.774)\tData  0.045 ( 0.058)\tLoss 5.3516e-01 (4.9979e-01)\tAcc@1  87.50 ( 79.79)\n",
      "[xla:3]Validation: [0/7]\tTime  0.164 ( 0.164)\tLoss 5.0000e-01 (5.0000e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Validation: [0/7]\tTime  0.423 ( 0.423)\tLoss 6.9141e-01 (6.9141e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Finished training epoch 4\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:6]Validation: [0/7]\tTime  0.217 ( 0.217)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.230 ( 0.230)\tLoss 7.1875e-01 (7.1875e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.163 ( 0.163)\tLoss 5.0000e-01 (5.0000e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.204 ( 0.204)\tLoss 5.5859e-01 (5.5859e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.227 ( 0.227)\tLoss 4.9414e-01 (4.9414e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Validation: [0/7]\tTime  0.157 ( 0.157)\tLoss 4.4922e-01 (4.4922e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [5][  0/128]\tTime  0.606 ( 0.606)\tData  0.058 ( 0.058)\tLoss 6.1328e-01 (6.1328e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Train:  Epoch: [5][  0/128]\tTime  0.519 ( 0.519)\tData  0.050 ( 0.050)\tLoss 2.8320e-01 (2.8320e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [5][  0/128]\tTime  0.593 ( 0.593)\tData  0.055 ( 0.055)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Train:  Epoch: [5][  0/128]\tTime  0.571 ( 0.571)\tData  0.066 ( 0.066)\tLoss 4.2969e-01 (4.2969e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Train:  Epoch: [5][  0/128]\tTime  0.605 ( 0.605)\tData  0.048 ( 0.048)\tLoss 5.5078e-01 (5.5078e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:5]Train:  Epoch: [5][  0/128]\tTime  0.559 ( 0.559)\tData  0.077 ( 0.077)\tLoss 3.6328e-01 (3.6328e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [5][  0/128]\tTime  0.449 ( 0.449)\tData  0.026 ( 0.026)\tLoss 6.2109e-01 (6.2109e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Train:  Epoch: [5][  0/128]\tTime  0.495 ( 0.495)\tData  0.064 ( 0.064)\tLoss 3.4375e-01 (3.4375e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:0]Train:  Epoch: [5][ 30/128]\tTime  0.597 ( 0.745)\tData  0.055 ( 0.061)\tLoss 4.2383e-01 (4.9707e-01)\tAcc@1  87.50 ( 81.65)\n",
      "[xla:5]Train:  Epoch: [5][ 30/128]\tTime  0.677 ( 0.752)\tData  0.065 ( 0.058)\tLoss 3.3008e-01 (4.0981e-01)\tAcc@1  87.50 ( 82.23)\n",
      "[xla:7]Train:  Epoch: [5][ 30/128]\tTime  0.725 ( 0.755)\tData  0.061 ( 0.059)\tLoss 2.5586e-01 (4.2874e-01)\tAcc@1  87.50 ( 82.10)\n",
      "[xla:3]Train:  Epoch: [5][ 30/128]\tTime  0.730 ( 0.770)\tData  0.074 ( 0.061)\tLoss 3.6719e-01 (4.5895e-01)\tAcc@1  81.00 ( 80.86)\n",
      "[xla:2]Train:  Epoch: [5][ 30/128]\tTime  0.975 ( 0.760)\tData  0.071 ( 0.056)\tLoss 4.5898e-01 (4.1915e-01)\tAcc@1  75.00 ( 84.27)\n",
      "[xla:1]Train:  Epoch: [5][ 30/128]\tTime  0.790 ( 0.777)\tData  0.037 ( 0.060)\tLoss 1.8945e-01 (4.0729e-01)\tAcc@1  87.50 ( 84.42)\n",
      "[xla:4]Train:  Epoch: [5][ 30/128]\tTime  0.746 ( 0.751)\tData  0.064 ( 0.061)\tLoss 4.4141e-01 (4.2216e-01)\tAcc@1  87.50 ( 84.68)\n",
      "[xla:6]Train:  Epoch: [5][ 30/128]\tTime  0.784 ( 0.758)\tData  0.046 ( 0.059)\tLoss 4.4531e-01 (4.4119e-01)\tAcc@1  69.00 ( 81.71)\n",
      "[xla:7]Train:  Epoch: [5][ 60/128]\tTime  0.710 ( 0.763)\tData  0.041 ( 0.057)\tLoss 2.9688e-01 (4.2813e-01)\tAcc@1  87.50 ( 82.74)\n",
      "[xla:0]Train:  Epoch: [5][ 60/128]\tTime  0.686 ( 0.759)\tData  0.063 ( 0.060)\tLoss 6.4453e-01 (4.6553e-01)\tAcc@1  69.00 ( 82.57)\n",
      "[xla:2]Train:  Epoch: [5][ 60/128]\tTime  0.727 ( 0.765)\tData  0.045 ( 0.057)\tLoss 2.8906e-01 (4.4085e-01)\tAcc@1  94.00 ( 82.17)\n",
      "[xla:6]Train:  Epoch: [5][ 60/128]\tTime  0.740 ( 0.764)\tData  0.053 ( 0.057)\tLoss 5.3906e-01 (4.5351e-01)\tAcc@1  81.00 ( 80.55)\n",
      "[xla:5]Train:  Epoch: [5][ 60/128]\tTime  0.706 ( 0.763)\tData  0.052 ( 0.059)\tLoss 2.0996e-01 (3.8979e-01)\tAcc@1  94.00 ( 84.32)\n",
      "[xla:3]Train:  Epoch: [5][ 60/128]\tTime  0.730 ( 0.771)\tData  0.053 ( 0.062)\tLoss 6.0938e-01 (4.5490e-01)\tAcc@1  69.00 ( 82.06)\n",
      "[xla:4]Train:  Epoch: [5][ 60/128]\tTime  0.845 ( 0.761)\tData  0.040 ( 0.062)\tLoss 1.9336e-01 (4.1168e-01)\tAcc@1  94.00 ( 85.44)\n",
      "[xla:1]Train:  Epoch: [5][ 60/128]\tTime  0.839 ( 0.775)\tData  0.045 ( 0.059)\tLoss 7.0703e-01 (4.0501e-01)\tAcc@1  75.00 ( 84.63)\n",
      "[xla:0]Train:  Epoch: [5][ 90/128]\tTime  0.632 ( 0.763)\tData  0.038 ( 0.060)\tLoss 4.6680e-01 (4.6191e-01)\tAcc@1  81.00 ( 81.57)\n",
      "[xla:3]Train:  Epoch: [5][ 90/128]\tTime  0.552 ( 0.770)\tData  0.050 ( 0.062)\tLoss 3.2812e-01 (4.7135e-01)\tAcc@1  81.00 ( 81.57)\n",
      "[xla:4]Train:  Epoch: [5][ 90/128]\tTime  0.618 ( 0.764)\tData  0.046 ( 0.060)\tLoss 6.6406e-01 (4.2746e-01)\tAcc@1  62.50 ( 84.40)\n",
      "[xla:7]Train:  Epoch: [5][ 90/128]\tTime  0.841 ( 0.767)\tData  0.057 ( 0.058)\tLoss 3.1055e-01 (4.3565e-01)\tAcc@1  87.50 ( 82.93)\n",
      "[xla:6]Train:  Epoch: [5][ 90/128]\tTime  0.930 ( 0.767)\tData  0.084 ( 0.057)\tLoss 2.5781e-01 (4.6062e-01)\tAcc@1  94.00 ( 80.51)\n",
      "[xla:5]Train:  Epoch: [5][ 90/128]\tTime  0.852 ( 0.767)\tData  0.044 ( 0.059)\tLoss 5.0391e-01 (3.9447e-01)\tAcc@1  75.00 ( 84.22)\n",
      "[xla:1]Train:  Epoch: [5][ 90/128]\tTime  0.791 ( 0.774)\tData  0.046 ( 0.058)\tLoss 5.2734e-01 (3.9714e-01)\tAcc@1  69.00 ( 85.17)\n",
      "[xla:2]Train:  Epoch: [5][ 90/128]\tTime  0.826 ( 0.769)\tData  0.056 ( 0.058)\tLoss 6.8359e-01 (4.4613e-01)\tAcc@1  69.00 ( 81.66)\n",
      "[xla:7]Train:  Epoch: [5][120/128]\tTime  0.549 ( 0.763)\tData  0.056 ( 0.058)\tLoss 5.8984e-01 (4.4426e-01)\tAcc@1  69.00 ( 82.62)\n",
      "[xla:5]Train:  Epoch: [5][120/128]\tTime  0.558 ( 0.763)\tData  0.045 ( 0.058)\tLoss 4.1992e-01 (4.1475e-01)\tAcc@1  81.00 ( 83.43)\n",
      "[xla:2]Train:  Epoch: [5][120/128]\tTime  0.727 ( 0.765)\tData  0.044 ( 0.057)\tLoss 2.7344e-01 (4.4992e-01)\tAcc@1  94.00 ( 82.04)\n",
      "[xla:0]Train:  Epoch: [5][120/128]\tTime  0.693 ( 0.762)\tData  0.045 ( 0.060)\tLoss 3.6133e-01 (4.6446e-01)\tAcc@1  81.00 ( 81.38)\n",
      "[xla:6]Train:  Epoch: [5][120/128]\tTime  0.711 ( 0.764)\tData  0.064 ( 0.058)\tLoss 7.8516e-01 (4.4737e-01)\tAcc@1  75.00 ( 81.79)\n",
      "[xla:4]Train:  Epoch: [5][120/128]\tTime  0.862 ( 0.763)\tData  0.072 ( 0.059)\tLoss 8.0078e-01 (4.4018e-01)\tAcc@1  75.00 ( 84.19)\n",
      "[xla:3]Train:  Epoch: [5][120/128]\tTime  0.732 ( 0.768)\tData  0.046 ( 0.062)\tLoss 4.6484e-01 (4.7505e-01)\tAcc@1  87.50 ( 81.50)\n",
      "[xla:1]Train:  Epoch: [5][120/128]\tTime  0.909 ( 0.770)\tData  0.044 ( 0.057)\tLoss 4.2578e-01 (4.2163e-01)\tAcc@1  75.00 ( 84.20)\n",
      "[xla:5]Validation: [0/7]\tTime  0.307 ( 0.307)\tLoss 7.5781e-01 (7.5781e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 5\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/7]\tTime  0.255 ( 0.255)\tLoss 3.8477e-01 (3.8477e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.269 ( 0.269)\tLoss 4.6875e-01 (4.6875e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Validation: [0/7]\tTime  0.184 ( 0.184)\tLoss 5.8984e-01 (5.8984e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.232 ( 0.232)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.235 ( 0.235)\tLoss 4.8047e-01 (4.8047e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.217 ( 0.217)\tLoss 6.8750e-01 (6.8750e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.191 ( 0.191)\tLoss 4.6484e-01 (4.6484e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:5]Train:  Epoch: [6][  0/128]\tTime  0.631 ( 0.631)\tData  0.097 ( 0.097)\tLoss 2.2461e-01 (2.2461e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:6]Train:  Epoch: [6][  0/128]\tTime  0.702 ( 0.702)\tData  0.091 ( 0.091)\tLoss 5.2344e-01 (5.2344e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Train:  Epoch: [6][  0/128]\tTime  0.697 ( 0.697)\tData  0.097 ( 0.097)\tLoss 3.9258e-01 (3.9258e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Train:  Epoch: [6][  0/128]\tTime  0.670 ( 0.670)\tData  0.046 ( 0.046)\tLoss 3.5352e-01 (3.5352e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:4]Train:  Epoch: [6][  0/128]\tTime  0.636 ( 0.636)\tData  0.072 ( 0.072)\tLoss 3.7695e-01 (3.7695e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [6][  0/128]\tTime  0.527 ( 0.527)\tData  0.031 ( 0.031)\tLoss 4.8242e-01 (4.8242e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:1]Train:  Epoch: [6][  0/128]\tTime  0.703 ( 0.703)\tData  0.073 ( 0.073)\tLoss 4.1797e-01 (4.1797e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:2]Train:  Epoch: [6][  0/128]\tTime  0.540 ( 0.540)\tData  0.087 ( 0.087)\tLoss 5.0781e-01 (5.0781e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [6][ 30/128]\tTime  0.614 ( 0.774)\tData  0.039 ( 0.060)\tLoss 4.0625e-01 (4.4594e-01)\tAcc@1  81.00 ( 80.44)\n",
      "[xla:6]Train:  Epoch: [6][ 30/128]\tTime  0.540 ( 0.776)\tData  0.061 ( 0.066)\tLoss 2.2461e-01 (4.1907e-01)\tAcc@1  87.50 ( 81.47)\n",
      "[xla:4]Train:  Epoch: [6][ 30/128]\tTime  0.754 ( 0.778)\tData  0.054 ( 0.060)\tLoss 5.0000e-01 (4.3577e-01)\tAcc@1  75.00 ( 83.48)\n",
      "[xla:2]Train:  Epoch: [6][ 30/128]\tTime  0.797 ( 0.773)\tData  0.044 ( 0.057)\tLoss 3.8867e-01 (4.1253e-01)\tAcc@1  87.50 ( 83.87)\n",
      "[xla:0]Train:  Epoch: [6][ 30/128]\tTime  0.789 ( 0.774)\tData  0.068 ( 0.057)\tLoss 5.9766e-01 (4.8497e-01)\tAcc@1  62.50 ( 79.58)\n",
      "[xla:3]Train:  Epoch: [6][ 30/128]\tTime  0.791 ( 0.780)\tData  0.079 ( 0.058)\tLoss 3.5938e-01 (4.7505e-01)\tAcc@1  81.00 ( 81.44)\n",
      "[xla:5]Train:  Epoch: [6][ 30/128]\tTime  0.784 ( 0.795)\tData  0.081 ( 0.070)\tLoss 2.9688e-01 (4.0590e-01)\tAcc@1  87.50 ( 83.28)\n",
      "[xla:1]Train:  Epoch: [6][ 30/128]\tTime  0.942 ( 0.779)\tData  0.068 ( 0.062)\tLoss 1.3281e-01 (3.9607e-01)\tAcc@1 100.00 ( 84.63)\n",
      "[xla:5]Train:  Epoch: [6][ 60/128]\tTime  0.714 ( 0.781)\tData  0.048 ( 0.064)\tLoss 1.8262e-01 (4.0145e-01)\tAcc@1  94.00 ( 83.53)\n",
      "[xla:4]Train:  Epoch: [6][ 60/128]\tTime  0.713 ( 0.773)\tData  0.073 ( 0.059)\tLoss 2.3828e-01 (4.2227e-01)\tAcc@1  94.00 ( 84.42)\n",
      "[xla:6]Train:  Epoch: [6][ 60/128]\tTime  0.824 ( 0.776)\tData  0.044 ( 0.064)\tLoss 6.6016e-01 (4.2932e-01)\tAcc@1  81.00 ( 82.07)\n",
      "[xla:2]Train:  Epoch: [6][ 60/128]\tTime  0.771 ( 0.771)\tData  0.049 ( 0.058)\tLoss 2.5391e-01 (4.2415e-01)\tAcc@1  94.00 ( 83.31)\n",
      "[xla:1]Train:  Epoch: [6][ 60/128]\tTime  0.800 ( 0.774)\tData  0.070 ( 0.060)\tLoss 9.0234e-01 (3.9404e-01)\tAcc@1  62.50 ( 84.81)\n",
      "[xla:7]Train:  Epoch: [6][ 60/128]\tTime  0.790 ( 0.775)\tData  0.068 ( 0.058)\tLoss 3.3984e-01 (4.4217e-01)\tAcc@1  81.00 ( 80.71)\n",
      "[xla:0]Train:  Epoch: [6][ 60/128]\tTime  0.866 ( 0.772)\tData  0.045 ( 0.056)\tLoss 7.8906e-01 (4.4731e-01)\tAcc@1  75.00 ( 81.91)\n",
      "[xla:3]Train:  Epoch: [6][ 60/128]\tTime  0.806 ( 0.775)\tData  0.073 ( 0.059)\tLoss 9.4531e-01 (4.2402e-01)\tAcc@1  62.50 ( 83.80)\n",
      "[xla:3]Train:  Epoch: [6][ 90/128]\tTime  0.651 ( 0.770)\tData  0.040 ( 0.059)\tLoss 2.4805e-01 (4.1988e-01)\tAcc@1  94.00 ( 83.71)\n",
      "[xla:1]Train:  Epoch: [6][ 90/128]\tTime  0.812 ( 0.771)\tData  0.047 ( 0.058)\tLoss 7.8516e-01 (3.9354e-01)\tAcc@1  56.25 ( 85.30)\n",
      "[xla:6]Train:  Epoch: [6][ 90/128]\tTime  0.723 ( 0.773)\tData  0.075 ( 0.063)\tLoss 1.0449e-01 (4.3908e-01)\tAcc@1 100.00 ( 82.27)\n",
      "[xla:7]Train:  Epoch: [6][ 90/128]\tTime  0.758 ( 0.773)\tData  0.046 ( 0.058)\tLoss 2.4609e-01 (4.3686e-01)\tAcc@1  94.00 ( 81.43)\n",
      "[xla:2]Train:  Epoch: [6][ 90/128]\tTime  0.848 ( 0.770)\tData  0.047 ( 0.058)\tLoss 6.7188e-01 (4.4060e-01)\tAcc@1  69.00 ( 82.89)\n",
      "[xla:4]Train:  Epoch: [6][ 90/128]\tTime  1.059 ( 0.772)\tData  0.068 ( 0.059)\tLoss 6.0547e-01 (4.2485e-01)\tAcc@1  87.50 ( 83.72)\n",
      "[xla:5]Train:  Epoch: [6][ 90/128]\tTime  0.824 ( 0.778)\tData  0.073 ( 0.061)\tLoss 3.8281e-01 (3.9197e-01)\tAcc@1  87.50 ( 84.37)\n",
      "[xla:0]Train:  Epoch: [6][ 90/128]\tTime  0.847 ( 0.771)\tData  0.067 ( 0.056)\tLoss 3.5742e-01 (4.3776e-01)\tAcc@1  87.50 ( 82.38)\n",
      "[xla:5]Train:  Epoch: [6][120/128]\tTime  0.612 ( 0.773)\tData  0.044 ( 0.061)\tLoss 2.1973e-01 (4.0434e-01)\tAcc@1  94.00 ( 84.12)\n",
      "[xla:0]Train:  Epoch: [6][120/128]\tTime  0.647 ( 0.767)\tData  0.044 ( 0.057)\tLoss 2.7930e-01 (4.2954e-01)\tAcc@1  87.50 ( 83.25)\n",
      "[xla:6]Train:  Epoch: [6][120/128]\tTime  0.577 ( 0.770)\tData  0.046 ( 0.062)\tLoss 7.2656e-01 (4.2736e-01)\tAcc@1  81.00 ( 83.16)\n",
      "[xla:1]Train:  Epoch: [6][120/128]\tTime  0.688 ( 0.770)\tData  0.046 ( 0.057)\tLoss 4.5312e-01 (4.1232e-01)\tAcc@1  81.00 ( 84.30)\n",
      "[xla:3]Train:  Epoch: [6][120/128]\tTime  0.710 ( 0.770)\tData  0.045 ( 0.059)\tLoss 5.6641e-01 (4.2138e-01)\tAcc@1  81.00 ( 83.61)\n",
      "[xla:7]Train:  Epoch: [6][120/128]\tTime  0.685 ( 0.771)\tData  0.061 ( 0.058)\tLoss 5.7422e-01 (4.4141e-01)\tAcc@1  81.00 ( 81.44)\n",
      "[xla:4]Train:  Epoch: [6][120/128]\tTime  0.822 ( 0.770)\tData  0.046 ( 0.060)\tLoss 5.9375e-01 (4.3099e-01)\tAcc@1  81.00 ( 83.35)\n",
      "[xla:2]Train:  Epoch: [6][120/128]\tTime  0.894 ( 0.769)\tData  0.051 ( 0.058)\tLoss 3.1836e-01 (4.3932e-01)\tAcc@1  87.50 ( 83.15)\n",
      "Finished training epoch 6\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/7]\tTime  0.388 ( 0.388)\tLoss 6.5234e-01 (6.5234e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.276 ( 0.276)\tLoss 7.3828e-01 (7.3828e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.279 ( 0.279)\tLoss 5.3906e-01 (5.3906e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.233 ( 0.233)\tLoss 4.4141e-01 (4.4141e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.195 ( 0.195)\tLoss 4.3750e-01 (4.3750e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.203 ( 0.203)\tLoss 3.5352e-01 (3.5352e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.212 ( 0.212)\tLoss 4.4531e-01 (4.4531e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.225 ( 0.225)\tLoss 5.4688e-01 (5.4688e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:1]Train:  Epoch: [7][  0/128]\tTime  0.751 ( 0.751)\tData  0.085 ( 0.085)\tLoss 3.1641e-01 (3.1641e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:2]Train:  Epoch: [7][  0/128]\tTime  0.557 ( 0.557)\tData  0.063 ( 0.063)\tLoss 5.2344e-01 (5.2344e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [7][  0/128]\tTime  0.663 ( 0.663)\tData  0.067 ( 0.067)\tLoss 4.5508e-01 (4.5508e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:0]Train:  Epoch: [7][  0/128]\tTime  0.597 ( 0.597)\tData  0.055 ( 0.055)\tLoss 5.1172e-01 (5.1172e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Train:  Epoch: [7][  0/128]\tTime  0.678 ( 0.678)\tData  0.062 ( 0.062)\tLoss 2.8711e-01 (2.8711e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Train:  Epoch: [7][  0/128]\tTime  0.559 ( 0.559)\tData  0.081 ( 0.081)\tLoss 2.6367e-01 (2.6367e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Train:  Epoch: [7][  0/128]\tTime  0.511 ( 0.511)\tData  0.083 ( 0.083)\tLoss 2.8516e-01 (2.8516e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Train:  Epoch: [7][  0/128]\tTime  0.421 ( 0.421)\tData  0.057 ( 0.057)\tLoss 4.9805e-01 (4.9805e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:5]Train:  Epoch: [7][ 30/128]\tTime  0.589 ( 0.757)\tData  0.038 ( 0.057)\tLoss 2.5000e-01 (3.5900e-01)\tAcc@1  87.50 ( 85.47)\n",
      "[xla:6]Train:  Epoch: [7][ 30/128]\tTime  0.654 ( 0.741)\tData  0.036 ( 0.055)\tLoss 1.7773e-01 (3.7966e-01)\tAcc@1 100.00 ( 87.32)\n",
      "[xla:7]Train:  Epoch: [7][ 30/128]\tTime  0.715 ( 0.753)\tData  0.082 ( 0.057)\tLoss 2.0312e-01 (3.9113e-01)\tAcc@1 100.00 ( 84.50)\n",
      "[xla:2]Train:  Epoch: [7][ 30/128]\tTime  0.939 ( 0.761)\tData  0.070 ( 0.055)\tLoss 4.0039e-01 (3.7484e-01)\tAcc@1  81.00 ( 84.84)\n",
      "[xla:3]Train:  Epoch: [7][ 30/128]\tTime  0.761 ( 0.751)\tData  0.046 ( 0.053)\tLoss 5.8984e-01 (4.4459e-01)\tAcc@1  75.00 ( 82.85)\n",
      "[xla:1]Train:  Epoch: [7][ 30/128]\tTime  0.779 ( 0.771)\tData  0.067 ( 0.057)\tLoss 1.7480e-01 (3.6360e-01)\tAcc@1 100.00 ( 86.13)\n",
      "[xla:0]Train:  Epoch: [7][ 30/128]\tTime  0.829 ( 0.762)\tData  0.081 ( 0.055)\tLoss 6.6797e-01 (4.7281e-01)\tAcc@1  69.00 ( 79.62)\n",
      "[xla:4]Train:  Epoch: [7][ 30/128]\tTime  1.056 ( 0.766)\tData  0.072 ( 0.056)\tLoss 4.0430e-01 (4.1096e-01)\tAcc@1  81.00 ( 83.29)\n",
      "[xla:1]Train:  Epoch: [7][ 60/128]\tTime  0.669 ( 0.773)\tData  0.064 ( 0.060)\tLoss 7.5000e-01 (3.5857e-01)\tAcc@1  69.00 ( 86.39)\n",
      "[xla:6]Train:  Epoch: [7][ 60/128]\tTime  0.745 ( 0.762)\tData  0.088 ( 0.059)\tLoss 3.7109e-01 (3.9322e-01)\tAcc@1  94.00 ( 86.27)\n",
      "[xla:5]Train:  Epoch: [7][ 60/128]\tTime  0.749 ( 0.771)\tData  0.051 ( 0.057)\tLoss 9.0332e-02 (3.3997e-01)\tAcc@1 100.00 ( 86.25)\n",
      "[xla:7]Train:  Epoch: [7][ 60/128]\tTime  0.772 ( 0.766)\tData  0.045 ( 0.058)\tLoss 2.2461e-01 (3.8713e-01)\tAcc@1  94.00 ( 85.18)\n",
      "[xla:2]Train:  Epoch: [7][ 60/128]\tTime  0.739 ( 0.770)\tData  0.060 ( 0.057)\tLoss 2.3047e-01 (3.9192e-01)\tAcc@1  94.00 ( 84.18)\n",
      "[xla:0]Train:  Epoch: [7][ 60/128]\tTime  0.768 ( 0.769)\tData  0.061 ( 0.055)\tLoss 5.5078e-01 (4.4557e-01)\tAcc@1  81.00 ( 82.05)\n",
      "[xla:4]Train:  Epoch: [7][ 60/128]\tTime  0.777 ( 0.770)\tData  0.076 ( 0.054)\tLoss 1.2695e-01 (3.9981e-01)\tAcc@1 100.00 ( 84.01)\n",
      "[xla:3]Train:  Epoch: [7][ 60/128]\tTime  0.836 ( 0.766)\tData  0.060 ( 0.057)\tLoss 5.8594e-01 (4.0481e-01)\tAcc@1  81.00 ( 84.36)\n",
      "[xla:1]Train:  Epoch: [7][ 90/128]\tTime  0.666 ( 0.772)\tData  0.039 ( 0.059)\tLoss 5.4688e-01 (3.6937e-01)\tAcc@1  69.00 ( 86.14)\n",
      "[xla:6]Train:  Epoch: [7][ 90/128]\tTime  0.742 ( 0.766)\tData  0.090 ( 0.058)\tLoss 2.9102e-01 (4.0606e-01)\tAcc@1  94.00 ( 85.09)\n",
      "[xla:4]Train:  Epoch: [7][ 90/128]\tTime  0.769 ( 0.772)\tData  0.050 ( 0.055)\tLoss 7.2656e-01 (4.1112e-01)\tAcc@1  69.00 ( 83.57)\n",
      "[xla:0]Train:  Epoch: [7][ 90/128]\tTime  0.835 ( 0.771)\tData  0.046 ( 0.055)\tLoss 5.4297e-01 (4.3345e-01)\tAcc@1  75.00 ( 82.90)\n",
      "[xla:5]Train:  Epoch: [7][ 90/128]\tTime  1.006 ( 0.773)\tData  0.099 ( 0.058)\tLoss 3.4766e-01 (3.5997e-01)\tAcc@1  94.00 ( 85.71)\n",
      "[xla:7]Train:  Epoch: [7][ 90/128]\tTime  0.838 ( 0.770)\tData  0.058 ( 0.059)\tLoss 2.0703e-01 (3.9670e-01)\tAcc@1  94.00 ( 84.93)\n",
      "[xla:3]Train:  Epoch: [7][ 90/128]\tTime  0.877 ( 0.769)\tData  0.052 ( 0.055)\tLoss 3.2422e-01 (4.0910e-01)\tAcc@1  87.50 ( 84.29)\n",
      "[xla:2]Train:  Epoch: [7][ 90/128]\tTime  0.849 ( 0.772)\tData  0.064 ( 0.057)\tLoss 5.6641e-01 (3.9663e-01)\tAcc@1  69.00 ( 84.24)\n",
      "[xla:4]Train:  Epoch: [7][120/128]\tTime  0.670 ( 0.770)\tData  0.040 ( 0.056)\tLoss 4.3555e-01 (4.1571e-01)\tAcc@1  81.00 ( 83.92)\n",
      "[xla:0]Train:  Epoch: [7][120/128]\tTime  0.732 ( 0.771)\tData  0.042 ( 0.055)\tLoss 2.2461e-01 (4.2624e-01)\tAcc@1  94.00 ( 83.17)\n",
      "[xla:1]Train:  Epoch: [7][120/128]\tTime  0.710 ( 0.774)\tData  0.056 ( 0.059)\tLoss 4.2578e-01 (3.9125e-01)\tAcc@1  87.50 ( 85.61)\n",
      "[xla:6]Train:  Epoch: [7][120/128]\tTime  0.971 ( 0.768)\tData  0.057 ( 0.058)\tLoss 6.7188e-01 (4.0702e-01)\tAcc@1  81.00 ( 84.71)\n",
      "[xla:2]Train:  Epoch: [7][120/128]\tTime  0.754 ( 0.772)\tData  0.048 ( 0.057)\tLoss 2.7734e-01 (4.0214e-01)\tAcc@1  87.50 ( 83.92)\n",
      "[xla:5]Train:  Epoch: [7][120/128]\tTime  0.758 ( 0.773)\tData  0.043 ( 0.057)\tLoss 3.7500e-01 (3.7978e-01)\tAcc@1  87.50 ( 84.75)\n",
      "[xla:7]Train:  Epoch: [7][120/128]\tTime  1.012 ( 0.770)\tData  0.067 ( 0.059)\tLoss 5.1953e-01 (4.1380e-01)\tAcc@1  81.00 ( 84.42)\n",
      "[xla:3]Train:  Epoch: [7][120/128]\tTime  0.773 ( 0.769)\tData  0.059 ( 0.055)\tLoss 4.2773e-01 (4.0264e-01)\tAcc@1  94.00 ( 84.48)\n",
      "[xla:4]Validation: [0/7]\tTime  0.353 ( 0.353)\tLoss 4.5312e-01 (4.5312e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Finished training epoch 7\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:3]Validation: [0/7]\tTime  0.189 ( 0.189)\tLoss 4.7656e-01 (4.7656e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.262 ( 0.262)\tLoss 5.5469e-01 (5.5469e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.215 ( 0.215)\tLoss 5.3125e-01 (5.3125e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.210 ( 0.210)\tLoss 7.5391e-01 (7.5391e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.200 ( 0.200)\tLoss 4.6875e-01 (4.6875e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.196 ( 0.196)\tLoss 6.3281e-01 (6.3281e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.183 ( 0.183)\tLoss 3.4180e-01 (3.4180e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [8][  0/128]\tTime  0.607 ( 0.607)\tData  0.046 ( 0.046)\tLoss 3.3984e-01 (3.3984e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Train:  Epoch: [8][  0/128]\tTime  0.666 ( 0.666)\tData  0.023 ( 0.023)\tLoss 4.2383e-01 (4.2383e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Train:  Epoch: [8][  0/128]\tTime  0.605 ( 0.605)\tData  0.088 ( 0.088)\tLoss 4.4336e-01 (4.4336e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:1]Train:  Epoch: [8][  0/128]\tTime  0.508 ( 0.508)\tData  0.034 ( 0.034)\tLoss 3.9258e-01 (3.9258e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:2]Train:  Epoch: [8][  0/128]\tTime  0.673 ( 0.673)\tData  0.062 ( 0.062)\tLoss 5.7031e-01 (5.7031e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:5]Train:  Epoch: [8][  0/128]\tTime  0.584 ( 0.584)\tData  0.040 ( 0.040)\tLoss 2.9492e-01 (2.9492e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [8][  0/128]\tTime  0.561 ( 0.561)\tData  0.101 ( 0.101)\tLoss 5.9375e-01 (5.9375e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [8][  0/128]\tTime  0.512 ( 0.512)\tData  0.087 ( 0.087)\tLoss 2.1191e-01 (2.1191e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:6]Train:  Epoch: [8][ 30/128]\tTime  0.544 ( 0.760)\tData  0.078 ( 0.061)\tLoss 1.6211e-01 (3.9488e-01)\tAcc@1 100.00 ( 83.87)\n",
      "[xla:1]Train:  Epoch: [8][ 30/128]\tTime  0.642 ( 0.753)\tData  0.037 ( 0.059)\tLoss 1.5918e-01 (3.3074e-01)\tAcc@1  94.00 ( 87.16)\n",
      "[xla:5]Train:  Epoch: [8][ 30/128]\tTime  0.693 ( 0.758)\tData  0.047 ( 0.054)\tLoss 2.0410e-01 (3.5859e-01)\tAcc@1  94.00 ( 83.92)\n",
      "[xla:3]Train:  Epoch: [8][ 30/128]\tTime  0.865 ( 0.770)\tData  0.059 ( 0.053)\tLoss 2.2656e-01 (4.1510e-01)\tAcc@1  94.00 ( 84.11)\n",
      "[xla:4]Train:  Epoch: [8][ 30/128]\tTime  0.726 ( 0.775)\tData  0.055 ( 0.058)\tLoss 2.7148e-01 (4.0152e-01)\tAcc@1  94.00 ( 85.66)\n",
      "[xla:2]Train:  Epoch: [8][ 30/128]\tTime  0.776 ( 0.764)\tData  0.041 ( 0.065)\tLoss 2.3730e-01 (3.7319e-01)\tAcc@1  94.00 ( 86.06)\n",
      "[xla:7]Train:  Epoch: [8][ 30/128]\tTime  0.763 ( 0.752)\tData  0.085 ( 0.060)\tLoss 4.1602e-01 (4.1986e-01)\tAcc@1  81.00 ( 82.90)\n",
      "[xla:0]Train:  Epoch: [8][ 30/128]\tTime  0.740 ( 0.757)\tData  0.048 ( 0.058)\tLoss 3.4570e-01 (4.2736e-01)\tAcc@1  94.00 ( 83.06)\n",
      "[xla:5]Train:  Epoch: [8][ 60/128]\tTime  0.514 ( 0.767)\tData  0.039 ( 0.055)\tLoss 8.6426e-02 (3.5267e-01)\tAcc@1 100.00 ( 85.48)\n",
      "[xla:0]Train:  Epoch: [8][ 60/128]\tTime  0.550 ( 0.766)\tData  0.043 ( 0.059)\tLoss 7.1484e-01 (4.1875e-01)\tAcc@1  69.00 ( 83.41)\n",
      "[xla:2]Train:  Epoch: [8][ 60/128]\tTime  0.659 ( 0.771)\tData  0.047 ( 0.063)\tLoss 1.6992e-01 (3.7296e-01)\tAcc@1  94.00 ( 85.84)\n",
      "[xla:6]Train:  Epoch: [8][ 60/128]\tTime  0.756 ( 0.773)\tData  0.042 ( 0.060)\tLoss 2.7539e-01 (3.8241e-01)\tAcc@1  94.00 ( 84.03)\n",
      "[xla:1]Train:  Epoch: [8][ 60/128]\tTime  0.815 ( 0.770)\tData  0.045 ( 0.056)\tLoss 8.0469e-01 (3.3752e-01)\tAcc@1  69.00 ( 86.52)\n",
      "[xla:3]Train:  Epoch: [8][ 60/128]\tTime  0.779 ( 0.777)\tData  0.038 ( 0.055)\tLoss 7.5781e-01 (3.8845e-01)\tAcc@1  62.50 ( 84.77)\n",
      "[xla:4]Train:  Epoch: [8][ 60/128]\tTime  0.805 ( 0.779)\tData  0.053 ( 0.056)\tLoss 1.1230e-01 (3.8518e-01)\tAcc@1  94.00 ( 85.02)\n",
      "[xla:7]Train:  Epoch: [8][ 60/128]\tTime  0.804 ( 0.768)\tData  0.047 ( 0.060)\tLoss 3.1641e-01 (4.1704e-01)\tAcc@1  94.00 ( 83.42)\n",
      "[xla:2]Train:  Epoch: [8][ 90/128]\tTime  0.701 ( 0.771)\tData  0.041 ( 0.061)\tLoss 5.6250e-01 (3.8503e-01)\tAcc@1  69.00 ( 85.71)\n",
      "[xla:0]Train:  Epoch: [8][ 90/128]\tTime  0.711 ( 0.769)\tData  0.059 ( 0.061)\tLoss 3.1250e-01 (4.1588e-01)\tAcc@1  87.50 ( 83.67)\n",
      "[xla:1]Train:  Epoch: [8][ 90/128]\tTime  0.713 ( 0.771)\tData  0.066 ( 0.058)\tLoss 4.5703e-01 (3.3429e-01)\tAcc@1  75.00 ( 87.26)\n",
      "[xla:3]Train:  Epoch: [8][ 90/128]\tTime  0.765 ( 0.775)\tData  0.047 ( 0.058)\tLoss 3.0859e-01 (4.0459e-01)\tAcc@1  81.00 ( 83.87)\n",
      "[xla:6]Train:  Epoch: [8][ 90/128]\tTime  0.742 ( 0.774)\tData  0.057 ( 0.061)\tLoss 1.7773e-01 (4.0002e-01)\tAcc@1 100.00 ( 83.80)\n",
      "[xla:5]Train:  Epoch: [8][ 90/128]\tTime  0.763 ( 0.771)\tData  0.061 ( 0.056)\tLoss 3.1055e-01 (3.5193e-01)\tAcc@1  87.50 ( 85.38)\n",
      "[xla:7]Train:  Epoch: [8][ 90/128]\tTime  0.796 ( 0.769)\tData  0.049 ( 0.060)\tLoss 2.2754e-01 (4.1834e-01)\tAcc@1  87.50 ( 83.95)\n",
      "[xla:4]Train:  Epoch: [8][ 90/128]\tTime  0.860 ( 0.777)\tData  0.044 ( 0.059)\tLoss 6.6797e-01 (3.9528e-01)\tAcc@1  75.00 ( 84.59)\n",
      "[xla:0]Train:  Epoch: [8][120/128]\tTime  0.712 ( 0.766)\tData  0.045 ( 0.060)\tLoss 2.1582e-01 (4.0784e-01)\tAcc@1  94.00 ( 83.75)\n",
      "[xla:7]Train:  Epoch: [8][120/128]\tTime  0.586 ( 0.765)\tData  0.058 ( 0.059)\tLoss 5.2344e-01 (4.2885e-01)\tAcc@1  81.00 ( 83.33)\n",
      "[xla:6]Train:  Epoch: [8][120/128]\tTime  0.687 ( 0.770)\tData  0.066 ( 0.061)\tLoss 4.6680e-01 (3.9273e-01)\tAcc@1  87.50 ( 84.62)\n",
      "[xla:1]Train:  Epoch: [8][120/128]\tTime  0.739 ( 0.768)\tData  0.050 ( 0.058)\tLoss 2.6953e-01 (3.5678e-01)\tAcc@1  94.00 ( 86.29)\n",
      "[xla:5]Train:  Epoch: [8][120/128]\tTime  0.770 ( 0.769)\tData  0.049 ( 0.057)\tLoss 3.4375e-01 (3.6968e-01)\tAcc@1  81.00 ( 84.98)\n",
      "[xla:4]Train:  Epoch: [8][120/128]\tTime  0.792 ( 0.773)\tData  0.045 ( 0.059)\tLoss 6.4844e-01 (4.0445e-01)\tAcc@1  75.00 ( 84.39)\n",
      "[xla:2]Train:  Epoch: [8][120/128]\tTime  1.044 ( 0.770)\tData  0.107 ( 0.062)\tLoss 1.7773e-01 (3.9822e-01)\tAcc@1  87.50 ( 84.60)\n",
      "[xla:3]Train:  Epoch: [8][120/128]\tTime  0.771 ( 0.772)\tData  0.050 ( 0.057)\tLoss 4.2773e-01 (4.0518e-01)\tAcc@1  81.00 ( 84.36)\n",
      "[xla:1]Validation: [0/7]\tTime  0.329 ( 0.329)\tLoss 6.5625e-01 (6.5625e-01)\tAcc@1  75.00 ( 75.00)\n",
      "Finished training epoch 8\n",
      "[xla:6]Validation: [0/7]\tTime  0.200 ( 0.200)\tLoss 5.1953e-01 (5.1953e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:7]Validation: [0/7]\tTime  0.264 ( 0.264)\tLoss 3.3398e-01 (3.3398e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.238 ( 0.238)\tLoss 7.8906e-01 (7.8906e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.183 ( 0.183)\tLoss 4.5312e-01 (4.5312e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Validation: [0/7]\tTime  0.220 ( 0.220)\tLoss 4.6094e-01 (4.6094e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.286 ( 0.286)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.229 ( 0.229)\tLoss 4.3555e-01 (4.3555e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:1]Train:  Epoch: [9][  0/128]\tTime  0.707 ( 0.707)\tData  0.044 ( 0.044)\tLoss 3.5352e-01 (3.5352e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:5]Train:  Epoch: [9][  0/128]\tTime  0.535 ( 0.535)\tData  0.078 ( 0.078)\tLoss 4.9609e-01 (4.9609e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Train:  Epoch: [9][  0/128]\tTime  0.685 ( 0.685)\tData  0.115 ( 0.115)\tLoss 4.4531e-01 (4.4531e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:7]Train:  Epoch: [9][  0/128]\tTime  0.686 ( 0.686)\tData  0.050 ( 0.050)\tLoss 3.5938e-01 (3.5938e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [9][  0/128]\tTime  0.486 ( 0.486)\tData  0.033 ( 0.033)\tLoss 3.5938e-01 (3.5938e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Train:  Epoch: [9][  0/128]\tTime  0.520 ( 0.520)\tData  0.077 ( 0.077)\tLoss 5.0391e-01 (5.0391e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Train:  Epoch: [9][  0/128]\tTime  0.455 ( 0.455)\tData  0.109 ( 0.109)\tLoss 5.6250e-01 (5.6250e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Train:  Epoch: [9][  0/128]\tTime  0.334 ( 0.334)\tData  0.019 ( 0.019)\tLoss 3.1055e-01 (3.1055e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [9][ 30/128]\tTime  0.482 ( 0.778)\tData  0.033 ( 0.059)\tLoss 3.7500e-01 (3.9970e-01)\tAcc@1  75.00 ( 82.43)\n",
      "[xla:6]Train:  Epoch: [9][ 30/128]\tTime  0.674 ( 0.782)\tData  0.047 ( 0.059)\tLoss 1.7188e-01 (3.7637e-01)\tAcc@1  94.00 ( 86.32)\n",
      "[xla:3]Train:  Epoch: [9][ 30/128]\tTime  0.575 ( 0.767)\tData  0.046 ( 0.060)\tLoss 3.5156e-01 (3.9699e-01)\tAcc@1  81.00 ( 84.73)\n",
      "[xla:2]Train:  Epoch: [9][ 30/128]\tTime  0.695 ( 0.767)\tData  0.046 ( 0.062)\tLoss 3.6328e-01 (3.6268e-01)\tAcc@1  87.50 ( 86.10)\n",
      "[xla:5]Train:  Epoch: [9][ 30/128]\tTime  0.895 ( 0.784)\tData  0.071 ( 0.064)\tLoss 1.9727e-01 (3.8215e-01)\tAcc@1 100.00 ( 83.86)\n",
      "[xla:0]Train:  Epoch: [9][ 30/128]\tTime  0.769 ( 0.773)\tData  0.048 ( 0.061)\tLoss 3.5156e-01 (4.0666e-01)\tAcc@1  81.00 ( 84.60)\n",
      "[xla:4]Train:  Epoch: [9][ 30/128]\tTime  0.834 ( 0.762)\tData  0.048 ( 0.058)\tLoss 3.9258e-01 (3.7861e-01)\tAcc@1  87.50 ( 87.08)\n",
      "[xla:1]Train:  Epoch: [9][ 30/128]\tTime  0.823 ( 0.792)\tData  0.046 ( 0.060)\tLoss 1.5332e-01 (3.5235e-01)\tAcc@1 100.00 ( 87.08)\n",
      "[xla:6]Train:  Epoch: [9][ 60/128]\tTime  0.614 ( 0.772)\tData  0.041 ( 0.058)\tLoss 1.8066e-01 (3.8561e-01)\tAcc@1  94.00 ( 85.38)\n",
      "[xla:4]Train:  Epoch: [9][ 60/128]\tTime  0.669 ( 0.759)\tData  0.034 ( 0.058)\tLoss 2.1484e-01 (3.7142e-01)\tAcc@1  94.00 ( 86.16)\n",
      "[xla:0]Train:  Epoch: [9][ 60/128]\tTime  0.598 ( 0.765)\tData  0.091 ( 0.061)\tLoss 5.0391e-01 (3.8363e-01)\tAcc@1  75.00 ( 85.32)\n",
      "[xla:5]Train:  Epoch: [9][ 60/128]\tTime  0.676 ( 0.772)\tData  0.069 ( 0.060)\tLoss 8.8379e-02 (3.4981e-01)\tAcc@1 100.00 ( 85.66)\n",
      "[xla:3]Train:  Epoch: [9][ 60/128]\tTime  0.709 ( 0.767)\tData  0.055 ( 0.059)\tLoss 5.8594e-01 (3.9101e-01)\tAcc@1  75.00 ( 83.92)\n",
      "[xla:2]Train:  Epoch: [9][ 60/128]\tTime  0.763 ( 0.766)\tData  0.052 ( 0.059)\tLoss 2.0801e-01 (3.8810e-01)\tAcc@1  94.00 ( 84.84)\n",
      "[xla:1]Train:  Epoch: [9][ 60/128]\tTime  0.823 ( 0.777)\tData  0.052 ( 0.058)\tLoss 5.4688e-01 (3.4427e-01)\tAcc@1  75.00 ( 86.66)\n",
      "[xla:7]Train:  Epoch: [9][ 60/128]\tTime  0.778 ( 0.774)\tData  0.097 ( 0.061)\tLoss 2.3242e-01 (3.9780e-01)\tAcc@1  87.50 ( 83.39)\n",
      "[xla:6]Train:  Epoch: [9][ 90/128]\tTime  0.479 ( 0.775)\tData  0.042 ( 0.057)\tLoss 3.9062e-01 (3.9246e-01)\tAcc@1  87.50 ( 84.71)\n",
      "[xla:3]Train:  Epoch: [9][ 90/128]\tTime  0.528 ( 0.771)\tData  0.040 ( 0.059)\tLoss 1.7090e-01 (3.9868e-01)\tAcc@1  94.00 ( 84.43)\n",
      "[xla:2]Train:  Epoch: [9][ 90/128]\tTime  0.691 ( 0.771)\tData  0.044 ( 0.059)\tLoss 3.8867e-01 (3.8515e-01)\tAcc@1  81.00 ( 84.61)\n",
      "[xla:5]Train:  Epoch: [9][ 90/128]\tTime  0.709 ( 0.776)\tData  0.046 ( 0.059)\tLoss 3.0469e-01 (3.5033e-01)\tAcc@1  94.00 ( 85.59)\n",
      "[xla:7]Train:  Epoch: [9][ 90/128]\tTime  0.841 ( 0.777)\tData  0.049 ( 0.061)\tLoss 2.2070e-01 (3.8957e-01)\tAcc@1  94.00 ( 84.12)\n",
      "[xla:0]Train:  Epoch: [9][ 90/128]\tTime  0.858 ( 0.773)\tData  0.056 ( 0.060)\tLoss 4.9219e-01 (3.8195e-01)\tAcc@1  75.00 ( 85.49)\n",
      "[xla:1]Train:  Epoch: [9][ 90/128]\tTime  0.789 ( 0.780)\tData  0.044 ( 0.058)\tLoss 6.1719e-01 (3.4316e-01)\tAcc@1  62.50 ( 87.23)\n",
      "[xla:4]Train:  Epoch: [9][ 90/128]\tTime  0.779 ( 0.770)\tData  0.058 ( 0.059)\tLoss 4.3555e-01 (3.7512e-01)\tAcc@1  81.00 ( 85.76)\n",
      "[xla:6]Train:  Epoch: [9][120/128]\tTime  0.653 ( 0.773)\tData  0.036 ( 0.059)\tLoss 6.2109e-01 (3.8242e-01)\tAcc@1  87.50 ( 85.35)\n",
      "[xla:7]Train:  Epoch: [9][120/128]\tTime  0.653 ( 0.773)\tData  0.035 ( 0.061)\tLoss 6.7578e-01 (3.9810e-01)\tAcc@1  81.00 ( 83.81)\n",
      "[xla:2]Train:  Epoch: [9][120/128]\tTime  0.792 ( 0.769)\tData  0.050 ( 0.060)\tLoss 2.1875e-01 (3.9683e-01)\tAcc@1  94.00 ( 84.04)\n",
      "[xla:3]Train:  Epoch: [9][120/128]\tTime  0.701 ( 0.770)\tData  0.078 ( 0.059)\tLoss 4.7852e-01 (4.0248e-01)\tAcc@1  81.00 ( 84.52)\n",
      "[xla:4]Train:  Epoch: [9][120/128]\tTime  0.894 ( 0.768)\tData  0.071 ( 0.060)\tLoss 5.1953e-01 (3.9289e-01)\tAcc@1  75.00 ( 84.95)\n",
      "[xla:1]Train:  Epoch: [9][120/128]\tTime  0.731 ( 0.775)\tData  0.075 ( 0.058)\tLoss 5.1562e-01 (3.5939e-01)\tAcc@1  81.00 ( 86.77)\n",
      "[xla:0]Train:  Epoch: [9][120/128]\tTime  0.748 ( 0.771)\tData  0.048 ( 0.060)\tLoss 2.8516e-01 (3.7472e-01)\tAcc@1  87.50 ( 85.69)\n",
      "[xla:5]Train:  Epoch: [9][120/128]\tTime  0.759 ( 0.774)\tData  0.069 ( 0.057)\tLoss 4.7461e-01 (3.7054e-01)\tAcc@1  81.00 ( 85.25)\n",
      "[xla:5]Validation: [0/7]\tTime  0.238 ( 0.238)\tLoss 8.2031e-01 (8.2031e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.268 ( 0.268)\tLoss 3.1445e-01 (3.1445e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Finished training epoch 9\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/7]\tTime  0.216 ( 0.216)\tLoss 6.6406e-01 (6.6406e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.257 ( 0.257)\tLoss 4.1602e-01 (4.1602e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.235 ( 0.235)\tLoss 5.0781e-01 (5.0781e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.248 ( 0.248)\tLoss 5.5078e-01 (5.5078e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.170 ( 0.170)\tLoss 4.2773e-01 (4.2773e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.148 ( 0.148)\tLoss 4.0820e-01 (4.0820e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:5]Train:  Epoch: [10][  0/128]\tTime  0.644 ( 0.644)\tData  0.073 ( 0.073)\tLoss 3.6328e-01 (3.6328e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:7]Train:  Epoch: [10][  0/128]\tTime  0.665 ( 0.665)\tData  0.064 ( 0.064)\tLoss 4.9414e-01 (4.9414e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [10][  0/128]\tTime  0.694 ( 0.694)\tData  0.073 ( 0.073)\tLoss 5.1172e-01 (5.1172e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:2]Train:  Epoch: [10][  0/128]\tTime  0.695 ( 0.695)\tData  0.058 ( 0.058)\tLoss 5.2344e-01 (5.2344e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:6]Train:  Epoch: [10][  0/128]\tTime  0.739 ( 0.739)\tData  0.083 ( 0.083)\tLoss 4.8438e-01 (4.8438e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [10][  0/128]\tTime  0.524 ( 0.524)\tData  0.040 ( 0.040)\tLoss 5.2734e-01 (5.2734e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Train:  Epoch: [10][  0/128]\tTime  0.611 ( 0.611)\tData  0.073 ( 0.073)\tLoss 1.6992e-01 (1.6992e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Train:  Epoch: [10][  0/128]\tTime  0.621 ( 0.621)\tData  0.072 ( 0.072)\tLoss 2.5391e-01 (2.5391e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [10][ 30/128]\tTime  0.507 ( 0.764)\tData  0.059 ( 0.058)\tLoss 2.8516e-01 (3.7303e-01)\tAcc@1  87.50 ( 85.71)\n",
      "[xla:5]Train:  Epoch: [10][ 30/128]\tTime  0.586 ( 0.776)\tData  0.034 ( 0.064)\tLoss 1.7773e-01 (3.8439e-01)\tAcc@1  94.00 ( 84.06)\n",
      "[xla:0]Train:  Epoch: [10][ 30/128]\tTime  0.657 ( 0.758)\tData  0.044 ( 0.053)\tLoss 5.2344e-01 (4.2310e-01)\tAcc@1  75.00 ( 83.07)\n",
      "[xla:4]Train:  Epoch: [10][ 30/128]\tTime  0.753 ( 0.765)\tData  0.045 ( 0.059)\tLoss 3.2617e-01 (3.6919e-01)\tAcc@1  87.50 ( 84.69)\n",
      "[xla:2]Train:  Epoch: [10][ 30/128]\tTime  0.914 ( 0.770)\tData  0.070 ( 0.055)\tLoss 3.8086e-01 (3.6373e-01)\tAcc@1  87.50 ( 86.66)\n",
      "[xla:3]Train:  Epoch: [10][ 30/128]\tTime  0.743 ( 0.765)\tData  0.057 ( 0.061)\tLoss 2.9883e-01 (4.1529e-01)\tAcc@1  81.00 ( 83.69)\n",
      "[xla:7]Train:  Epoch: [10][ 30/128]\tTime  0.792 ( 0.774)\tData  0.046 ( 0.057)\tLoss 3.4375e-01 (3.9176e-01)\tAcc@1  81.00 ( 85.06)\n",
      "[xla:6]Train:  Epoch: [10][ 30/128]\tTime  0.976 ( 0.773)\tData  0.045 ( 0.054)\tLoss 3.5742e-01 (3.7272e-01)\tAcc@1  87.50 ( 86.15)\n",
      "[xla:4]Train:  Epoch: [10][ 60/128]\tTime  0.529 ( 0.767)\tData  0.057 ( 0.058)\tLoss 1.3965e-01 (3.5383e-01)\tAcc@1 100.00 ( 86.08)\n",
      "[xla:2]Train:  Epoch: [10][ 60/128]\tTime  0.680 ( 0.770)\tData  0.037 ( 0.055)\tLoss 1.8164e-01 (3.7929e-01)\tAcc@1  94.00 ( 85.74)\n",
      "[xla:3]Train:  Epoch: [10][ 60/128]\tTime  0.678 ( 0.769)\tData  0.076 ( 0.058)\tLoss 4.6289e-01 (3.8065e-01)\tAcc@1  81.00 ( 85.07)\n",
      "[xla:6]Train:  Epoch: [10][ 60/128]\tTime  0.761 ( 0.772)\tData  0.049 ( 0.056)\tLoss 2.2070e-01 (3.7921e-01)\tAcc@1  94.00 ( 85.80)\n",
      "[xla:5]Train:  Epoch: [10][ 60/128]\tTime  0.751 ( 0.779)\tData  0.046 ( 0.060)\tLoss 1.0107e-01 (3.5088e-01)\tAcc@1 100.00 ( 86.07)\n",
      "[xla:0]Train:  Epoch: [10][ 60/128]\tTime  0.730 ( 0.769)\tData  0.050 ( 0.057)\tLoss 4.0625e-01 (3.9873e-01)\tAcc@1  81.00 ( 83.61)\n",
      "[xla:7]Train:  Epoch: [10][ 60/128]\tTime  0.774 ( 0.774)\tData  0.043 ( 0.056)\tLoss 3.1250e-01 (3.8164e-01)\tAcc@1  87.50 ( 85.76)\n",
      "[xla:1]Train:  Epoch: [10][ 60/128]\tTime  0.760 ( 0.774)\tData  0.047 ( 0.058)\tLoss 6.7969e-01 (3.5875e-01)\tAcc@1  75.00 ( 86.07)\n",
      "[xla:4]Train:  Epoch: [10][ 90/128]\tTime  0.594 ( 0.766)\tData  0.037 ( 0.059)\tLoss 7.1094e-01 (3.7112e-01)\tAcc@1  81.00 ( 85.25)\n",
      "[xla:2]Train:  Epoch: [10][ 90/128]\tTime  0.556 ( 0.768)\tData  0.050 ( 0.056)\tLoss 5.8984e-01 (3.8871e-01)\tAcc@1  75.00 ( 85.64)\n",
      "[xla:3]Train:  Epoch: [10][ 90/128]\tTime  0.943 ( 0.768)\tData  0.081 ( 0.058)\tLoss 4.0039e-01 (3.9322e-01)\tAcc@1  75.00 ( 84.01)\n",
      "[xla:6]Train:  Epoch: [10][ 90/128]\tTime  0.711 ( 0.770)\tData  0.055 ( 0.057)\tLoss 2.3145e-01 (3.8190e-01)\tAcc@1  87.50 ( 85.82)\n",
      "[xla:5]Train:  Epoch: [10][ 90/128]\tTime  0.745 ( 0.774)\tData  0.053 ( 0.059)\tLoss 5.0391e-01 (3.5618e-01)\tAcc@1  75.00 ( 85.93)\n",
      "[xla:7]Train:  Epoch: [10][ 90/128]\tTime  0.801 ( 0.771)\tData  0.047 ( 0.057)\tLoss 2.9883e-01 (3.8406e-01)\tAcc@1  94.00 ( 85.65)\n",
      "[xla:1]Train:  Epoch: [10][ 90/128]\tTime  0.812 ( 0.771)\tData  0.045 ( 0.057)\tLoss 5.0781e-01 (3.5190e-01)\tAcc@1  75.00 ( 87.31)\n",
      "[xla:0]Train:  Epoch: [10][ 90/128]\tTime  0.764 ( 0.768)\tData  0.046 ( 0.060)\tLoss 3.2227e-01 (3.9528e-01)\tAcc@1  87.50 ( 84.41)\n",
      "[xla:3]Train:  Epoch: [10][120/128]\tTime  0.585 ( 0.768)\tData  0.038 ( 0.058)\tLoss 5.5469e-01 (3.9073e-01)\tAcc@1  69.00 ( 84.41)\n",
      "[xla:1]Train:  Epoch: [10][120/128]\tTime  0.691 ( 0.771)\tData  0.048 ( 0.058)\tLoss 4.1406e-01 (3.6784e-01)\tAcc@1  81.00 ( 86.85)\n",
      "[xla:5]Train:  Epoch: [10][120/128]\tTime  0.841 ( 0.775)\tData  0.058 ( 0.059)\tLoss 2.5391e-01 (3.7616e-01)\tAcc@1  87.50 ( 85.19)\n",
      "[xla:0]Train:  Epoch: [10][120/128]\tTime  0.714 ( 0.770)\tData  0.048 ( 0.059)\tLoss 1.8848e-01 (3.8732e-01)\tAcc@1  94.00 ( 84.76)\n",
      "[xla:7]Train:  Epoch: [10][120/128]\tTime  0.909 ( 0.772)\tData  0.083 ( 0.057)\tLoss 5.3906e-01 (3.9560e-01)\tAcc@1  81.00 ( 85.01)\n",
      "[xla:4]Train:  Epoch: [10][120/128]\tTime  0.764 ( 0.770)\tData  0.044 ( 0.059)\tLoss 5.0000e-01 (3.8155e-01)\tAcc@1  94.00 ( 85.34)\n",
      "[xla:6]Train:  Epoch: [10][120/128]\tTime  0.855 ( 0.772)\tData  0.044 ( 0.056)\tLoss 6.1328e-01 (3.7971e-01)\tAcc@1  87.50 ( 85.78)\n",
      "[xla:2]Train:  Epoch: [10][120/128]\tTime  0.767 ( 0.772)\tData  0.061 ( 0.057)\tLoss 2.1582e-01 (3.9535e-01)\tAcc@1  94.00 ( 84.61)\n",
      "Finished training epoch 10\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:0]Validation: [0/7]\tTime  0.298 ( 0.298)\tLoss 4.3164e-01 (4.3164e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.259 ( 0.259)\tLoss 5.6641e-01 (5.6641e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.200 ( 0.200)\tLoss 5.0781e-01 (5.0781e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.188 ( 0.188)\tLoss 3.0664e-01 (3.0664e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.236 ( 0.236)\tLoss 8.3984e-01 (8.3984e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.166 ( 0.166)\tLoss 3.9258e-01 (3.9258e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.279 ( 0.279)\tLoss 6.6016e-01 (6.6016e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.197 ( 0.197)\tLoss 4.1016e-01 (4.1016e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:2]Train:  Epoch: [11][  0/128]\tTime  0.637 ( 0.637)\tData  0.053 ( 0.053)\tLoss 4.9805e-01 (4.9805e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Train:  Epoch: [11][  0/128]\tTime  0.872 ( 0.872)\tData  0.102 ( 0.102)\tLoss 5.1953e-01 (5.1953e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:4]Train:  Epoch: [11][  0/128]\tTime  0.676 ( 0.676)\tData  0.068 ( 0.068)\tLoss 3.3008e-01 (3.3008e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Train:  Epoch: [11][  0/128]\tTime  0.681 ( 0.681)\tData  0.077 ( 0.077)\tLoss 3.1641e-01 (3.1641e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:7]Train:  Epoch: [11][  0/128]\tTime  0.697 ( 0.697)\tData  0.099 ( 0.099)\tLoss 4.8633e-01 (4.8633e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [11][  0/128]\tTime  0.595 ( 0.595)\tData  0.055 ( 0.055)\tLoss 3.7500e-01 (3.7500e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Train:  Epoch: [11][  0/128]\tTime  0.706 ( 0.706)\tData  0.091 ( 0.091)\tLoss 4.7070e-01 (4.7070e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Train:  Epoch: [11][  0/128]\tTime  0.366 ( 0.366)\tData  0.019 ( 0.019)\tLoss 3.5352e-01 (3.5352e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [11][ 30/128]\tTime  0.490 ( 0.754)\tData  0.045 ( 0.057)\tLoss 1.8945e-01 (3.3200e-01)\tAcc@1  94.00 ( 86.74)\n",
      "[xla:3]Train:  Epoch: [11][ 30/128]\tTime  0.621 ( 0.741)\tData  0.045 ( 0.056)\tLoss 2.9883e-01 (4.0994e-01)\tAcc@1  87.50 ( 83.68)\n",
      "[xla:0]Train:  Epoch: [11][ 30/128]\tTime  0.675 ( 0.772)\tData  0.045 ( 0.059)\tLoss 4.6875e-01 (4.2482e-01)\tAcc@1  87.50 ( 84.10)\n",
      "[xla:2]Train:  Epoch: [11][ 30/128]\tTime  0.763 ( 0.770)\tData  0.044 ( 0.059)\tLoss 2.4316e-01 (3.7101e-01)\tAcc@1  94.00 ( 86.15)\n",
      "[xla:6]Train:  Epoch: [11][ 30/128]\tTime  0.810 ( 0.763)\tData  0.045 ( 0.059)\tLoss 1.4062e-01 (3.5878e-01)\tAcc@1 100.00 ( 86.87)\n",
      "[xla:5]Train:  Epoch: [11][ 30/128]\tTime  0.721 ( 0.766)\tData  0.045 ( 0.062)\tLoss 1.8359e-01 (3.5840e-01)\tAcc@1 100.00 ( 83.27)\n",
      "[xla:7]Train:  Epoch: [11][ 30/128]\tTime  0.845 ( 0.765)\tData  0.049 ( 0.058)\tLoss 3.2812e-01 (3.7313e-01)\tAcc@1  87.50 ( 86.10)\n",
      "[xla:4]Train:  Epoch: [11][ 30/128]\tTime  0.968 ( 0.768)\tData  0.044 ( 0.057)\tLoss 3.7500e-01 (3.6596e-01)\tAcc@1  81.00 ( 86.68)\n",
      "[xla:1]Train:  Epoch: [11][ 60/128]\tTime  0.548 ( 0.766)\tData  0.067 ( 0.058)\tLoss 6.5234e-01 (3.4764e-01)\tAcc@1  69.00 ( 86.31)\n",
      "[xla:2]Train:  Epoch: [11][ 60/128]\tTime  0.668 ( 0.773)\tData  0.054 ( 0.058)\tLoss 2.2559e-01 (3.8004e-01)\tAcc@1  94.00 ( 85.67)\n",
      "[xla:7]Train:  Epoch: [11][ 60/128]\tTime  0.730 ( 0.771)\tData  0.064 ( 0.058)\tLoss 2.5781e-01 (3.7715e-01)\tAcc@1  87.50 ( 85.77)\n",
      "[xla:4]Train:  Epoch: [11][ 60/128]\tTime  0.781 ( 0.772)\tData  0.045 ( 0.059)\tLoss 1.0449e-01 (3.6070e-01)\tAcc@1 100.00 ( 86.99)\n",
      "[xla:6]Train:  Epoch: [11][ 60/128]\tTime  1.016 ( 0.770)\tData  0.111 ( 0.063)\tLoss 4.6289e-01 (3.6780e-01)\tAcc@1  87.50 ( 86.57)\n",
      "[xla:5]Train:  Epoch: [11][ 60/128]\tTime  0.788 ( 0.772)\tData  0.060 ( 0.060)\tLoss 1.8848e-01 (3.2496e-01)\tAcc@1  94.00 ( 86.71)\n",
      "[xla:0]Train:  Epoch: [11][ 60/128]\tTime  0.794 ( 0.776)\tData  0.074 ( 0.059)\tLoss 5.4688e-01 (4.0873e-01)\tAcc@1  75.00 ( 84.75)\n",
      "[xla:3]Train:  Epoch: [11][ 60/128]\tTime  0.786 ( 0.762)\tData  0.047 ( 0.059)\tLoss 6.0547e-01 (3.9377e-01)\tAcc@1  75.00 ( 83.92)\n",
      "[xla:7]Train:  Epoch: [11][ 90/128]\tTime  0.626 ( 0.767)\tData  0.035 ( 0.059)\tLoss 3.2031e-01 (3.7679e-01)\tAcc@1  87.50 ( 86.14)\n",
      "[xla:3]Train:  Epoch: [11][ 90/128]\tTime  0.650 ( 0.761)\tData  0.050 ( 0.058)\tLoss 2.2070e-01 (3.9649e-01)\tAcc@1  94.00 ( 84.13)\n",
      "[xla:4]Train:  Epoch: [11][ 90/128]\tTime  0.642 ( 0.769)\tData  0.068 ( 0.059)\tLoss 5.8984e-01 (3.6954e-01)\tAcc@1  75.00 ( 86.11)\n",
      "[xla:6]Train:  Epoch: [11][ 90/128]\tTime  0.890 ( 0.768)\tData  0.084 ( 0.062)\tLoss 1.5527e-01 (3.7879e-01)\tAcc@1  94.00 ( 85.71)\n",
      "[xla:1]Train:  Epoch: [11][ 90/128]\tTime  0.754 ( 0.768)\tData  0.046 ( 0.058)\tLoss 5.7812e-01 (3.4898e-01)\tAcc@1  75.00 ( 86.98)\n",
      "[xla:5]Train:  Epoch: [11][ 90/128]\tTime  0.713 ( 0.769)\tData  0.054 ( 0.059)\tLoss 4.3750e-01 (3.3777e-01)\tAcc@1  75.00 ( 86.01)\n",
      "[xla:0]Train:  Epoch: [11][ 90/128]\tTime  0.724 ( 0.772)\tData  0.054 ( 0.059)\tLoss 4.3555e-01 (4.1004e-01)\tAcc@1  81.00 ( 84.40)\n",
      "[xla:2]Train:  Epoch: [11][ 90/128]\tTime  0.924 ( 0.771)\tData  0.090 ( 0.057)\tLoss 3.2422e-01 (3.7781e-01)\tAcc@1  87.50 ( 85.95)\n",
      "[xla:0]Train:  Epoch: [11][120/128]\tTime  0.553 ( 0.767)\tData  0.064 ( 0.060)\tLoss 2.8125e-01 (4.1252e-01)\tAcc@1  87.50 ( 84.05)\n",
      "[xla:6]Train:  Epoch: [11][120/128]\tTime  0.664 ( 0.765)\tData  0.038 ( 0.061)\tLoss 6.7578e-01 (3.7611e-01)\tAcc@1  81.00 ( 85.97)\n",
      "[xla:3]Train:  Epoch: [11][120/128]\tTime  0.844 ( 0.761)\tData  0.082 ( 0.058)\tLoss 5.6250e-01 (3.9553e-01)\tAcc@1  81.00 ( 84.46)\n",
      "[xla:4]Train:  Epoch: [11][120/128]\tTime  0.722 ( 0.767)\tData  0.064 ( 0.058)\tLoss 6.0547e-01 (3.7839e-01)\tAcc@1  81.00 ( 85.85)\n",
      "[xla:1]Train:  Epoch: [11][120/128]\tTime  0.735 ( 0.766)\tData  0.048 ( 0.058)\tLoss 4.6875e-01 (3.6101e-01)\tAcc@1  81.00 ( 86.29)\n",
      "[xla:7]Train:  Epoch: [11][120/128]\tTime  0.752 ( 0.767)\tData  0.047 ( 0.058)\tLoss 3.7891e-01 (3.8984e-01)\tAcc@1  87.50 ( 85.34)\n",
      "[xla:2]Train:  Epoch: [11][120/128]\tTime  0.801 ( 0.768)\tData  0.037 ( 0.057)\tLoss 2.0996e-01 (3.8577e-01)\tAcc@1  87.50 ( 85.50)\n",
      "[xla:5]Train:  Epoch: [11][120/128]\tTime  0.778 ( 0.767)\tData  0.077 ( 0.059)\tLoss 2.0605e-01 (3.5446e-01)\tAcc@1  94.00 ( 85.61)\n",
      "Finished training epoch 11\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:4]Validation: [0/7]\tTime  0.279 ( 0.279)\tLoss 4.0625e-01 (4.0625e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Validation: [0/7]\tTime  0.310 ( 0.310)\tLoss 4.3750e-01 (4.3750e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.222 ( 0.222)\tLoss 6.6406e-01 (6.6406e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.267 ( 0.267)\tLoss 9.0625e-01 (9.0625e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.195 ( 0.195)\tLoss 5.2344e-01 (5.2344e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.255 ( 0.255)\tLoss 4.4141e-01 (4.4141e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.242 ( 0.242)\tLoss 5.8594e-01 (5.8594e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.237 ( 0.237)\tLoss 3.0664e-01 (3.0664e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:4]Train:  Epoch: [12][  0/128]\tTime  0.904 ( 0.904)\tData  0.058 ( 0.058)\tLoss 1.6895e-01 (1.6895e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:5]Train:  Epoch: [12][  0/128]\tTime  0.943 ( 0.943)\tData  0.048 ( 0.048)\tLoss 2.3047e-01 (2.3047e-01)\tAcc@1 100.00 (100.00)\n",
      "[xla:3]Train:  Epoch: [12][  0/128]\tTime  0.938 ( 0.938)\tData  0.052 ( 0.052)\tLoss 3.4180e-01 (3.4180e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [12][  0/128]\tTime  0.975 ( 0.975)\tData  0.040 ( 0.040)\tLoss 4.1211e-01 (4.1211e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:2]Train:  Epoch: [12][  0/128]\tTime  0.772 ( 0.772)\tData  0.209 ( 0.209)\tLoss 4.3945e-01 (4.3945e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:7]Train:  Epoch: [12][  0/128]\tTime  0.898 ( 0.898)\tData  0.195 ( 0.195)\tLoss 3.5547e-01 (3.5547e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Train:  Epoch: [12][  0/128]\tTime  0.951 ( 0.951)\tData  0.182 ( 0.182)\tLoss 5.0781e-01 (5.0781e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Train:  Epoch: [12][  0/128]\tTime  0.824 ( 0.824)\tData  0.111 ( 0.111)\tLoss 5.1562e-01 (5.1562e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:0]Train:  Epoch: [12][ 30/128]\tTime  0.505 ( 0.774)\tData  0.039 ( 0.055)\tLoss 3.3008e-01 (4.5253e-01)\tAcc@1  87.50 ( 82.85)\n",
      "[xla:6]Train:  Epoch: [12][ 30/128]\tTime  0.720 ( 0.783)\tData  0.055 ( 0.058)\tLoss 1.8359e-01 (3.7546e-01)\tAcc@1  94.00 ( 86.52)\n",
      "[xla:5]Train:  Epoch: [12][ 30/128]\tTime  0.860 ( 0.792)\tData  0.061 ( 0.064)\tLoss 1.7871e-01 (3.5462e-01)\tAcc@1 100.00 ( 84.27)\n",
      "[xla:4]Train:  Epoch: [12][ 30/128]\tTime  0.797 ( 0.794)\tData  0.036 ( 0.062)\tLoss 3.8672e-01 (3.6517e-01)\tAcc@1  81.00 ( 86.03)\n",
      "[xla:3]Train:  Epoch: [12][ 30/128]\tTime  0.775 ( 0.792)\tData  0.044 ( 0.064)\tLoss 3.2227e-01 (4.2772e-01)\tAcc@1  87.50 ( 83.81)\n",
      "[xla:2]Train:  Epoch: [12][ 30/128]\tTime  0.778 ( 0.781)\tData  0.047 ( 0.063)\tLoss 4.1992e-01 (3.8162e-01)\tAcc@1  81.00 ( 87.52)\n",
      "[xla:7]Train:  Epoch: [12][ 30/128]\tTime  0.967 ( 0.785)\tData  0.082 ( 0.064)\tLoss 3.2422e-01 (3.7941e-01)\tAcc@1  87.50 ( 85.52)\n",
      "[xla:1]Train:  Epoch: [12][ 30/128]\tTime  0.787 ( 0.789)\tData  0.048 ( 0.059)\tLoss 1.9238e-01 (3.3926e-01)\tAcc@1  94.00 ( 86.32)\n",
      "[xla:2]Train:  Epoch: [12][ 60/128]\tTime  0.652 ( 0.766)\tData  0.040 ( 0.059)\tLoss 3.0469e-01 (3.8997e-01)\tAcc@1  87.50 ( 86.48)\n",
      "[xla:3]Train:  Epoch: [12][ 60/128]\tTime  0.674 ( 0.773)\tData  0.045 ( 0.059)\tLoss 5.6641e-01 (3.7819e-01)\tAcc@1  75.00 ( 85.83)\n",
      "[xla:4]Train:  Epoch: [12][ 60/128]\tTime  0.700 ( 0.775)\tData  0.058 ( 0.058)\tLoss 2.0801e-01 (3.4113e-01)\tAcc@1  94.00 ( 87.29)\n",
      "[xla:6]Train:  Epoch: [12][ 60/128]\tTime  0.735 ( 0.770)\tData  0.085 ( 0.056)\tLoss 2.6953e-01 (3.8602e-01)\tAcc@1  87.50 ( 84.96)\n",
      "[xla:0]Train:  Epoch: [12][ 60/128]\tTime  0.801 ( 0.768)\tData  0.038 ( 0.057)\tLoss 4.8828e-01 (4.1525e-01)\tAcc@1  75.00 ( 83.73)\n",
      "[xla:5]Train:  Epoch: [12][ 60/128]\tTime  0.774 ( 0.775)\tData  0.056 ( 0.060)\tLoss 8.9844e-02 (3.3681e-01)\tAcc@1 100.00 ( 85.65)\n",
      "[xla:1]Train:  Epoch: [12][ 60/128]\tTime  0.779 ( 0.773)\tData  0.068 ( 0.058)\tLoss 8.3984e-01 (3.4806e-01)\tAcc@1  69.00 ( 86.59)\n",
      "[xla:7]Train:  Epoch: [12][ 60/128]\tTime  0.787 ( 0.771)\tData  0.059 ( 0.062)\tLoss 1.6113e-01 (3.7701e-01)\tAcc@1 100.00 ( 85.05)\n",
      "[xla:5]Train:  Epoch: [12][ 90/128]\tTime  0.643 ( 0.774)\tData  0.041 ( 0.059)\tLoss 3.3594e-01 (3.3534e-01)\tAcc@1  94.00 ( 86.04)\n",
      "[xla:6]Train:  Epoch: [12][ 90/128]\tTime  0.673 ( 0.772)\tData  0.040 ( 0.057)\tLoss 2.9492e-01 (3.9329e-01)\tAcc@1  87.50 ( 84.20)\n",
      "[xla:3]Train:  Epoch: [12][ 90/128]\tTime  0.681 ( 0.775)\tData  0.076 ( 0.060)\tLoss 2.9492e-01 (3.8420e-01)\tAcc@1  87.50 ( 85.29)\n",
      "[xla:0]Train:  Epoch: [12][ 90/128]\tTime  0.717 ( 0.771)\tData  0.050 ( 0.059)\tLoss 3.7109e-01 (3.9852e-01)\tAcc@1  81.00 ( 84.84)\n",
      "[xla:2]Train:  Epoch: [12][ 90/128]\tTime  0.933 ( 0.772)\tData  0.077 ( 0.061)\tLoss 4.2188e-01 (3.9056e-01)\tAcc@1  75.00 ( 85.79)\n",
      "[xla:7]Train:  Epoch: [12][ 90/128]\tTime  0.779 ( 0.773)\tData  0.062 ( 0.061)\tLoss 4.0625e-01 (3.7515e-01)\tAcc@1  87.50 ( 85.09)\n",
      "[xla:4]Train:  Epoch: [12][ 90/128]\tTime  1.041 ( 0.777)\tData  0.125 ( 0.060)\tLoss 7.0703e-01 (3.5223e-01)\tAcc@1  75.00 ( 86.61)\n",
      "[xla:1]Train:  Epoch: [12][ 90/128]\tTime  0.825 ( 0.775)\tData  0.050 ( 0.057)\tLoss 5.1562e-01 (3.4635e-01)\tAcc@1  75.00 ( 86.82)\n",
      "[xla:4]Train:  Epoch: [12][120/128]\tTime  0.673 ( 0.771)\tData  0.038 ( 0.059)\tLoss 6.0156e-01 (3.7389e-01)\tAcc@1  75.00 ( 85.60)\n",
      "[xla:2]Train:  Epoch: [12][120/128]\tTime  0.696 ( 0.769)\tData  0.056 ( 0.060)\tLoss 2.4609e-01 (4.0373e-01)\tAcc@1  87.50 ( 84.87)\n",
      "[xla:6]Train:  Epoch: [12][120/128]\tTime  0.878 ( 0.770)\tData  0.061 ( 0.057)\tLoss 4.8242e-01 (3.8780e-01)\tAcc@1  87.50 ( 84.82)\n",
      "[xla:5]Train:  Epoch: [12][120/128]\tTime  0.767 ( 0.772)\tData  0.062 ( 0.059)\tLoss 2.0312e-01 (3.6458e-01)\tAcc@1  94.00 ( 85.06)\n",
      "[xla:0]Train:  Epoch: [12][120/128]\tTime  0.767 ( 0.769)\tData  0.050 ( 0.059)\tLoss 2.5586e-01 (3.9943e-01)\tAcc@1  94.00 ( 84.52)\n",
      "[xla:7]Train:  Epoch: [12][120/128]\tTime  0.941 ( 0.770)\tData  0.086 ( 0.061)\tLoss 3.3203e-01 (3.9440e-01)\tAcc@1  87.50 ( 84.36)\n",
      "[xla:3]Train:  Epoch: [12][120/128]\tTime  0.787 ( 0.772)\tData  0.073 ( 0.060)\tLoss 4.8242e-01 (3.8412e-01)\tAcc@1  75.00 ( 85.38)\n",
      "[xla:1]Train:  Epoch: [12][120/128]\tTime  0.773 ( 0.771)\tData  0.069 ( 0.057)\tLoss 4.3164e-01 (3.6236e-01)\tAcc@1  75.00 ( 86.26)\n",
      "[xla:1]Validation: [0/7]\tTime  0.315 ( 0.315)\tLoss 6.6797e-01 (6.6797e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Validation: [0/7]\tTime  0.159 ( 0.159)\tLoss 8.7109e-01 (8.7109e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 12\n",
      "[xla:7]Validation: [0/7]\tTime  0.185 ( 0.185)\tLoss 3.0664e-01 (3.0664e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:3]Validation: [0/7]\tTime  0.182 ( 0.182)\tLoss 4.1602e-01 (4.1602e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:2]Validation: [0/7]\tTime  0.205 ( 0.205)\tLoss 5.7812e-01 (5.7812e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.243 ( 0.243)\tLoss 3.9844e-01 (3.9844e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Validation: [0/7]\tTime  0.222 ( 0.222)\tLoss 5.0391e-01 (5.0391e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.181 ( 0.181)\tLoss 4.2383e-01 (4.2383e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:7]Train:  Epoch: [13][  0/128]\tTime  0.803 ( 0.803)\tData  0.075 ( 0.075)\tLoss 5.7031e-01 (5.7031e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Train:  Epoch: [13][  0/128]\tTime  0.665 ( 0.665)\tData  0.104 ( 0.104)\tLoss 4.5117e-01 (4.5117e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:5]Train:  Epoch: [13][  0/128]\tTime  0.862 ( 0.862)\tData  0.061 ( 0.061)\tLoss 2.6367e-01 (2.6367e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:3]Train:  Epoch: [13][  0/128]\tTime  0.691 ( 0.691)\tData  0.059 ( 0.059)\tLoss 3.7109e-01 (3.7109e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:1]Train:  Epoch: [13][  0/128]\tTime  0.729 ( 0.729)\tData  0.072 ( 0.072)\tLoss 2.7539e-01 (2.7539e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:0]Train:  Epoch: [13][  0/128]\tTime  0.537 ( 0.537)\tData  0.029 ( 0.029)\tLoss 5.3516e-01 (5.3516e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Train:  Epoch: [13][  0/128]\tTime  0.459 ( 0.459)\tData  0.067 ( 0.067)\tLoss 6.0156e-01 (6.0156e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:4]Train:  Epoch: [13][  0/128]\tTime  0.521 ( 0.521)\tData  0.062 ( 0.062)\tLoss 1.8066e-01 (1.8066e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:4]Train:  Epoch: [13][ 30/128]\tTime  0.473 ( 0.753)\tData  0.043 ( 0.050)\tLoss 2.4805e-01 (3.5033e-01)\tAcc@1  94.00 ( 86.31)\n",
      "[xla:1]Train:  Epoch: [13][ 30/128]\tTime  0.638 ( 0.764)\tData  0.041 ( 0.055)\tLoss 1.4941e-01 (3.3233e-01)\tAcc@1  94.00 ( 87.90)\n",
      "[xla:2]Train:  Epoch: [13][ 30/128]\tTime  0.540 ( 0.766)\tData  0.046 ( 0.059)\tLoss 4.1602e-01 (3.6854e-01)\tAcc@1  87.50 ( 86.50)\n",
      "[xla:7]Train:  Epoch: [13][ 30/128]\tTime  0.698 ( 0.775)\tData  0.050 ( 0.054)\tLoss 3.0664e-01 (4.1088e-01)\tAcc@1  94.00 ( 85.53)\n",
      "[xla:0]Train:  Epoch: [13][ 30/128]\tTime  0.713 ( 0.761)\tData  0.047 ( 0.054)\tLoss 3.7891e-01 (4.2786e-01)\tAcc@1  94.00 ( 83.74)\n",
      "[xla:6]Train:  Epoch: [13][ 30/128]\tTime  0.780 ( 0.759)\tData  0.044 ( 0.057)\tLoss 2.2559e-01 (3.7480e-01)\tAcc@1  81.00 ( 84.65)\n",
      "[xla:5]Train:  Epoch: [13][ 30/128]\tTime  0.805 ( 0.778)\tData  0.045 ( 0.055)\tLoss 2.1875e-01 (3.5002e-01)\tAcc@1 100.00 ( 85.94)\n",
      "[xla:3]Train:  Epoch: [13][ 30/128]\tTime  0.936 ( 0.772)\tData  0.057 ( 0.057)\tLoss 2.2461e-01 (4.2994e-01)\tAcc@1  94.00 ( 83.25)\n",
      "[xla:1]Train:  Epoch: [13][ 60/128]\tTime  0.657 ( 0.773)\tData  0.040 ( 0.056)\tLoss 5.4297e-01 (3.3595e-01)\tAcc@1  75.00 ( 87.61)\n",
      "[xla:6]Train:  Epoch: [13][ 60/128]\tTime  0.679 ( 0.767)\tData  0.037 ( 0.058)\tLoss 2.6953e-01 (3.7899e-01)\tAcc@1  87.50 ( 84.54)\n",
      "[xla:2]Train:  Epoch: [13][ 60/128]\tTime  0.664 ( 0.775)\tData  0.081 ( 0.061)\tLoss 2.7734e-01 (3.8397e-01)\tAcc@1  87.50 ( 86.16)\n",
      "[xla:7]Train:  Epoch: [13][ 60/128]\tTime  0.709 ( 0.778)\tData  0.078 ( 0.056)\tLoss 2.5000e-01 (3.9139e-01)\tAcc@1  87.50 ( 85.48)\n",
      "[xla:0]Train:  Epoch: [13][ 60/128]\tTime  0.746 ( 0.771)\tData  0.057 ( 0.056)\tLoss 6.4844e-01 (3.9674e-01)\tAcc@1  75.00 ( 85.09)\n",
      "[xla:3]Train:  Epoch: [13][ 60/128]\tTime  0.782 ( 0.776)\tData  0.094 ( 0.061)\tLoss 4.6484e-01 (3.7021e-01)\tAcc@1  81.00 ( 86.06)\n",
      "[xla:5]Train:  Epoch: [13][ 60/128]\tTime  0.947 ( 0.779)\tData  0.074 ( 0.057)\tLoss 9.4727e-02 (3.3325e-01)\tAcc@1 100.00 ( 86.20)\n",
      "[xla:4]Train:  Epoch: [13][ 60/128]\tTime  0.797 ( 0.771)\tData  0.070 ( 0.055)\tLoss 2.0117e-01 (3.4605e-01)\tAcc@1  94.00 ( 86.43)\n",
      "[xla:5]Train:  Epoch: [13][ 90/128]\tTime  0.621 ( 0.773)\tData  0.043 ( 0.058)\tLoss 5.0000e-01 (3.5223e-01)\tAcc@1  75.00 ( 85.26)\n",
      "[xla:0]Train:  Epoch: [13][ 90/128]\tTime  0.722 ( 0.769)\tData  0.048 ( 0.057)\tLoss 4.5312e-01 (3.9490e-01)\tAcc@1  81.00 ( 84.99)\n",
      "[xla:7]Train:  Epoch: [13][ 90/128]\tTime  0.767 ( 0.775)\tData  0.038 ( 0.057)\tLoss 2.8516e-01 (3.9231e-01)\tAcc@1  94.00 ( 84.97)\n",
      "[xla:2]Train:  Epoch: [13][ 90/128]\tTime  0.759 ( 0.773)\tData  0.061 ( 0.059)\tLoss 4.4727e-01 (3.9090e-01)\tAcc@1  81.00 ( 85.56)\n",
      "[xla:6]Train:  Epoch: [13][ 90/128]\tTime  0.760 ( 0.769)\tData  0.054 ( 0.057)\tLoss 2.4219e-01 (3.8715e-01)\tAcc@1  94.00 ( 84.15)\n",
      "[xla:1]Train:  Epoch: [13][ 90/128]\tTime  0.750 ( 0.773)\tData  0.046 ( 0.058)\tLoss 6.6016e-01 (3.4042e-01)\tAcc@1  75.00 ( 87.78)\n",
      "[xla:3]Train:  Epoch: [13][ 90/128]\tTime  0.990 ( 0.773)\tData  0.057 ( 0.059)\tLoss 2.1582e-01 (3.8146e-01)\tAcc@1  94.00 ( 85.85)\n",
      "[xla:4]Train:  Epoch: [13][ 90/128]\tTime  0.882 ( 0.770)\tData  0.040 ( 0.056)\tLoss 6.9922e-01 (3.5169e-01)\tAcc@1  75.00 ( 86.13)\n",
      "[xla:6]Train:  Epoch: [13][120/128]\tTime  0.674 ( 0.771)\tData  0.035 ( 0.058)\tLoss 4.6680e-01 (3.8248e-01)\tAcc@1  87.50 ( 84.62)\n",
      "[xla:0]Train:  Epoch: [13][120/128]\tTime  0.712 ( 0.772)\tData  0.074 ( 0.056)\tLoss 2.4805e-01 (3.9741e-01)\tAcc@1  94.00 ( 84.54)\n",
      "[xla:1]Train:  Epoch: [13][120/128]\tTime  0.754 ( 0.775)\tData  0.042 ( 0.058)\tLoss 5.4297e-01 (3.5913e-01)\tAcc@1  75.00 ( 86.84)\n",
      "[xla:5]Train:  Epoch: [13][120/128]\tTime  0.903 ( 0.777)\tData  0.065 ( 0.059)\tLoss 3.2227e-01 (3.8291e-01)\tAcc@1  81.00 ( 84.22)\n",
      "[xla:2]Train:  Epoch: [13][120/128]\tTime  0.738 ( 0.775)\tData  0.060 ( 0.060)\tLoss 3.5352e-01 (3.9526e-01)\tAcc@1  94.00 ( 85.26)\n",
      "[xla:7]Train:  Epoch: [13][120/128]\tTime  0.734 ( 0.777)\tData  0.050 ( 0.059)\tLoss 4.5703e-01 (4.0950e-01)\tAcc@1  87.50 ( 84.62)\n",
      "[xla:3]Train:  Epoch: [13][120/128]\tTime  0.780 ( 0.775)\tData  0.061 ( 0.059)\tLoss 5.5078e-01 (3.8339e-01)\tAcc@1  87.50 ( 85.80)\n",
      "[xla:4]Train:  Epoch: [13][120/128]\tTime  0.775 ( 0.773)\tData  0.047 ( 0.056)\tLoss 5.7422e-01 (3.7018e-01)\tAcc@1  81.00 ( 86.00)\n",
      "Finished training epoch 13\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:2]Validation: [0/7]\tTime  0.364 ( 0.364)\tLoss 5.2734e-01 (5.2734e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:0]Validation: [0/7]\tTime  0.418 ( 0.418)\tLoss 4.0234e-01 (4.0234e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:4]Validation: [0/7]\tTime  0.239 ( 0.239)\tLoss 3.6719e-01 (3.6719e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:6]Validation: [0/7]\tTime  0.210 ( 0.210)\tLoss 4.9609e-01 (4.9609e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:7]Validation: [0/7]\tTime  0.269 ( 0.269)\tLoss 3.0664e-01 (3.0664e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:5]Validation: [0/7]\tTime  0.236 ( 0.236)\tLoss 8.4375e-01 (8.4375e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:1]Validation: [0/7]\tTime  0.180 ( 0.180)\tLoss 6.7188e-01 (6.7188e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.194 ( 0.194)\tLoss 3.7500e-01 (3.7500e-01)\tAcc@1  87.50 ( 87.50)\n",
      "Parallel Loader Created. Training ...\n",
      "[xla:0]Train:  Epoch: [14][  0/128]\tTime  0.580 ( 0.580)\tData  0.023 ( 0.023)\tLoss 4.0039e-01 (4.0039e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:2]Train:  Epoch: [14][  0/128]\tTime  0.648 ( 0.648)\tData  0.058 ( 0.058)\tLoss 6.1719e-01 (6.1719e-01)\tAcc@1  69.00 ( 69.00)\n",
      "[xla:7]Train:  Epoch: [14][  0/128]\tTime  0.527 ( 0.527)\tData  0.006 ( 0.006)\tLoss 2.4609e-01 (2.4609e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:4]Train:  Epoch: [14][  0/128]\tTime  0.529 ( 0.529)\tData  0.049 ( 0.049)\tLoss 1.7969e-01 (1.7969e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:6]Train:  Epoch: [14][  0/128]\tTime  0.572 ( 0.572)\tData  0.078 ( 0.078)\tLoss 5.1172e-01 (5.1172e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:1]Train:  Epoch: [14][  0/128]\tTime  0.597 ( 0.597)\tData  0.057 ( 0.057)\tLoss 4.2383e-01 (4.2383e-01)\tAcc@1  94.00 ( 94.00)\n",
      "[xla:5]Train:  Epoch: [14][  0/128]\tTime  0.552 ( 0.552)\tData  0.081 ( 0.081)\tLoss 2.9883e-01 (2.9883e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Train:  Epoch: [14][  0/128]\tTime  0.357 ( 0.357)\tData  0.019 ( 0.019)\tLoss 2.4414e-01 (2.4414e-01)\tAcc@1 100.00 (100.00)\n",
      "[xla:7]Train:  Epoch: [14][ 30/128]\tTime  0.668 ( 0.755)\tData  0.036 ( 0.057)\tLoss 2.9883e-01 (3.9292e-01)\tAcc@1  87.50 ( 83.65)\n",
      "[xla:4]Train:  Epoch: [14][ 30/128]\tTime  0.710 ( 0.755)\tData  0.096 ( 0.057)\tLoss 4.4141e-01 (3.4243e-01)\tAcc@1  81.00 ( 86.94)\n",
      "[xla:0]Train:  Epoch: [14][ 30/128]\tTime  0.784 ( 0.770)\tData  0.045 ( 0.062)\tLoss 4.6094e-01 (3.8965e-01)\tAcc@1  87.50 ( 86.48)\n",
      "[xla:2]Train:  Epoch: [14][ 30/128]\tTime  0.756 ( 0.772)\tData  0.062 ( 0.057)\tLoss 2.8125e-01 (3.8527e-01)\tAcc@1  87.50 ( 84.68)\n",
      "[xla:3]Train:  Epoch: [14][ 30/128]\tTime  0.772 ( 0.745)\tData  0.069 ( 0.055)\tLoss 3.4180e-01 (4.0255e-01)\tAcc@1  87.50 ( 84.69)\n",
      "[xla:6]Train:  Epoch: [14][ 30/128]\tTime  0.765 ( 0.757)\tData  0.067 ( 0.057)\tLoss 1.7773e-01 (3.4884e-01)\tAcc@1 100.00 ( 87.52)\n",
      "[xla:5]Train:  Epoch: [14][ 30/128]\tTime  0.776 ( 0.755)\tData  0.079 ( 0.061)\tLoss 2.7344e-01 (3.3061e-01)\tAcc@1  94.00 ( 86.53)\n",
      "[xla:1]Train:  Epoch: [14][ 30/128]\tTime  0.824 ( 0.759)\tData  0.058 ( 0.058)\tLoss 1.4355e-01 (3.3950e-01)\tAcc@1  94.00 ( 86.27)\n",
      "[xla:7]Train:  Epoch: [14][ 60/128]\tTime  0.632 ( 0.765)\tData  0.035 ( 0.058)\tLoss 2.3438e-01 (3.8648e-01)\tAcc@1  87.50 ( 84.52)\n",
      "[xla:4]Train:  Epoch: [14][ 60/128]\tTime  0.627 ( 0.765)\tData  0.090 ( 0.058)\tLoss 1.2988e-01 (3.4992e-01)\tAcc@1  94.00 ( 86.11)\n",
      "[xla:5]Train:  Epoch: [14][ 60/128]\tTime  0.743 ( 0.764)\tData  0.065 ( 0.060)\tLoss 1.0010e-01 (3.2766e-01)\tAcc@1 100.00 ( 86.61)\n",
      "[xla:6]Train:  Epoch: [14][ 60/128]\tTime  0.749 ( 0.766)\tData  0.068 ( 0.059)\tLoss 2.9297e-01 (3.5484e-01)\tAcc@1  81.00 ( 85.44)\n",
      "[xla:0]Train:  Epoch: [14][ 60/128]\tTime  0.733 ( 0.773)\tData  0.072 ( 0.061)\tLoss 8.0859e-01 (3.9270e-01)\tAcc@1  62.50 ( 85.57)\n",
      "[xla:2]Train:  Epoch: [14][ 60/128]\tTime  0.786 ( 0.775)\tData  0.047 ( 0.057)\tLoss 1.3574e-01 (3.9253e-01)\tAcc@1 100.00 ( 84.30)\n",
      "[xla:1]Train:  Epoch: [14][ 60/128]\tTime  0.826 ( 0.768)\tData  0.104 ( 0.060)\tLoss 6.2891e-01 (3.4578e-01)\tAcc@1  75.00 ( 86.58)\n",
      "[xla:3]Train:  Epoch: [14][ 60/128]\tTime  0.811 ( 0.761)\tData  0.097 ( 0.057)\tLoss 5.0000e-01 (3.8232e-01)\tAcc@1  81.00 ( 85.04)\n",
      "[xla:2]Train:  Epoch: [14][ 90/128]\tTime  0.632 ( 0.770)\tData  0.044 ( 0.057)\tLoss 5.5859e-01 (3.8007e-01)\tAcc@1  81.00 ( 84.66)\n",
      "[xla:4]Train:  Epoch: [14][ 90/128]\tTime  0.689 ( 0.765)\tData  0.044 ( 0.058)\tLoss 6.1328e-01 (3.6581e-01)\tAcc@1  75.00 ( 85.19)\n",
      "[xla:5]Train:  Epoch: [14][ 90/128]\tTime  0.756 ( 0.764)\tData  0.052 ( 0.059)\tLoss 3.2617e-01 (3.4500e-01)\tAcc@1  87.50 ( 86.56)\n",
      "[xla:6]Train:  Epoch: [14][ 90/128]\tTime  0.802 ( 0.765)\tData  0.061 ( 0.060)\tLoss 1.3672e-01 (3.7310e-01)\tAcc@1 100.00 ( 84.68)\n",
      "[xla:7]Train:  Epoch: [14][ 90/128]\tTime  0.798 ( 0.767)\tData  0.045 ( 0.058)\tLoss 2.3438e-01 (3.8089e-01)\tAcc@1  87.50 ( 85.45)\n",
      "[xla:3]Train:  Epoch: [14][ 90/128]\tTime  0.733 ( 0.761)\tData  0.045 ( 0.057)\tLoss 2.1973e-01 (3.9227e-01)\tAcc@1  87.50 ( 84.46)\n",
      "[xla:0]Train:  Epoch: [14][ 90/128]\tTime  0.860 ( 0.770)\tData  0.041 ( 0.061)\tLoss 2.7930e-01 (3.8606e-01)\tAcc@1  87.50 ( 85.80)\n",
      "[xla:1]Train:  Epoch: [14][ 90/128]\tTime  0.767 ( 0.766)\tData  0.046 ( 0.060)\tLoss 5.6250e-01 (3.4108e-01)\tAcc@1  75.00 ( 86.91)\n",
      "[xla:1]Train:  Epoch: [14][120/128]\tTime  0.662 ( 0.764)\tData  0.046 ( 0.060)\tLoss 4.1406e-01 (3.6157e-01)\tAcc@1  87.50 ( 86.29)\n",
      "[xla:7]Train:  Epoch: [14][120/128]\tTime  0.619 ( 0.765)\tData  0.037 ( 0.058)\tLoss 6.5234e-01 (3.9139e-01)\tAcc@1  81.00 ( 84.81)\n",
      "[xla:2]Train:  Epoch: [14][120/128]\tTime  0.605 ( 0.768)\tData  0.052 ( 0.057)\tLoss 3.3984e-01 (3.9232e-01)\tAcc@1  94.00 ( 84.33)\n",
      "[xla:4]Train:  Epoch: [14][120/128]\tTime  0.696 ( 0.765)\tData  0.044 ( 0.058)\tLoss 7.9688e-01 (3.8247e-01)\tAcc@1  62.50 ( 84.71)\n",
      "[xla:3]Train:  Epoch: [14][120/128]\tTime  0.852 ( 0.762)\tData  0.065 ( 0.057)\tLoss 6.0547e-01 (3.9223e-01)\tAcc@1  81.00 ( 84.84)\n",
      "[xla:6]Train:  Epoch: [14][120/128]\tTime  0.749 ( 0.765)\tData  0.080 ( 0.060)\tLoss 5.2734e-01 (3.6723e-01)\tAcc@1  87.50 ( 85.48)\n",
      "[xla:0]Train:  Epoch: [14][120/128]\tTime  0.891 ( 0.769)\tData  0.070 ( 0.060)\tLoss 3.1641e-01 (3.9592e-01)\tAcc@1  87.50 ( 84.94)\n",
      "[xla:5]Train:  Epoch: [14][120/128]\tTime  0.749 ( 0.765)\tData  0.069 ( 0.059)\tLoss 3.8867e-01 (3.6911e-01)\tAcc@1  87.50 ( 85.65)\n",
      "[xla:4]Validation: [0/7]\tTime  0.321 ( 0.321)\tLoss 3.7305e-01 (3.7305e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:5]Validation: [0/7]\tTime  0.195 ( 0.195)\tLoss 8.2422e-01 (8.2422e-01)\tAcc@1  69.00 ( 69.00)\n",
      "Finished training epoch 14\n",
      "[xla:7]Validation: [0/7]\tTime  0.416 ( 0.416)\tLoss 3.0078e-01 (3.0078e-01)\tAcc@1  87.50 ( 87.50)\n",
      "Parallel Loader Created. Validating ...\n",
      "[xla:1]Validation: [0/7]\tTime  0.369 ( 0.369)\tLoss 6.7969e-01 (6.7969e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:6]Validation: [0/7]\tTime  0.257 ( 0.257)\tLoss 5.0000e-01 (5.0000e-01)\tAcc@1  75.00 ( 75.00)\n",
      "[xla:2]Validation: [0/7]\tTime  0.327 ( 0.327)\tLoss 5.0781e-01 (5.0781e-01)\tAcc@1  81.00 ( 81.00)\n",
      "[xla:3]Validation: [0/7]\tTime  0.334 ( 0.334)\tLoss 3.7695e-01 (3.7695e-01)\tAcc@1  87.50 ( 87.50)\n",
      "[xla:0]Validation: [0/7]\tTime  0.281 ( 0.281)\tLoss 4.0625e-01 (4.0625e-01)\tAcc@1  94.00 ( 94.00)\n",
      "Saving Model ..\n",
      "Model Saved.\n",
      "Metric: CompileTime\n",
      "  TotalSamples: 12\n",
      "  Accumulator: 03m30s639ms811.989us\n",
      "  ValueRate: 120ms354.876us / second\n",
      "  Rate: 0.00688927 / second\n",
      "  Percentiles: 1%=009ms744.346us; 5%=009ms744.346us; 10%=025ms268.793us; 20%=026ms937.050us; 50%=08s436ms845.003us; 80%=20s416ms301.245us; 90%=01m10s325ms555.284us; 95%=01m13s365ms176.885us; 99%=01m13s365ms176.885us\n",
      "Metric: DeviceLockWait\n",
      "  TotalSamples: 6252\n",
      "  Accumulator: 391ms260.307us\n",
      "  ValueRate: 001ms087.104us / second\n",
      "  Rate: 4.47971 / second\n",
      "  Percentiles: 1%=002.487us; 5%=002.902us; 10%=003.094us; 20%=003.268us; 50%=003.795us; 80%=004.363us; 90%=004.644us; 95%=004.857us; 99%=013.445us\n",
      "Metric: ExecuteTime\n",
      "  TotalSamples: 6222\n",
      "  Accumulator: 13m01s216ms905.117us\n",
      "  ValueRate: 491ms683.325us / second\n",
      "  Rate: 4.45227 / second\n",
      "  Percentiles: 1%=002ms953.192us; 5%=002ms264.456us; 10%=002ms492.641us; 20%=035ms397.538us; 50%=042ms757.489us; 80%=263ms183.391us; 90%=330ms603.431us; 95%=387ms729.485us; 99%=478ms381.204us\n",
      "Metric: InboundData\n",
      "  TotalSamples: 4445\n",
      "  Accumulator: 1.04GB\n",
      "  ValueRate: 4.32MB / second\n",
      "  Rate: 4.14637 / second\n",
      "  Percentiles: 1%=2.00B; 5%=2.00B; 10%=2.00B; 20%=2.00B; 50%=2.00B; 80%=2.00KB; 90%=2.00MB; 95%=2.00MB; 99%=8.00MB\n",
      "Metric: InputOutputAliasCount\n",
      "  TotalSamples: 8\n",
      "  Accumulator: 3557.00\n",
      "  ValueRate: 11.62 / second\n",
      "  Rate: 0.0261289 / second\n",
      "  Percentiles: 1%=1.00; 5%=1.00; 10%=1.00; 20%=1.00; 50%=4.00; 80%=1577.00; 90%=1577.00; 95%=1577.00; 99%=1577.00\n",
      "Metric: IrValueTensorToXlaData\n",
      "  TotalSamples: 395\n",
      "  Accumulator: 05s338ms620.652us\n",
      "  ValueRate: 02s129ms701.662us / second\n",
      "  Rate: 157.53 / second\n",
      "  Percentiles: 1%=801.370us; 5%=893.457us; 10%=938.927us; 20%=001ms030.556us; 50%=001ms314.298us; 80%=008ms021.594us; 90%=025ms988.606us; 95%=026ms341.837us; 99%=041ms268.748us\n",
      "Metric: OutboundData\n",
      "  TotalSamples: 10519\n",
      "  Accumulator: 1.09GB\n",
      "  ValueRate: 31.97KB / second\n",
      "  Rate: 6.71423 / second\n",
      "  Percentiles: 1%=2.00B; 5%=2.00B; 10%=2.00B; 20%=2.00B; 50%=2.00B; 80%=8.00B; 90%=8.00B; 95%=8.00B; 99%=96.50KB\n",
      "Metric: ReleaseDataHandlesTime\n",
      "  TotalSamples: 33288\n",
      "  Accumulator: 42s563ms460.764us\n",
      "  ValueRate: 026ms494.016us / second\n",
      "  Rate: 19.3263 / second\n",
      "  Percentiles: 1%=455.106us; 5%=517.233us; 10%=571.573us; 20%=644.875us; 50%=821.452us; 80%=001ms312.211us; 90%=003ms159.606us; 95%=005ms665.869us; 99%=007ms480.054us\n",
      "Metric: TensorsGraphSize\n",
      "  TotalSamples: 6222\n",
      "  Accumulator: 60805554.00\n",
      "  ValueRate: 37722.86 / second\n",
      "  Rate: 4.45152 / second\n",
      "  Percentiles: 1%=1.00; 5%=1.00; 10%=1.00; 20%=3703.00; 50%=4517.00; 80%=22038.00; 90%=22038.00; 95%=22038.00; 99%=22038.00\n",
      "Metric: TransferFromServerTime\n",
      "  TotalSamples: 4445\n",
      "  Accumulator: 13s804ms908.994us\n",
      "  ValueRate: 031ms030.546us / second\n",
      "  Rate: 4.14637 / second\n",
      "  Percentiles: 1%=989.140us; 5%=001ms077.612us; 10%=001ms125.162us; 20%=001ms195.970us; 50%=001ms407.821us; 80%=002ms140.779us; 90%=007ms942.273us; 95%=019ms392.167us; 99%=035ms704.402us\n",
      "Metric: TransferToServerTime\n",
      "  TotalSamples: 10519\n",
      "  Accumulator: 10m21s322ms816.874us\n",
      "  ValueRate: 416ms276.853us / second\n",
      "  Rate: 6.71411 / second\n",
      "  Percentiles: 1%=904.420us; 5%=001ms016.380us; 10%=001ms112.945us; 20%=001ms280.511us; 50%=003ms744.504us; 80%=198ms395.101us; 90%=256ms210.896us; 95%=311ms430.334us; 99%=428ms180.830us\n",
      "Metric: TransferToServerTransformTime\n",
      "  TotalSamples: 10519\n",
      "  Accumulator: 12s421ms737.651us\n",
      "  ValueRate: 006ms345.664us / second\n",
      "  Rate: 6.71423 / second\n",
      "  Percentiles: 1%=044.775us; 5%=048.963us; 10%=053.366us; 20%=057.293us; 50%=091.483us; 80%=187.062us; 90%=003ms790.568us; 95%=007ms553.897us; 99%=014ms803.485us\n",
      "Counter: CachedCompile\n",
      "  Value: 6210\n",
      "Counter: CreateCompileHandles\n",
      "  Value: 12\n",
      "Counter: CreateDataHandles\n",
      "  Value: 3048935\n",
      "Counter: CreateXlaTensor\n",
      "  Value: 12360181\n",
      "Counter: DestroyDataHandles\n",
      "  Value: 3047087\n",
      "Counter: DestroyXlaTensor\n",
      "  Value: 12358607\n",
      "Counter: DeviceDataCacheMiss\n",
      "  Value: 7694\n",
      "Counter: MarkStep\n",
      "  Value: 2055\n",
      "Counter: ReleaseDataHandles\n",
      "  Value: 3047087\n",
      "Counter: UncachedCompile\n",
      "  Value: 12\n",
      "Counter: XRTAllocateFromTensor_Empty\n",
      "  Value: 39\n",
      "Counter: XrtCompile_Empty\n",
      "  Value: 128\n",
      "Counter: XrtExecuteChained_Empty\n",
      "  Value: 128\n",
      "Counter: XrtExecute_Empty\n",
      "  Value: 128\n",
      "Counter: XrtMemoryInfo_Empty\n",
      "  Value: 128\n",
      "Counter: XrtRead_Empty\n",
      "  Value: 128\n",
      "Counter: XrtReleaseAllocationHandle_Empty\n",
      "  Value: 128\n",
      "Counter: XrtReleaseCompileHandle_Empty\n",
      "  Value: 128\n",
      "Counter: XrtSessionCount\n",
      "  Value: 10\n",
      "Counter: XrtSubTuple_Empty\n",
      "  Value: 128\n",
      "Counter: aten::_local_scalar_dense\n",
      "  Value: 4050\n",
      "Counter: xla::_log_softmax\n",
      "  Value: 2025\n",
      "Counter: xla::_log_softmax_backward_data\n",
      "  Value: 1920\n",
      "Counter: xla::_softmax\n",
      "  Value: 48600\n",
      "Counter: xla::_softmax_backward_data\n",
      "  Value: 46080\n",
      "Counter: xla::_unsafe_view\n",
      "  Value: 388800\n",
      "Counter: xla::add\n",
      "  Value: 338115\n",
      "Counter: xla::add_\n",
      "  Value: 3309447\n",
      "Counter: xla::addcdiv_\n",
      "  Value: 754560\n",
      "Counter: xla::addcmul\n",
      "  Value: 101250\n",
      "Counter: xla::addcmul_\n",
      "  Value: 754560\n",
      "Counter: xla::addmm\n",
      "  Value: 4050\n",
      "Counter: xla::as_strided\n",
      "  Value: 395\n",
      "Counter: xla::bernoulli_\n",
      "  Value: 142080\n",
      "Counter: xla::bmm\n",
      "  Value: 281520\n",
      "Counter: xla::cat\n",
      "  Value: 2025\n",
      "Counter: xla::copy_\n",
      "  Value: 10915\n",
      "Counter: xla::cumsum\n",
      "  Value: 2025\n",
      "Counter: xla::div\n",
      "  Value: 96600\n",
      "Counter: xla::div_\n",
      "  Value: 142080\n",
      "Counter: xla::embedding\n",
      "  Value: 6075\n",
      "Counter: xla::embedding_dense_backward\n",
      "  Value: 5760\n",
      "Counter: xla::empty\n",
      "  Value: 159251\n",
      "Counter: xla::empty_strided\n",
      "  Value: 395\n",
      "Counter: xla::eq\n",
      "  Value: 2025\n",
      "Counter: xla::expand\n",
      "  Value: 198345\n",
      "Counter: xla::fill_\n",
      "  Value: 1920\n",
      "Counter: xla::gelu\n",
      "  Value: 48600\n",
      "Counter: xla::gelu_backward\n",
      "  Value: 46080\n",
      "Counter: xla::index_select\n",
      "  Value: 6075\n",
      "Counter: xla::max\n",
      "  Value: 2025\n",
      "Counter: xla::mean\n",
      "  Value: 2025\n",
      "Counter: xla::mm\n",
      "  Value: 848400\n",
      "Counter: xla::mul\n",
      "  Value: 576210\n",
      "Counter: xla::mul_\n",
      "  Value: 1511145\n",
      "Counter: xla::native_batch_norm\n",
      "  Value: 101250\n",
      "Counter: xla::native_batch_norm_backward\n",
      "  Value: 96000\n",
      "Counter: xla::native_layer_norm\n",
      "  Value: 101250\n",
      "Counter: xla::native_layer_norm_backward\n",
      "  Value: 96000\n",
      "Counter: xla::ne\n",
      "  Value: 2025\n",
      "Counter: xla::nll_loss_backward\n",
      "  Value: 1920\n",
      "Counter: xla::nll_loss_forward\n",
      "  Value: 3945\n",
      "Counter: xla::permute\n",
      "  Value: 378720\n",
      "Counter: xla::rsub\n",
      "  Value: 2025\n",
      "Counter: xla::scatter_\n",
      "  Value: 1920\n",
      "Counter: xla::select\n",
      "  Value: 2025\n",
      "Counter: xla::slice\n",
      "  Value: 11940\n",
      "Counter: xla::sqrt\n",
      "  Value: 754560\n",
      "Counter: xla::sub\n",
      "  Value: 96000\n",
      "Counter: xla::sum\n",
      "  Value: 472425\n",
      "Counter: xla::t\n",
      "  Value: 1132875\n",
      "Counter: xla::tanh\n",
      "  Value: 2025\n",
      "Counter: xla::topk\n",
      "  Value: 2025\n",
      "Counter: xla::transpose\n",
      "  Value: 279000\n",
      "Counter: xla::unsqueeze\n",
      "  Value: 9810\n",
      "Counter: xla::view\n",
      "  Value: 3348060\n",
      "Counter: xla::zero_\n",
      "  Value: 758898\n",
      "Metric: XrtAllocateFromTensor\n",
      "  TotalSamples: 128672\n",
      "  Accumulator: 03m15s940ms382.443us\n",
      "  Mean: 001ms386.131us\n",
      "  StdDev: 001ms021.837us\n",
      "  Rate: 76.2734 / second\n",
      "  Percentiles: 25%=418.943us; 50%=995.905us; 80%=003ms630.062us; 90%=003ms879.788us; 95%=003ms986.091us; 99%=003ms309.552us\n",
      "Metric: XrtCompile\n",
      "  TotalSamples: 68\n",
      "  Accumulator: 25m09s891ms401.566us\n",
      "  Mean: 22s190ms579.435us\n",
      "  StdDev: 28s887ms490.643us\n",
      "  Rate: 0.0390369 / second\n",
      "  Percentiles: 25%=03s004ms582.583us; 50%=08s407ms429.386us; 80%=01m10s900ms527.171us; 90%=01m13s882ms865.470us; 95%=01m13s135ms295.055us; 99%=01m13s330ms805.607us\n",
      "Metric: XrtExecute\n",
      "  TotalSamples: 48754\n",
      "  Accumulator: 02h43m35s853ms533.964us\n",
      "  Mean: 098ms469.788us\n",
      "  StdDev: 126ms220.033us\n",
      "  Rate: 25.9297 / second\n",
      "  Percentiles: 25%=033ms487.971us; 50%=040ms303.938us; 80%=243ms979.552us; 90%=312ms585.103us; 95%=384ms600.217us; 99%=448ms176.236us\n",
      "Metric: XrtExecutorEvict\n",
      "  TotalSamples: 0\n",
      "  Accumulator: nanB\n",
      "  Mean: nanB\n",
      "  StdDev: nanB\n",
      "  Percentiles: \n",
      "Metric: XrtReadLiteral\n",
      "  TotalSamples: 32795\n",
      "  Accumulator: 17s946ms239.906us\n",
      "  Mean: 002ms296.110us\n",
      "  StdDev: 042ms554.065us\n",
      "  Rate: 24.5652 / second\n",
      "  Percentiles: 25%=413.933us; 50%=500.130us; 80%=634.905us; 90%=002ms227.208us; 95%=003ms846.749us; 99%=009ms651.076us\n",
      "Metric: XrtReleaseAllocation\n",
      "  TotalSamples: 255432\n",
      "  Accumulator: 01m23s003ms475.096us\n",
      "  Mean: 199.271us\n",
      "  StdDev: 578.688us\n",
      "  Rate: 49.8069 / second\n",
      "  Percentiles: 25%=019.252us; 50%=029.604us; 80%=066.104us; 90%=338.621us; 95%=002ms502.491us; 99%=003ms955.240us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    # torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    _run()\n",
    "\n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetRetriever(Dataset):\n",
    "    def __init__(self, df, encoded):\n",
    "        self.df = df\n",
    "        self.encoded = encoded\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):   \n",
    "        ids = self.encoded['input_ids'][index]\n",
    "        mask = self.encoded['attention_mask'][index]\n",
    "        return {\n",
    "            'ids':torch.tensor(ids),\n",
    "            'mask':torch.tensor(mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "test_text = test[['premise', 'hypothesis']].values.tolist()\n",
    "test_encoded = tokenizer.batch_encode_plus(\n",
    "    test_text,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=MAX_LEN\n",
    ")\n",
    "\n",
    "test_dataset = TestDatasetRetriever(df=test, encoded=test_encoded)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load Serialized Model\n",
    "device = xm.xla_device()\n",
    "model = WRAPPED_MODEL.to(device).eval()\n",
    "model.load_state_dict(xser.load(\"model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:36<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "\n",
    "for i, data in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "    ids = data[\"ids\"]\n",
    "    mask = data[\"mask\"]\n",
    "    ids = ids.to(device, dtype=torch.long)\n",
    "    mask = mask.to(device, dtype=torch.long)\n",
    "    outputs = model(\n",
    "        input_ids = ids,\n",
    "        attention_mask = mask,\n",
    "    )\n",
    "    outputs_np = outputs.cpu().detach().numpy().tolist()\n",
    "    test_preds.extend(outputs_np)  \n",
    "    \n",
    "test_preds = torch.FloatTensor(test_preds)\n",
    "top1_prob, top1_label = torch.topk(test_preds, 1)\n",
    "y = top1_label.cpu().detach().numpy()\n",
    "sample_submission.prediction = y\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01d641f8813b4f0eab7161e2c392cad2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "06f55ae3918d4a62b35d9989275a5186": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08f69a877633407bb2676c112f8983bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0eb0db4a191b47348356e15e1bbbc561": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d3183e17c85a4f408c563af1c5eb9056",
        "IPY_MODEL_8d091b01b29546718e64d209325beb40"
       ],
       "layout": "IPY_MODEL_581619d588b54bf180eccc2e01405ee8"
      }
     },
     "52c4ae7995d0408b9f675347612da75c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5528ea3955354ec9b2a34ae728790fa5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_08f69a877633407bb2676c112f8983bd",
       "placeholder": "​",
       "style": "IPY_MODEL_951ec7c0abb543bcb1d16288bda0cf23",
       "value": " 2.24G/2.24G [01:09&lt;00:00, 32.2MB/s]"
      }
     },
     "57ccf391bd334aeead5055a0ed0152d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "581619d588b54bf180eccc2e01405ee8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "665f81de248e4223ba082aec81c8fe8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "78b8024340be4227ab9494532484fb2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b08bebf728864bda975f538d3d2f2a98",
        "IPY_MODEL_5528ea3955354ec9b2a34ae728790fa5"
       ],
       "layout": "IPY_MODEL_e87f0d51df0d42d0b936539c37e871b3"
      }
     },
     "7ce60d729e0a40beafb68d901717e944": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7df94faba3534283b6520f6c8d7ea27b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01d641f8813b4f0eab7161e2c392cad2",
       "placeholder": "​",
       "style": "IPY_MODEL_c1cc3b9c770d477db6c3cda6b85c76d7",
       "value": " 5.07M/5.07M [00:01&lt;00:00, 3.36MB/s]"
      }
     },
     "8544145ba6844322a4bb9c38cddb0d6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d091b01b29546718e64d209325beb40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_06f55ae3918d4a62b35d9989275a5186",
       "placeholder": "​",
       "style": "IPY_MODEL_a827e65f5488426ba6f6193d26869e94",
       "value": " 513/513 [01:11&lt;00:00, 7.23B/s]"
      }
     },
     "92586c4c595e43be98f6510f04e7e251": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f2dab958193c442aa0bae039acfe50ce",
        "IPY_MODEL_7df94faba3534283b6520f6c8d7ea27b"
       ],
       "layout": "IPY_MODEL_aa860e90c76c4a13ad34af689266dbe9"
      }
     },
     "951ec7c0abb543bcb1d16288bda0cf23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a827e65f5488426ba6f6193d26869e94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aa860e90c76c4a13ad34af689266dbe9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b08bebf728864bda975f538d3d2f2a98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd498b84f54149198af93443afadeed8",
       "max": 2244861551.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_52c4ae7995d0408b9f675347612da75c",
       "value": 2244861551.0
      }
     },
     "c1cc3b9c770d477db6c3cda6b85c76d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d3183e17c85a4f408c563af1c5eb9056": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8544145ba6844322a4bb9c38cddb0d6e",
       "max": 513.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7ce60d729e0a40beafb68d901717e944",
       "value": 513.0
      }
     },
     "dd498b84f54149198af93443afadeed8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e87f0d51df0d42d0b936539c37e871b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2dab958193c442aa0bae039acfe50ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_57ccf391bd334aeead5055a0ed0152d4",
       "max": 5069051.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_665f81de248e4223ba082aec81c8fe8e",
       "value": 5069051.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
