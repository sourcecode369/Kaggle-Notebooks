{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a55c8903-ea51-41cf-a76c-ce3543ca3c2a",
    "_uuid": "b0f52dfd-26f4-418d-94ba-7d491e369946"
   },
   "source": [
    "# Load packages and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "84e9ae4a-9ed0-4a01-b6e8-d48d7a031cf9",
    "_kg_hide-input": true,
    "_uuid": "628a811b-af67-4965-8bff-033635f45c53"
   },
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os, argparse\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, gc\n",
    "from typing import Dict\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# torch related\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR, StepLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "# transformers & tokenizers\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer\n",
    "import tokenizers\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a43695ea-ff55-423e-b9f9-e0a035b9331e",
    "_uuid": "25816c94-636c-4e49-9aef-dbeb93da7a60"
   },
   "source": [
    "# Model Inference Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66933405-97c7-4100-8da9-43b6c89433b3",
    "_uuid": "d4becff6-8080-4336-9d06-972016471e89"
   },
   "outputs": [],
   "source": [
    "copyfile(src = \"../input/utils-v10/utilsv10.py\", dst = \"../working/utilsv10.py\")\n",
    "copyfile(src = \"../input/utils-v10/dataset10.py\", dst = \"../working/dataset10.py\")\n",
    "copyfile(src = \"../input/utils-v10/dataset11.py\", dst = \"../working/dataset11.py\")\n",
    "\n",
    "from utilsv10 import (binary_focal_loss, get_learning_rate, jaccard_list, get_best_pred, ensemble, ensemble_words,get_char_prob,\n",
    "                   load_model, save_model, set_seed, write_event, evaluate, get_predicts_from_token_logits)\n",
    "\n",
    "from dataset10 import TrainDataset, MyCollator\n",
    "from dataset11 import TrainDataset as TrainDataset11"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1dc0bfb7-3ac4-418e-a631-c5ab591564a1",
    "_uuid": "2275123d-5a5d-4aa6-aa04-5d46b406d074"
   },
   "source": [
    "## Preapare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "aa663282-288a-4016-aded-a2dd0de06c3b",
    "_uuid": "f5fe4d8a-ae79-4a57-8675-14090e8f13da"
   },
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('../input/tweet-sentiment-fast/test_all_post_finetune_0608.csv') #lb724\n",
    "df_pred1 = pd.read_csv('../input/tweet-sentiment-fast/test_all_post_finetune_large.csv') #lb717\n",
    "df_train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "\n",
    "df_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "#df_test = pd.read_csv('../input/tweet-sentiment-fast/test_hidden.csv')\n",
    "df_test.loc[:, 'selected_text'] = df_test.text.values\n",
    "df_test['text_clean'] = df_test['text'].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "df_full = pd.read_csv('/kaggle/input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv')\n",
    "df_full = df_full[df_full.text.notnull()].copy()\n",
    "df_full['text_clean'] = df_full['text'].apply(lambda x: \" \".join(x.split()))\n",
    "df_full = df_full.drop_duplicates(subset='text')\n",
    "df_full = df_full[~df_full.aux_id.isin(df_train.textID)]\n",
    "df_full.rename(columns={'sentiment': 'raw_sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5d7bcd7c-d8da-4785-8ce1-32aa7f64642b",
    "_uuid": "495f0c59-a7e9-448d-93ee-877b21add4ee"
   },
   "outputs": [],
   "source": [
    "def find_sentiment(textID, text, sentiment):\n",
    "    text_clean = \" \".join(text.split())\n",
    "    if textID in df_full.aux_id.values:\n",
    "        return df_full['raw_sentiment'].loc[df_full.aux_id==textID].values[0]\n",
    "    elif text in df_full.text.values:\n",
    "        return df_full['raw_sentiment'].loc[df_full.text==text].values[0]\n",
    "    elif text_clean in df_full.text_clean.values:\n",
    "        return df_full['raw_sentiment'].loc[df_full.text_clean==text_clean].values[0]\n",
    "    else:\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a6a4c0cf-78d7-4592-b2fd-8d72da0eb87e",
    "_uuid": "7e074941-04d3-4056-ab13-ae519160315d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# find raw sentiment\n",
    "df_test['raw_sentiment'] = df_test.apply(lambda x: find_sentiment(x['textID'], x['text'], x['sentiment']), axis=1)\n",
    "na_mask = df_test.raw_sentiment.isnull()\n",
    "print(na_mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "699b4a6a-743c-4035-b632-e08faf5d8e3f",
    "_uuid": "4f8153db-ff54-4815-b899-fbc3d0baa711"
   },
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "9eda1cfa-5f49-4653-941b-a39aec3a37d7",
    "_uuid": "6b3714bc-dc69-4777-a1ce-763ed2ab4d39"
   },
   "outputs": [],
   "source": [
    "#test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "test = df_test.copy()\n",
    "tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base/', do_lower_case=False)\n",
    "\n",
    "class Args:\n",
    "    post = True\n",
    "    tokenizer = tokenizer\n",
    "    offset = 4\n",
    "    batch_size = 4\n",
    "    workers = 0\n",
    "args = Args()\n",
    "\n",
    "# v11\n",
    "class Args11:\n",
    "    post = True\n",
    "    tokenizer = tokenizer\n",
    "    offset = 7\n",
    "    batch_size = 4\n",
    "    workers = 0\n",
    "args11 = Args11()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2bf1d1ef-dad9-4c62-a254-f0f2d5f703a0",
    "_uuid": "4bc33b59-2a5b-4c47-b106-91271f2f3d6d"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "673cde9d-c98b-4af1-a995-1bd1760ecc47",
    "_uuid": "96fceb66-e339-4e33-a321-55b3289474ec"
   },
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrain_path=None, dropout=0.2, config=None):\n",
    "        super(TweetModel, self).__init__()\n",
    "        if config is not None:\n",
    "            self.bert = AutoModel.from_config(config)\n",
    "        else:\n",
    "            config = AutoConfig.from_pretrained(pretrain_path, output_hidden_states=True)\n",
    "            self.bert = AutoModel.from_pretrained(\n",
    "                pretrain_path, cache_dir=None, config=config)\n",
    "        \n",
    "        self.cnn =  nn.Conv1d(self.bert.config.hidden_size*3, self.bert.config.hidden_size, 3, padding=1)\n",
    "\n",
    "        # self.rnn = nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size//2, num_layers=2,\n",
    "        #                     batch_first=True, bidirectional=True)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.whole_head = nn.Sequential(OrderedDict([\n",
    "            ('dropout', nn.Dropout(0.1)),\n",
    "            ('l1', nn.Linear(self.bert.config.hidden_size*3, 256)),\n",
    "            ('act1', nn.GELU()),\n",
    "            ('dropout', nn.Dropout(0.1)),\n",
    "            ('l2', nn.Linear(256, 2))\n",
    "        ]))\n",
    "        self.se_head = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.inst_head = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inputs, masks, token_type_ids=None, input_emb=None):\n",
    "        _, pooled_output, hs = self.bert(\n",
    "            inputs, masks, token_type_ids=token_type_ids, inputs_embeds=input_emb)\n",
    "\n",
    "        seq_output = torch.cat([hs[-1],hs[-2],hs[-3]], dim=-1)\n",
    "\n",
    "        # seq_output = hs[-1]\n",
    "\n",
    "        avg_output = torch.sum(seq_output*masks.unsqueeze(-1), dim=1, keepdim=False)\n",
    "        avg_output = avg_output/torch.sum(masks, dim=-1, keepdim=True)\n",
    "        # +max_output\n",
    "        whole_out = self.whole_head(avg_output)\n",
    "\n",
    "        seq_output = self.gelu(self.cnn(seq_output.permute(0,2,1)).permute(0,2,1))\n",
    "        \n",
    "        se_out = self.se_head(self.dropout(seq_output))  #()\n",
    "        inst_out = self.inst_head(self.dropout(seq_output))\n",
    "        return whole_out, se_out[:, :, 0], se_out[:, :, 1], inst_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "3e4c0da5-d944-4b99-a762-a0af52158ece",
    "_uuid": "4bd98a9d-bbd3-4c5e-961a-f3192cce559e"
   },
   "outputs": [],
   "source": [
    "def predict_wu(model: nn.Module, valid_df, valid_loader, args, progress=False) -> Dict[str, float]:\n",
    "    # run_root = Path('../experiments/' + args.run_root)\n",
    "    model.eval()\n",
    "    all_end_pred, all_whole_pred, all_start_pred, all_inst_out = [], [], [], []\n",
    "    if progress:\n",
    "        tq = tqdm.tqdm(total=len(valid_df))\n",
    "    with torch.no_grad():\n",
    "        for tokens, types, masks, _, _, _, _, _, _, _ in valid_loader:\n",
    "            if progress:\n",
    "                batch_size = tokens.size(0)\n",
    "                tq.update(batch_size)\n",
    "            masks = masks.cuda()\n",
    "            tokens = tokens.cuda()\n",
    "            types = types.cuda()\n",
    "            whole_out, start_out, end_out, inst_out = model(tokens, masks, types)\n",
    "            \n",
    "            all_whole_pred.append(torch.softmax(whole_out, dim=-1)[:,1].cpu().numpy())\n",
    "            inst_out = torch.softmax(inst_out, dim=-1)\n",
    "            for idx in range(len(start_out)):\n",
    "                length = torch.sum(masks[idx,:]).item()-1 # -1 for last token\n",
    "                all_start_pred.append(torch.softmax(start_out[idx, args.offset:length], axis=-1).cpu())\n",
    "                all_end_pred.append(torch.softmax(end_out[idx, args.offset:length], axis=-1).cpu())\n",
    "                all_inst_out.append(inst_out[idx,:,1].cpu())\n",
    "            assert all_start_pred[-1].dim()==1\n",
    "\n",
    "    all_whole_pred = np.concatenate(all_whole_pred)\n",
    "    \n",
    "    if progress:\n",
    "        tq.close()\n",
    "    return all_whole_pred, all_start_pred, all_end_pred, all_inst_out"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d2bb9e0-8253-4e89-a9e8-b4b0fc9782cb",
    "_uuid": "da199e5e-d871-4d8e-ae8a-cb933ed467c5"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "570fa2af-e87a-42a5-ab0c-973ad2ea3404",
    "_uuid": "43a8c126-991a-4b60-947e-b3662001d5b6"
   },
   "outputs": [],
   "source": [
    "# convrt part I char-level probability to clean text version (an array of length 160)\n",
    "def _convrt_prob_partI(text, prob, max_char_len=160):\n",
    "    clean_text = \" \".join(text.split())\n",
    "    new_prob = []\n",
    "    p1, p2 = 0, 0\n",
    "    while p1 < len(text):\n",
    "        if text[p1] not in [\" \", \"\\t\", \"\\xa0\"]:\n",
    "            if text[p1] == clean_text[p2]:\n",
    "                new_prob.append(prob[p1])\n",
    "                p1 += 1\n",
    "                p2 += 1\n",
    "        else:\n",
    "            if p1 + 1 < len(text) and text[p1+1] not in [\" \", \"\\t\", \"\\xa0\"]:\n",
    "                if clean_text[p2] == \" \":\n",
    "                    new_prob.append(prob[p1])\n",
    "                    p1 += 1\n",
    "                    p2 += 1\n",
    "                else:\n",
    "                    p1 += 1                   \n",
    "            else:\n",
    "                p1 += 1\n",
    "    if len(new_prob) < max_char_len:\n",
    "        new_prob = new_prob + (max_char_len - len(new_prob))*[0]\n",
    "    return new_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "96364f08-c7fd-4d22-836f-951049fad4f9",
    "_uuid": "7d886d71-75d9-4af2-a7ca-2c69936adc88"
   },
   "outputs": [],
   "source": [
    "def get_prediction_partI(weights_path, test, test_loader, args, output_name):\n",
    "    # load model\n",
    "    config = RobertaConfig.from_pretrained('../input/roberta-base', output_hidden_states=True)\n",
    "    model = TweetModel(config=config)\n",
    "\n",
    "    # 10-fold predict\n",
    "    all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = [], [], [], []  \n",
    "    for fold in range(10):\n",
    "        load_model(model, f'{weights_path}/best-model-%d.pt' % fold)\n",
    "        model.cuda()\n",
    "        fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds = predict_wu(model, test, test_loader, args, progress=True)\n",
    "\n",
    "        all_whole_preds.append(fold_whole_preds)\n",
    "        all_start_preds.append(fold_start_preds)\n",
    "        all_end_preds.append(fold_end_preds)\n",
    "        all_inst_preds.append(fold_inst_preds)\n",
    "\n",
    "\n",
    "    all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = ensemble(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test)\n",
    "    word_preds, inst_word_preds, scores = get_predicts_from_token_logits(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test, args)\n",
    "    # word_preds, inst_word_preds, scores = get_predicts_from_token_logits(fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds, test, args)\n",
    "    start_char_prob, end_char_prob = get_char_prob(all_start_preds, all_end_preds, test, args)\n",
    "    \n",
    "    test['start_char_prob'] = start_char_prob\n",
    "    test['end_char_prob'] = end_char_prob\n",
    "    test['selected_text'] = word_preds\n",
    "\n",
    "    test['prob_start'] = test.apply(lambda x: _convrt_prob_partI(x['text'], x['start_char_prob']), axis=1)\n",
    "    test['prob_end'] = test.apply(lambda x: _convrt_prob_partI(x['text'], x['end_char_prob']), axis=1)\n",
    "\n",
    "    test.to_pickle(f'{output_name}.pkl')\n",
    "    np.save(f\"start_{output_name}.npy\", np.array(test['prob_start'].tolist()))\n",
    "    np.save(f\"end_{output_name}.npy\", np.array(test['prob_end'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b988161-3895-4c45-98cb-7bf01c8b854c",
    "_uuid": "87a26da7-f1d5-4a75-b6ae-0b505ea08189"
   },
   "source": [
    "### V10 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "6b078b27-6e3d-443a-a069-838f9cb72a5e",
    "_uuid": "1a4f4fa1-931f-4752-a045-5e5ee720f7b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3534 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 237.63it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 282.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 231.18it/s]\n",
      "  1%|          | 28/3534 [00:00<00:13, 265.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 250.43it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 287.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 244.72it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 284.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 247.36it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 287.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 230.82it/s]\n",
      "  1%|          | 20/3534 [00:00<00:22, 157.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 248.69it/s]\n",
      "  1%|          | 28/3534 [00:00<00:13, 261.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 245.71it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 284.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 250.71it/s]\n",
      "  1%|          | 20/3534 [00:00<00:18, 185.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 228.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22.1 -       6.04 -      12.20 -       3.18\n"
     ]
    }
   ],
   "source": [
    "collator = MyCollator()\n",
    "test_set = TrainDataset(test, None, tokenizer=tokenizer, mode='test', offset=args.offset)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n",
    "                         num_workers=args.workers)\n",
    "get_prediction_partI(weights_path='../input/roberta-v10-10',\n",
    "                     test=test, \n",
    "                     test_loader=test_loader, \n",
    "                     args=args, \n",
    "                     output_name='output_v10',\n",
    "                    )\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2abf5b0f-4159-408c-9bf3-8e8e8a89d450",
    "_uuid": "970829a7-d14c-4b28-bedb-aa3ecc4030b5"
   },
   "source": [
    "### V11 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "8c975dc6-d762-415c-b457-a488fe79d417",
    "_uuid": "42e73052-e2b8-443b-88e0-0b097cf66954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean token length 31.135257498585172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3534 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 244.26it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 277.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 246.03it/s]\n",
      "  1%|          | 20/3534 [00:00<00:24, 142.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 227.44it/s]\n",
      "  1%|          | 28/3534 [00:00<00:13, 259.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 245.24it/s]\n",
      "  1%|          | 20/3534 [00:00<00:17, 197.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 233.62it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 282.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 246.18it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 284.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 244.91it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 280.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 232.96it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 280.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:15<00:00, 235.47it/s]\n",
      "  1%|          | 32/3534 [00:00<00:12, 284.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3534/3534 [00:14<00:00, 243.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24.3 -       1.38 -      11.86 -       3.53\n"
     ]
    }
   ],
   "source": [
    "collator = MyCollator()\n",
    "test_set = TrainDataset11(test, None, tokenizer=tokenizer, mode='test', offset=args11.offset)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n",
    "                         num_workers=args.workers)\n",
    "get_prediction_partI(weights_path='../input/roberta-v11-10',\n",
    "                     test=test, \n",
    "                     test_loader=test_loader, \n",
    "                     args=args11, \n",
    "                     output_name='output_v11',\n",
    "                    )\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "615bc3f8-cf63-41d5-9106-cc20724cf543",
    "_uuid": "3c4e8542-7154-4b45-b8fb-72b8b131dddb"
   },
   "source": [
    "# Model Inference Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "824867ec-94c1-4e6e-a711-538a7315c7a0",
    "_uuid": "5d980ad7-558e-41d2-b704-90e45fad9c9c"
   },
   "outputs": [],
   "source": [
    "# import helper scripts\n",
    "from shutil import copyfile\n",
    "\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"../input/tweet-sentiment/common.py\", dst = \"../working/common.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/dataset.py\", dst = \"../working/dataset.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/models.py\", dst = \"../working/models.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/metrics.py\", dst = \"../working/metrics.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/utils.py\", dst = \"../working/utils.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/predict_fn.py\", dst = \"../working/predict_fn.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/nlp_albumentations.py\", dst = \"../working/nlp_albumentations.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/transform.py\", dst = \"../working/transform.py\")\n",
    "copyfile(src = \"../input/tweet-sentiment/run_inference_kaggle.py\", dst = \"../working/run_inference_kaggle.py\")\n",
    "\n",
    "from dataset import process_data\n",
    "from dataset import TweetDataset_kaggle as TweetDataset\n",
    "from models import TweetModel, TweetModel_v2\n",
    "from common import *\n",
    "from metrics import *\n",
    "from utils import *\n",
    "from utils import _convrt_back\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# # %% [code]\n",
    "# !python run_inference_kaggle.py --model_name='roberta_base' \\\n",
    "#                                 --model_path='../input/roberta-base/' \\\n",
    "#                                 --raw_sentiment=1\n",
    "\n",
    "# mem = psutil.virtual_memory()\n",
    "# print(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')\n",
    "\n",
    "# # %% [code]\n",
    "# !python run_inference_kaggle.py --model_name='roberta_base_noRawSenti' \\\n",
    "#                                 --model_path='../input/roberta-base-bs32-v2-0608' \\\n",
    "#                                 --raw_sentiment=0\n",
    "\n",
    "# mem = psutil.virtual_memory()\n",
    "# print(f'{mem.percent:5} - {mem.free/1024**3:10.2f} - {mem.available/1024**3:10.2f} - {mem.used/1024**3:10.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18b61e72-b80b-456e-822a-f431cee955de",
    "_uuid": "7ae5f929-65d6-4a00-a6d9-1bd9f689ee7a"
   },
   "source": [
    "## Helper functions & params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "93ec38f7-d0f5-4fa1-b513-b85465f6b500",
    "_uuid": "e57e3fd9-472e-49ed-a32e-3cf8fdad840a"
   },
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    # return model and tokenizer\n",
    "    if model_name.startswith('roberta_base'):\n",
    "        model_info = {\n",
    "            'name': 'roberta-base',\n",
    "            'model_path': '../input/roberta-base' if ON_KAGGLE else 'roberta-base',\n",
    "            'from_pretrained': False if ON_KAGGLE else True,\n",
    "            'vocab_file': '../input/roberta-base/vocab.json',\n",
    "            'merges_file': '../input/roberta-base/merges.txt',\n",
    "        }\n",
    "    elif model_name.startswith('roberta_large'):\n",
    "        model_info = {\n",
    "            'name': 'roberta-large',\n",
    "            'model_path': '../input/roberta-large' if ON_KAGGLE else 'roberta-large',\n",
    "            'from_pretrained': False if ON_KAGGLE else True,\n",
    "            'vocab_file': '../input/roberta-large/vocab.json',\n",
    "            'merges_file': '../input/roberta-large/merges.txt',\n",
    "        }\n",
    "    else:\n",
    "        raise RuntimeError('%s is not implemented.' % model_name)\n",
    "\n",
    "    model = TweetModel_v2(model_info)\n",
    "    tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "        vocab_file=model_info['vocab_file'],\n",
    "        merges_file=model_info['merges_file'],\n",
    "        lowercase=False,\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "84bc7dfd-8b1d-4ccf-ab3f-19cef069ff38",
    "_uuid": "d12eb71b-ffdb-4bb6-8a32-3e982876e314"
   },
   "source": [
    "## Dataloader, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "7fce52d3-4c28-47e4-9028-ed2ded767a13",
    "_uuid": "cb9c994c-89f2-4e5d-87e0-196d8a4aa0df"
   },
   "outputs": [],
   "source": [
    "N_FOLD = 10\n",
    "POST_PROCESS = True\n",
    "params = {\n",
    "    'models': [\n",
    "        #'roberta_base', \n",
    "        'roberta_base_noRawSenti', \n",
    "        'roberta_large',\n",
    "    ],\n",
    "    'batch_size': 4,\n",
    "    'workers': 1 if ON_KAGGLE else 8,\n",
    "    'max_len': 192, \n",
    "    'folds': list(x for x in range(N_FOLD)),\n",
    "    'limit': 0,\n",
    "}\n",
    "\n",
    "path_lib = {\n",
    "    'roberta_base': '../input/roberta-base/',\n",
    "    'roberta_base_noRawSenti': '../input/roberta-base-bs32-v2-0608',\n",
    "    'roberta_large': '../input/roberta-large-bs32-v2-0608',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "635c973a-090a-48b3-a2cf-c709f812381d",
    "_uuid": "5a37afea-d775-4113-921a-c49cbf4904e3"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "a90c98d2-9f42-4b06-8b6e-8a300ad060a6",
    "_uuid": "76d4f09e-074b-4e56-a8a9-439cd26e4df2"
   },
   "outputs": [],
   "source": [
    "def predict_II(model, data_loader, tokenizer):\n",
    "    start_probs, end_probs = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(tqdm.tqdm(data_loader, ascii=True)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            decode_selected = d[\"decode_selected\"]\n",
    "            raw_tweets = d[\"raw_tweet\"]\n",
    "            raw_selecteds = d[\"raw_selected_text\"]\n",
    "            text_span = d[\"text_span\"]\n",
    "            \n",
    "            ids = ids.cuda()\n",
    "            token_type_ids = token_type_ids.cuda()\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "            start_idxs, end_idsx = [], []\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            if len(outputs) == 2:\n",
    "                outputs_start, outputs_end = outputs\n",
    "                outputs_mask = None\n",
    "            elif len(outputs) == 3:\n",
    "                outputs_start, outputs_end, outputs_mask = outputs   \n",
    "            # probability\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy() \n",
    "            \n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                # char level probs\n",
    "                raw_tweet = raw_tweets[px]\n",
    "                span_start, span_end = text_span[0][px], text_span[1][px]\n",
    "                span_start, span_end = int(span_start), int(span_end)\n",
    "                token_ids = d[\"ids\"][px][span_start: span_end]\n",
    "                clean_text = \" \".join(raw_tweet.split())\n",
    "                _, prob_char = convrt_prob_char_level(clean_text, token_ids, outputs_start[px, span_start: span_end], tokenizer)\n",
    "                if len(prob_char) < 160: # padding\n",
    "                    prob_char += [0] * (160 - len(prob_char))\n",
    "                start_probs.append(prob_char)\n",
    "                _, prob_char = convrt_prob_char_level(clean_text, token_ids, outputs_end[px, span_start: span_end], tokenizer)\n",
    "                if len(prob_char) < 160: # padding\n",
    "                    prob_char += [0] * (160 - len(prob_char))\n",
    "                end_probs.append(prob_char)\n",
    "    start_probs = np.array(start_probs)\n",
    "    end_probs = np.array(end_probs)\n",
    "    return start_probs, end_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "e3c54273-1e6d-4557-b417-058ee6b17cb1",
    "_uuid": "bc95f75e-1f1a-49d7-b04c-8cee384ff65b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/884 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 1,600, valid-jac 0.70945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 28.64it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3, step 2,000, valid-jac 0.71515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 29.06it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 5, step 4,372, valid-jac 0.71794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 28.55it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 3, step 2,372, valid-jac 0.72638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 28.98it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 5, step 4,200, valid-jac 0.71415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:31<00:00, 28.49it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 4, step 3,600, valid-jac 0.72371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 29.10it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 2,316, valid-jac 0.72284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 28.85it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 4, step 3,600, valid-jac 0.71006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 29.12it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 6, step 4,600, valid-jac 0.71886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 28.68it/s]\n",
      "  0%|          | 3/884 [00:00<00:29, 29.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 1,600, valid-jac 0.71078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [00:30<00:00, 29.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 4, step 3,600, valid-jac 0.70697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.00it/s]\n",
      "  0%|          | 2/884 [00:00<01:18, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 1,600, valid-jac 0.71515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.03it/s]\n",
      "  0%|          | 2/884 [00:00<01:18, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 7, step 5,800, valid-jac 0.71371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.02it/s]\n",
      "  0%|          | 2/884 [00:00<01:19, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 2,000, valid-jac 0.72997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.04it/s]\n",
      "  0%|          | 2/884 [00:00<01:18, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 2,000, valid-jac 0.71748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.04it/s]\n",
      "  0%|          | 2/884 [00:00<01:19, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 5, step 4,200, valid-jac 0.72598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.01it/s]\n",
      "  0%|          | 2/884 [00:00<01:19, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 2,000, valid-jac 0.71164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.05it/s]\n",
      "  0%|          | 2/884 [00:00<01:18, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 5, step 3,916, valid-jac 0.70656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.03it/s]\n",
      "  0%|          | 2/884 [00:00<01:21, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 6, step 4,800, valid-jac 0.72162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.04it/s]\n",
      "  0%|          | 2/884 [00:00<01:20, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 2, step 2,000, valid-jac 0.71281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 884/884 [01:20<00:00, 11.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "for model_name in params['models']:\n",
    "    model, tokenizer = get_model(model_name)\n",
    "    if model_name.endswith(\"noRawSenti\"):\n",
    "        df_test_tmp = df_test.copy()\n",
    "        df_test_tmp['raw_sentiment'] = \"\"\n",
    "        tweet_dataset = TweetDataset(\n",
    "            df=df_test_tmp,\n",
    "            sentiment_weights=[1,1,1],\n",
    "            tokenizer=tokenizer,\n",
    "            mode='test',\n",
    "            lower_case=0,\n",
    "            max_len=params['max_len'],\n",
    "        )\n",
    "    else:\n",
    "        tweet_dataset = TweetDataset(\n",
    "            df=df_test,\n",
    "            sentiment_weights=[1,1,1],\n",
    "            tokenizer=tokenizer,\n",
    "            mode='test',\n",
    "            lower_case=0,\n",
    "            max_len=params['max_len'],\n",
    "        )      \n",
    "        \n",
    "    data_loader = DataLoader(\n",
    "        tweet_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        num_workers=0,\n",
    "    )\n",
    "        \n",
    "    for i, fold in enumerate(params['folds']):\n",
    "        if model_name == \"roberta_base_noRawSenti\":\n",
    "            if fold < 5:\n",
    "                path = path_lib[model_name] + '-part2'\n",
    "            else:\n",
    "                path = path_lib[model_name]\n",
    "        elif model_name == \"roberta_large\":\n",
    "            if fold <= 2:\n",
    "                path = path_lib[model_name] + '-part1'\n",
    "            elif fold <= 5:\n",
    "                path = path_lib[model_name] + '-part2'\n",
    "            elif fold == 6:\n",
    "                path = path_lib[model_name] + '-part3-2'\n",
    "            elif fold == 7:\n",
    "                path = path_lib[model_name] + '-part3'\n",
    "            else:\n",
    "                path = path_lib[model_name] + '-part4'\n",
    "        else:\n",
    "            path = path_lib[model_name]\n",
    "        load_model(model, f\"{path}/best_jac_{fold}.pt\", multi2single=False)\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        probs_start_pred, probs_end_pred = predict_II(model, data_loader, tokenizer)\n",
    "        if i == 0:\n",
    "            probs_start = probs_start_pred\n",
    "            probs_end = probs_end_pred\n",
    "        else:\n",
    "            probs_start += probs_start_pred\n",
    "            probs_end += probs_end_pred \n",
    "            \n",
    "    probs_start /= len(params['folds'])\n",
    "    probs_end /= len(params['folds'])\n",
    "\n",
    "    df_test['prob_start'] = probs_start.tolist()\n",
    "    df_test['prob_end'] = probs_end.tolist()\n",
    "    df_test.to_pickle(f'{model_name}.pkl')\n",
    "    np.save(f\"start_{model_name}.npy\", probs_start)\n",
    "    np.save(f\"end_{model_name}.npy\", probs_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9f10617a-41b9-44e5-92a7-ef01c64f2cc9",
    "_uuid": "07dadec6-4db7-425f-b947-2aa18a074b84"
   },
   "source": [
    "# Ensemble of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "50f78ff6-af97-4ceb-b423-15bf68daf98c",
    "_uuid": "1002ed7a-ebcd-4f85-9433-578b55f0f0f9"
   },
   "outputs": [],
   "source": [
    "# load .npy file from disk and ensemble char level probability\n",
    "\n",
    "model_names = [\n",
    "               #'roberta_base', \n",
    "               'roberta_base_noRawSenti',\n",
    "               'roberta_large',\n",
    "               'output_v10',\n",
    "               'output_v11',\n",
    "              ]\n",
    "for i, model_name in enumerate(model_names):\n",
    "    prob_start_tmp = np.load(f'start_{model_name}.npy')\n",
    "    prob_end_tmp = np.load(f'end_{model_name}.npy')\n",
    "    if i == 0:\n",
    "        prob_start = prob_start_tmp\n",
    "        prob_end = prob_end_tmp\n",
    "    else:\n",
    "        prob_start += prob_start_tmp\n",
    "        prob_end += prob_end_tmp\n",
    "\n",
    "prob_start /= len(model_names)\n",
    "prob_end /= len(model_names)\n",
    "        \n",
    "df_test['prob_start'] = prob_start.tolist()\n",
    "df_test['prob_end'] = prob_end.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a85adf6a-9f3d-433e-a379-f36662ceb471",
    "_uuid": "4648362a-e4a6-41f7-a110-7597685beefb"
   },
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "308eb779-b0ac-4b92-ba93-1ff575608d70",
    "_uuid": "84ddab4b-8433-41da-a7bd-211086a36735"
   },
   "outputs": [],
   "source": [
    "def _get_pred_char(df, probs_start, probs_end):\n",
    "    df['start_idx'] = np.argmax(probs_start, axis=1)\n",
    "    df['end_idx'] = probs_end.shape[1] - np.argmax(probs_end[:, ::-1], axis=1) - 1\n",
    "    df['prob_start'] = probs_start.tolist()\n",
    "    df['prob_end'] = probs_end.tolist()\n",
    "    idxs = np.where(df.start_idx > df.end_idx)\n",
    "    \n",
    "    for idx in idxs[0]:\n",
    "        prob_start = df.prob_start.values[idx]\n",
    "        prob_end = df.prob_end.values[idx]\n",
    "        start_idx = df.start_idx.values[idx]\n",
    "        end_idx = df.end_idx.values[idx]\n",
    "        if prob_start[start_idx] > prob_end[end_idx] or end_idx == 0:\n",
    "            end_idx = len(prob_start) - np.argmax(prob_end[start_idx:][::-1]) - 1\n",
    "        else:\n",
    "            start_idx = np.argmax(prob_start[:end_idx])\n",
    "        df['start_idx'].iloc[idx] = start_idx     \n",
    "        df['end_idx'].iloc[idx] = end_idx    \n",
    "    #df.rename(columns={'selected_text': 'pred'}, inplace=True)\n",
    "    df['pred_char'] = df.apply(lambda x: x['text_clean'][x['start_idx']: x['end_idx']+1], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d090b1bf-0720-4003-a444-d8d4cd1f8dc6",
    "_uuid": "ca1db0b4-097e-4e2a-9875-a0110c0d5e8a"
   },
   "outputs": [],
   "source": [
    "def post_neutral(df):\n",
    "    df['select_pt'] = df.apply(lambda x: len(x['pred_char'].strip())/len(x['text_clean']), axis=1).values\n",
    "\n",
    "    raw_sents = ['neutral', 'sadness', 'worry', 'happiness', 'love', 'enthusiasm']\n",
    "    mm = (df['sentiment'] == 'neutral') & (df['raw_sentiment'].isin(raw_sents))\n",
    "    mm = (mm |\\\n",
    "          ((df.select_pt > 0.85) & (df.sentiment.isin(['positive']))) |\\\n",
    "          ((df.select_pt > 0.85) & (df.sentiment.isin(['negative']))) |\\\n",
    "          ((df.select_pt < 0.2) & (df.sentiment.isin(['neutral']))))\n",
    "    \n",
    "    df['pred_exp'] = df['pred_char'].values\n",
    "    df['pred_exp'].loc[mm] = df['text_clean'].loc[mm].values\n",
    "    print(f\"# of modified samples: {np.sum(df['pred_exp'] != df['pred_char'])}\")\n",
    "    return df\n",
    "\n",
    "def _post_shift_new(text, pred):\n",
    "    clean_text = \" \".join(text.split())\n",
    "    start_clean = clean_text.find(\" \".join(pred.split()))\n",
    "    \n",
    "    raw_pred = _convrt_back(text, pred, \"neutral\")\n",
    "    raw_pred = raw_pred.strip()\n",
    "    start = text.find(raw_pred)\n",
    "    end = start + len(raw_pred)\n",
    "    \n",
    "    extra_space = start - start_clean \n",
    "    \n",
    "    if start>extra_space and extra_space>0:\n",
    "        if extra_space==1:\n",
    "            if text[start-1] in [',','.','?','!'] and text[start-2]!=' ':\n",
    "                start -= 1\n",
    "        elif extra_space==2:\n",
    "            start -= extra_space\n",
    "            if text[end-1] in [',','.','!','?','*']:\n",
    "                end -= 1\n",
    "        else:\n",
    "            end -= (extra_space-2)\n",
    "            start -= extra_space\n",
    "    \n",
    "    pred = text[start:end]\n",
    "    if pred.count(\"'\") == 1:\n",
    "        if pred[0] == \"'\":\n",
    "            if text.find(pred) + len(pred) < len(text) and text[text.find(pred) + len(pred)] == \"'\":\n",
    "                pred += \"'\"\n",
    "        else:\n",
    "            if text.find(pred) - 1 >= 0 and text[text.find(pred) - 1] == \"'\":\n",
    "                pred = \"'\" + pred   \n",
    "                \n",
    "    return pred\n",
    "\n",
    "def post_shift(df):\n",
    "    df['pred_final'] = df['pred_exp'].copy()\n",
    "    df['jac_text'] = df.apply(lambda x: jaccard(x['text'], x['pred_exp']), axis=1)\n",
    "    mask = (df.sentiment != 'neutral') & (df.start_idx != 0) & (df.jac_text != 1)\n",
    "    df['pred_final'].loc[mask] = df.apply(lambda x: _post_shift_new(x['text'], x['pred_exp']), axis=1).loc[mask].values\n",
    "    jac = df.apply(lambda x: jaccard(x['pred_exp'], x['pred_final']), axis=1)\n",
    "    print(f\"# of modified samples: {np.sum(jac != 1)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "90fbfcfa-aa99-404e-9e09-41c6e96c2732",
    "_uuid": "aeedec43-804d-404d-ab67-b8ec44ce1cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of modified samples: 29\n",
      "# of modified samples: 159\n"
     ]
    }
   ],
   "source": [
    "# _get_pred_char bug ==> lb 717 (pred1)\n",
    "# POST_PROCESS bug ==> lb 0\n",
    "POST_PROCESS=True\n",
    "try:\n",
    "    df_test = _get_pred_char(df_test, np.array(df_test.prob_start.tolist()), np.array(df_test.prob_end.tolist()))\n",
    "    try:\n",
    "        if POST_PROCESS:\n",
    "            df_test = post_neutral(df_test)\n",
    "            df_test = post_shift(df_test)\n",
    "            df_test['selected_text'] = df_test['pred_final'].values\n",
    "        else:\n",
    "            df_test['selected_text'] = df_test['pred_char'].values\n",
    "    except:\n",
    "        df_test['selected_text'] = \" \"\n",
    "except:\n",
    "    df_test = pd.merge(df_test[['textID']], df_pred1, how='left', on='textID')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9952a025-077c-4d8f-9955-480135d632a6",
    "_uuid": "46e35f0c-887e-4034-a4ce-22112ab6afdf"
   },
   "source": [
    "# Save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "9e27ebcf-1d2b-42ba-9d8d-fd830cf1acd4",
    "_uuid": "c05f3dcd-070d-4111-8247-5472bc7abee4"
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(\"raw_prediction.csv\", index=False)\n",
    "df_sub = df_test[['textID', 'selected_text']].copy()\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "d3e9d398-fcfd-4b6d-ad36-0737bb0767f8",
    "_uuid": "ee82f25f-4cc8-4524-91b9-a4b642ad9904"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day http://twitpic.com/67ezh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>I like it!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                     selected_text\n",
       "0  f87dea47db  Last session of the day http://twitpic.com/67ezh\n",
       "1  96d74cb729                                          exciting\n",
       "2  eee518ae67                                            shame!\n",
       "3  01082688c6                                             happy\n",
       "4  33987a8ee5                                       I like it!!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cabea972-e514-4b15-94e0-6ce8c67f9b84",
    "_uuid": "f36c6310-9cfd-44f8-a0ae-a975ead9bb65"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
