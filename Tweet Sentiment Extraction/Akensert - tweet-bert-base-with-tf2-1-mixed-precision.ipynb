{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Complete TensorFlow mixed-precision implementation with Bert\n\n*It seems that mixed-precision isn't working in this kernel, but it does locally with suitable graphics card (try it out!), and reduces the runtime by a factor of 2.<br><br>*\n*Update 1: Corrected the implementation, so that it now works as it should.<br><br>*\n*Update 2: Small adjustments, and added hyperparameters to transformer model<br><br>*\n*Update 3: `\" \".join(set(selected.lower().split()))`<br><br>*\n*Update 4: `len(text[i].split()) < 2: decoded_text = text`<br><br>*\n*Update 5: Removing softmax step on predictions<br><br>*\n*Update 6: Small fixes.<br><br>*\n*Update 7: corrected `input_type_ids`<br><br>*\n\nDone in three steps:\n1. define preprocessing procedure and add it to tf.data.Dataset.from_generator\n2. define Bert model together with train, predict and decode_prediction functions\n3. run the K-fold cross-validation including everything defined in step 1. and 2.\n\nNote: A simple **post processing** is used: predicting all 'neutral' sentiments to full text.\n\ncredits to [abhishek](https://www.kaggle.com/abhishek) and all the 'get started' kernels to help/inspire me and many others to get started with this competition. The preprocessing in this notebook follows abhishek's [implementation](https://www.kaggle.com/abhishek/tweet-text-extraction-roberta-infer) with some modifications."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom math import ceil, floor\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom sklearn import model_selection\nfrom transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\nfrom tokenizers import BertWordPieceTokenizer\n\nimport logging\ntf.get_logger().setLevel(logging.ERROR)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n    \ntf.config.optimizer.set_jit(True)\ntf.config.optimizer.set_experimental_options(\n    {\"auto_mixed_precision\": True})","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read csv files\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntrain_df.dropna(inplace=True)\n\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_df.loc[:, \"selected_text\"] = test_df.text.values\n\nsubmission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n\nprint(\"train shape =\", train_df.shape)\nprint(\"test shape  =\", test_df.shape)\n\n# set some global variables\nPATH = \"../input/bert-base-uncased/\"\nMAX_SEQUENCE_LENGTH = 128\nTOKENIZER = BertWordPieceTokenizer(f\"{PATH}/vocab.txt\", lowercase=True, add_special_tokens=False)\n\n# let's take a look at the data\ntrain_df.head(10)","execution_count":8,"outputs":[{"output_type":"stream","text":"train shape = (27480, 4)\ntest shape  = (3534, 4)\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n7  50e14c0bb8                                         Soooo high   \n8  e050245fbd                                        Both of you   \n9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n\n                                       selected_text sentiment  \n0                I`d have responded, if I were going   neutral  \n1                                           Sooo SAD  negative  \n2                                        bullying me  negative  \n3                                     leave me alone  negative  \n4                                      Sons of ****,  negative  \n5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n6                                                fun  positive  \n7                                         Soooo high   neutral  \n8                                        Both of you   neutral  \n9                       Wow... u just became cooler.  positive  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>28b57f3990</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6e0c6d75b1</td>\n      <td>2am feedings for the baby are fun when he is a...</td>\n      <td>fun</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>50e14c0bb8</td>\n      <td>Soooo high</td>\n      <td>Soooo high</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>e050245fbd</td>\n      <td>Both of you</td>\n      <td>Both of you</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fc2cbefa9d</td>\n      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n      <td>Wow... u just became cooler.</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"```\nI. Set up preprocessing and dataset/datagenerator\n```\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef preprocess(tweet, selected_text, sentiment):\n    \"\"\"\n    Will be used in tf.data.Dataset.from_generator(...)\n    \n    \"\"\"\n    \n    # The original strings have been converted to \n    # byte strings, so we need to decode it\n    tweet = tweet.decode('utf-8')\n    selected_text = selected_text.decode('utf-8')\n    sentiment = sentiment.decode('utf-8')\n    \n    # Clean up the strings a bit\n    tweet = \" \".join(str(tweet).split())\n    selected_text = \" \".join(str(selected_text).split())\n    \n    # find the intersection between text and selected text\n    idx_start, idx_end = None, None\n    for index in (i for i, c in enumerate(tweet) if c == selected_text[0]):\n        if tweet[index:index+len(selected_text)] == selected_text:\n            idx_start = index\n            idx_end = index + len(selected_text)\n            break\n    \n    intersection = [0] * len(tweet)\n    if idx_start != None and idx_end != None:\n        for char_idx in range(idx_start, idx_end):\n            intersection[char_idx] = 1\n    \n    # tokenize with offsets\n    enc = TOKENIZER.encode(tweet)\n    input_ids_orig, offsets = enc.ids, enc.offsets\n\n    # compute targets\n    target_idx = []\n    for i, (o1, o2) in enumerate(offsets):\n        if sum(intersection[o1: o2]) > 0:\n            target_idx.append(i)\n    \n    target_start = target_idx[0]\n    target_end = target_idx[-1]\n\n    # add and pad data (hardcoded for BERT)\n    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n    sentiment_map = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699,\n    }\n    \n    input_ids = [101] + [sentiment_map[sentiment]] + [102] + input_ids_orig + [102]\n    input_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n    attention_mask = [1] * (len(input_ids_orig) + 4)\n    offsets = [(0, 0), (0, 0), (0, 0)] + offsets + [(0, 0)]\n    target_start += 3\n    target_end += 3\n\n    padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        attention_mask = attention_mask + ([0] * padding_length)\n        input_type_ids = input_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n    elif padding_length < 0:\n        input_ids = input_ids[:padding_length-1] + [102]\n        attention_mask = attention_mask[:padding_length-1] + [1]\n        input_type_ids = input_type_ids[:padding_length-1] + [1]\n        offsets = offsets[:padding_length-1] + [(0, 0)]\n        if target_start >= MAX_SEQUENCE_LENGTH:\n            target_start = MAX_SEQUENCE_LENGTH - 1\n        if target_end >= MAX_SEQUENCE_LENGTH:\n            target_end = MAX_SEQUENCE_LENGTH - 1\n        \n    return (\n        input_ids, attention_mask, input_type_ids, offsets,\n        target_start, target_end, tweet, selected_text, sentiment, \n    )\n\n\nclass TweetSentimentDataset(tf.data.Dataset):\n    \n    OUTPUT_TYPES = (\n        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n    )\n    \n    OUTPUT_SHAPES = (\n        (MAX_SEQUENCE_LENGTH,),   (MAX_SEQUENCE_LENGTH,), (MAX_SEQUENCE_LENGTH,), \n        (MAX_SEQUENCE_LENGTH, 2), (),                     (),\n        (),                       (),                     (),\n    )\n    \n    # AutoGraph will automatically convert Python code to\n    # Tensorflow graph code. You could also wrap 'preprocess' \n    # in tf.py_function(..) for arbitrary python code\n    def _generator(tweet, selected_text, sentiment):\n        for tw, st, se in zip(tweet, selected_text, sentiment):\n            yield preprocess(tw, st, se)\n    \n    # This dataset object will return a generator\n    def __new__(cls, tweet, selected_text, sentiment):\n        return tf.data.Dataset.from_generator(\n            cls._generator,\n            output_types=cls.OUTPUT_TYPES,\n            output_shapes=cls.OUTPUT_SHAPES,\n            args=(tweet, selected_text, sentiment)\n        )\n    \n    @staticmethod\n    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n        dataset = TweetSentimentDataset(\n            dataframe.text.values, \n            dataframe.selected_text.values, \n            dataframe.sentiment.values\n        )\n\n        dataset = dataset.cache()\n        if shuffle_buffer_size != -1:\n            dataset = dataset.shuffle(shuffle_buffer_size)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n        return dataset\n        ","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nII. Set up transformer model and functions\n```"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"class BertQAModel(TFBertPreTrainedModel):\n    \n    DROPOUT_RATE = 0.1\n    NUM_HIDDEN_STATES = 2\n    \n    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        \n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        self.concat = L.Concatenate()\n        self.dropout = L.Dropout(self.DROPOUT_RATE)\n        self.qa_outputs = L.Dense(\n            config.num_labels, \n            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n            dtype='float32',\n            name=\"qa_outputs\")\n        \n    @tf.function\n    def call(self, inputs, **kwargs):\n        # outputs: Tuple[sequence, pooled, hidden_states]\n        _, _, hidden_states = self.bert(inputs, **kwargs)\n        \n        hidden_states = self.concat([\n            hidden_states[-i] for i in range(1, self.NUM_HIDDEN_STATES+1)\n        ])\n        \n        hidden_states = self.dropout(hidden_states, training=kwargs.get(\"training\", False))\n        logits = self.qa_outputs(hidden_states)\n        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n        start_logits = tf.squeeze(start_logits, axis=-1)\n        end_logits = tf.squeeze(end_logits, axis=-1)\n        \n        return start_logits, end_logits\n    \n    \ndef train(model, dataset, loss_fn, optimizer):\n    \n    @tf.function\n    def train_step(model, inputs, y_true, loss_fn, optimizer):\n        with tf.GradientTape() as tape:\n            y_pred = model(inputs, training=True)\n            loss  = loss_fn(y_true[0], y_pred[0])\n            loss += loss_fn(y_true[1], y_pred[1])\n            scaled_loss = optimizer.get_scaled_loss(loss)\n    \n        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, y_pred\n\n    epoch_loss = 0.\n    for batch_num, sample in enumerate(dataset):\n        loss, y_pred = train_step(\n            model, sample[:3], sample[4:6], loss_fn, optimizer)\n\n        epoch_loss += loss\n\n        print(\n            f\"training ... batch {batch_num+1:03d} : \"\n            f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n            end='\\r')\n        \n        \ndef predict(model, dataset, loss_fn, optimizer):\n    \n    @tf.function\n    def predict_step(model, inputs):\n        return model(inputs)\n        \n    def to_numpy(*args):\n        out = []\n        for arg in args:\n            if arg.dtype == tf.string:\n                arg = [s.decode('utf-8') for s in arg.numpy()]\n                out.append(arg)\n            else:\n                arg = arg.numpy()\n                out.append(arg)\n        return out\n    \n    # Initialize accumulators\n    offset = tf.zeros([0, MAX_SEQUENCE_LENGTH, 2], dtype=tf.dtypes.int32)\n    text = tf.zeros([0,], dtype=tf.dtypes.string)\n    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n    pred_start = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n    pred_end = tf.zeros([0, MAX_SEQUENCE_LENGTH], dtype=tf.dtypes.float32)\n    \n    for batch_num, sample in enumerate(dataset):\n        \n        print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n        \n        y_pred = predict_step(model, sample[:3])\n        \n        # add batch to accumulators\n        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n        offset = tf.concat((offset, sample[3]), axis=0)\n        text = tf.concat((text, sample[6]), axis=0)\n        selected_text = tf.concat((selected_text, sample[7]), axis=0)\n        sentiment = tf.concat((sentiment, sample[8]), axis=0)\n\n    # pred_start = tf.nn.softmax(pred_start)\n    # pred_end = tf.nn.softmax(pred_end)\n    \n    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n    \n    return pred_start, pred_end, text, selected_text, sentiment, offset\n\n\ndef decode_prediction(pred_start, pred_end, text, offset, sentiment):\n    \n    def decode(pred_start, pred_end, text, offset):\n\n        decoded_text = \"\"\n        for i in range(pred_start, pred_end+1):\n            decoded_text += text[offset[i][0]:offset[i][1]]\n            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n                decoded_text += \" \"\n        return decoded_text\n    \n    decoded_predictions = []\n    for i in range(len(text)):\n        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n            decoded_text = text[i]\n        else:\n            idx_start = np.argmax(pred_start[i])\n            idx_end = np.argmax(pred_end[i])\n            if idx_start > idx_end:\n                idx_end = idx_start \n            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n            if len(decoded_text) == 0:\n                decoded_text = text[i]\n        decoded_predictions.append(decoded_text)\n    \n    return decoded_predictions\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nIII. Run it all: \n\nmodel.create() -> dataset.create() -> train(train) ->\n       -> predict(val).decode() -> predict(test).decode() -> submit\n```"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"num_folds = 4\nnum_epochs = 3\nbatch_size = 32\nlearning_rate = 3e-5\n\noptimizer = tf.keras.optimizers.Adam(learning_rate)\noptimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n    optimizer, 'dynamic')\n\nconfig = BertConfig(output_hidden_states=True, num_labels=2)\nBertQAModel.DROPOUT_RATE = 0.2\nBertQAModel.NUM_HIDDEN_STATES = 2\nmodel = BertQAModel.from_pretrained(PATH, config=config)\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nkfold = model_selection.StratifiedKFold(\n    n_splits=num_folds, shuffle=True, random_state=42)\n\n# initialize test predictions\ntest_preds_start = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\ntest_preds_end = np.zeros((len(test_df), MAX_SEQUENCE_LENGTH), dtype=np.float32)\n\nfor fold_num, (train_idx, valid_idx) in enumerate(\n        kfold.split(X=train_df.text, y=train_df.sentiment.values)):\n    print(\"\\nfold %02d\" % (fold_num+1))\n        \n    train_dataset = TweetSentimentDataset.create(\n        train_df.iloc[train_idx], batch_size, shuffle_buffer_size=2048)\n    valid_dataset = TweetSentimentDataset.create(\n        train_df.iloc[valid_idx], batch_size, shuffle_buffer_size=-1)\n    test_dataset = TweetSentimentDataset.create(\n        test_df, batch_size, shuffle_buffer_size=-1)\n    \n    best_score = float('-inf')\n    for epoch_num in range(num_epochs):\n        print(\"\\nepoch %03d\" % (epoch_num+1))\n        \n        # train for an epoch\n        train(model, train_dataset, loss_fn, optimizer)\n        \n        # predict validation set and compute jaccardian distances\n        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n            predict(model, valid_dataset, loss_fn, optimizer)\n        \n        selected_text_pred = decode_prediction(\n            pred_start, pred_end, text, offset, sentiment)\n        jaccards = []\n        for i in range(len(selected_text)):\n            jaccards.append(\n                jaccard(selected_text[i], selected_text_pred[i]))\n        \n        score = np.mean(jaccards)\n        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n        \n        if score > best_score:\n            best_score = score\n            # requires you to have 'fold-{fold_num}' folder in PATH:\n            # model.save_pretrained(PATH+f'fold-{fold_num}')\n            # or\n            # model.save_weights(PATH + f'fold-{fold_num}.h5')\n            \n            # predict test set\n            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n                predict(model, test_dataset, loss_fn, optimizer)\n    \n    # add epoch's best test preds to test preds arrays\n    test_preds_start += test_pred_start\n    test_preds_end += test_pred_end\n    \n    # reset model, as well as session and graph (to avoid OOM issues?) \n    session = tf.compat.v1.get_default_session()\n    graph = tf.compat.v1.get_default_graph()\n    del session, graph, model\n    model = BertQAModel.from_pretrained(PATH, config=config)\n    \n# decode test set and add to submission file\nselected_text_pred = decode_prediction(\n    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n\nsubmission_df.loc[:, 'selected_text'] = selected_text_pred\nsubmission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[{"output_type":"stream","text":"\nfold 01\n\nepoch 001\ntraining ... batch 091 : train loss 3.694  \r","name":"stdout"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}