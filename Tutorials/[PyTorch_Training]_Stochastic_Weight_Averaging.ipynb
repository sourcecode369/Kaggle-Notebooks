{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[PyTorch Training] Stochastic Weight Averaging.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w--c3T4b8yjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "6e2d70b2-bc65-481a-eaeb-3315448d6816"
      },
      "source": [
        "!pip install pretrainedmodels\n",
        "!pip install torch_optimizer\n",
        "!pip install torchcontrib"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.6.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.6/dist-packages (0.0.1a14)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.5.1+cu101)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Requirement already satisfied: torchcontrib in /usr/local/lib/python3.6/dist-packages (0.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50T1XeQ680P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import sys\n",
        "import time\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, models, datasets\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "\n",
        "import pretrainedmodels\n",
        "import torchcontrib\n",
        "from torchcontrib.optim import SWA\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgcMhpyW9CpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
        "resize = transforms.Resize((224,224))\n",
        "\n",
        "augmentations = transforms.Compose([\n",
        "                                    resize,\n",
        "                                    to_rgb,\n",
        "                                    transforms.RandomResizedCrop(224),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.RandomVerticalFlip(),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    normalize\n",
        "                                    ])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "  os.path.join(\"/tmp/fashionmnist\"),\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform=augmentations)\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "  os.path.join(\"/tmp/fashionmnist\"),\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=augmentations)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm_SS4sDfbqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def schedule(epoch):\n",
        "    t = (epoch) / (swa_start)\n",
        "    lr_ratio = swa_lr / lr_init\n",
        "    if t <= 0.5:\n",
        "        factor = 1.0\n",
        "    elif t <= 0.9:\n",
        "        factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
        "    else:\n",
        "        factor = lr_ratio\n",
        "    return lr_init * factor\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz7c7PFwhja8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "27f42d23-3dd5-4d74-ec3f-69035a4b61c9"
      },
      "source": [
        "# Training Variables\n",
        "bs = 128\n",
        "epochs = 6\n",
        "num_classes = 10\n",
        "swa_start = 4\n",
        "swa_lr = 5e-4\n",
        "lr_init = 1e-4\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# DatLoaders\n",
        "train_dl = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
        "test_dl = DataLoader(test_dataset, batch_size=bs, shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
        "\n",
        "# Steps per epoch\n",
        "steps_per_epoch = len(train_dl.dataset) / bs\n",
        "steps_per_epoch = int(steps_per_epoch)\n",
        "print(\"Steps per epoch:\", steps_per_epoch)\n",
        "\n",
        "# Model\n",
        "model = pretrainedmodels.__dict__['resnet18'](pretrained=\"imagenet\")\n",
        "model.last_linear.out_features = num_classes\n",
        "model.to(device)\n",
        "if torch.cuda.device_count()>1:\n",
        "    model = nn.DataParallel(model)\n",
        "    \n",
        "# Loss\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Optimizer\n",
        "base_optimizer = optim.RAdam(model.parameters(), \n",
        "                        lr=1e-3, \n",
        "                        betas=(0.9,0.999), \n",
        "                        eps=1e-3, \n",
        "                        weight_decay=1e-4)\n",
        "\n",
        "# Stochastic Weight Averaging\n",
        "optimizer = SWA(base_optimizer, swa_start=swa_start * steps_per_epoch,\n",
        "                swa_freq=steps_per_epoch, swa_lr=swa_lr)\n",
        "print('SGD training')\n",
        "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(train_dl))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps per epoch: 468\n",
            "SGD training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOqwp009GEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNRgduUh9IQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "5798a45e-cb2c-4d31-a728-aeb4c16d8180"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    # Train\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    learning_rate = AverageMeter('LR',':2.8f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_dl),\n",
        "        [batch_time, data_time, losses, top1, top5, learning_rate],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "    lr = schedule(epoch)\n",
        "    adjust_learning_rate(optimizer, lr)\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "    for i, (images, targets) in enumerate(train_dl):\n",
        "        data_time.update(time.time()-end)\n",
        "        images = images.cuda(non_blocking=True)\n",
        "        targets = targets.cuda(non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc1, acc5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "        learning_rate.update(optimizer.param_groups[0]['lr'])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if i %100 == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "    # Validation\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    learning_rate = AverageMeter('LR',':2.8f')\n",
        "    progress = ProgressMeter(\n",
        "        len(test_dl),\n",
        "        [batch_time, losses, top1, top5, learning_rate],\n",
        "        prefix='Test: ')\n",
        "    if epoch>=swa_start:\n",
        "        if epoch==swa_start:\n",
        "            print('Starting SWA Training..')\n",
        "        optimizer.swap_swa_sgd()\n",
        "        optimizer.bn_update(train_dl, model, device='cuda')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (images, targets) in enumerate(test_dl):\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            targets = targets.cuda(non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            acc1, acc5 = accuracy(outputs, targets, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "            learning_rate.update(optimizer.param_groups[0]['lr'])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if i %100 == 0:\n",
        "                progress.display(i)\n",
        "    if epoch>=swa_start:\n",
        "        optimizer.swap_swa_sgd()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][  0/468]\tTime  2.259 ( 2.259)\tData  1.802 ( 1.802)\tLoss 9.4098e+00 (9.4098e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [0][100/468]\tTime  0.459 ( 0.339)\tData  0.272 ( 0.144)\tLoss 3.3850e+00 (6.6131e+00)\tAcc@1  39.06 ( 11.70)\tAcc@5  62.50 ( 21.04)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [0][200/468]\tTime  0.571 ( 0.324)\tData  0.434 ( 0.131)\tLoss 1.6739e+00 (4.4685e+00)\tAcc@1  63.28 ( 32.17)\tAcc@5  91.41 ( 52.01)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [0][300/468]\tTime  0.508 ( 0.320)\tData  0.377 ( 0.126)\tLoss 1.1103e+00 (3.4298e+00)\tAcc@1  67.97 ( 42.92)\tAcc@5  96.09 ( 66.12)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [0][400/468]\tTime  0.544 ( 0.320)\tData  0.429 ( 0.126)\tLoss 1.2800e+00 (2.8497e+00)\tAcc@1  66.41 ( 48.98)\tAcc@5  94.53 ( 73.68)\tLR 0.00010000 (0.00010000)\n",
            "Test: [ 0/78]\tTime  1.448 ( 1.448)\tLoss 8.6788e-01 (8.6788e-01)\tAcc@1  71.09 ( 71.09)\tAcc@5  96.09 ( 96.09)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [1][  0/468]\tTime  1.669 ( 1.669)\tData  1.499 ( 1.499)\tLoss 1.0115e+00 (1.0115e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  96.88 ( 96.88)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [1][100/468]\tTime  0.577 ( 0.335)\tData  0.444 ( 0.142)\tLoss 7.8147e-01 (8.7118e-01)\tAcc@1  71.88 ( 72.02)\tAcc@5  95.31 ( 97.52)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [1][200/468]\tTime  0.639 ( 0.328)\tData  0.493 ( 0.134)\tLoss 7.7859e-01 (8.4349e-01)\tAcc@1  77.34 ( 72.58)\tAcc@5  99.22 ( 97.71)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [1][300/468]\tTime  0.286 ( 0.325)\tData  0.005 ( 0.134)\tLoss 8.0330e-01 (8.1134e-01)\tAcc@1  70.31 ( 73.39)\tAcc@5 100.00 ( 97.89)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [1][400/468]\tTime  0.238 ( 0.321)\tData  0.010 ( 0.130)\tLoss 7.8410e-01 (7.8284e-01)\tAcc@1  72.66 ( 74.12)\tAcc@5  96.88 ( 98.05)\tLR 0.00010000 (0.00010000)\n",
            "Test: [ 0/78]\tTime  1.300 ( 1.300)\tLoss 6.4389e-01 (6.4389e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  99.22 ( 99.22)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [2][  0/468]\tTime  1.575 ( 1.575)\tData  1.359 ( 1.359)\tLoss 6.9831e-01 (6.9831e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  99.22 ( 99.22)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [2][100/468]\tTime  0.637 ( 0.333)\tData  0.500 ( 0.141)\tLoss 5.5760e-01 (6.4075e-01)\tAcc@1  78.12 ( 77.75)\tAcc@5  98.44 ( 98.86)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [2][200/468]\tTime  0.566 ( 0.322)\tData  0.444 ( 0.130)\tLoss 4.5070e-01 (6.3906e-01)\tAcc@1  83.59 ( 77.74)\tAcc@5  98.44 ( 98.74)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [2][300/468]\tTime  0.586 ( 0.320)\tData  0.423 ( 0.128)\tLoss 6.3739e-01 (6.3016e-01)\tAcc@1  79.69 ( 78.00)\tAcc@5  99.22 ( 98.82)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [2][400/468]\tTime  0.526 ( 0.317)\tData  0.359 ( 0.128)\tLoss 4.4681e-01 (6.2292e-01)\tAcc@1  85.16 ( 78.22)\tAcc@5  99.22 ( 98.83)\tLR 0.00010000 (0.00010000)\n",
            "Test: [ 0/78]\tTime  1.766 ( 1.766)\tLoss 6.0096e-01 (6.0096e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  99.22 ( 99.22)\tLR 0.00010000 (0.00010000)\n",
            "Epoch: [3][  0/468]\tTime  1.736 ( 1.736)\tData  1.534 ( 1.534)\tLoss 5.8787e-01 (5.8787e-01)\tAcc@1  78.91 ( 78.91)\tAcc@5  99.22 ( 99.22)\tLR 0.00035000 (0.00035000)\n",
            "Epoch: [3][100/468]\tTime  0.781 ( 0.346)\tData  0.661 ( 0.148)\tLoss 5.6312e-01 (6.5249e-01)\tAcc@1  83.59 ( 76.91)\tAcc@5  98.44 ( 98.67)\tLR 0.00035000 (0.00035000)\n",
            "Epoch: [3][200/468]\tTime  0.732 ( 0.334)\tData  0.574 ( 0.138)\tLoss 6.2775e-01 (6.3907e-01)\tAcc@1  75.00 ( 77.38)\tAcc@5  99.22 ( 98.73)\tLR 0.00035000 (0.00035000)\n",
            "Epoch: [3][300/468]\tTime  0.586 ( 0.328)\tData  0.419 ( 0.133)\tLoss 4.3913e-01 (6.1420e-01)\tAcc@1  84.38 ( 78.23)\tAcc@5  97.66 ( 98.80)\tLR 0.00035000 (0.00035000)\n",
            "Epoch: [3][400/468]\tTime  0.563 ( 0.328)\tData  0.431 ( 0.133)\tLoss 6.6267e-01 (6.0083e-01)\tAcc@1  78.91 ( 78.61)\tAcc@5  95.31 ( 98.86)\tLR 0.00035000 (0.00035000)\n",
            "Test: [ 0/78]\tTime  1.839 ( 1.839)\tLoss 6.0434e-01 (6.0434e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  99.22 ( 99.22)\tLR 0.00035000 (0.00035000)\n",
            "Epoch: [4][  0/468]\tTime  1.819 ( 1.819)\tData  1.677 ( 1.677)\tLoss 7.6798e-01 (7.6798e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  99.22 ( 99.22)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [4][100/468]\tTime  0.792 ( 0.337)\tData  0.671 ( 0.149)\tLoss 4.5546e-01 (5.4821e-01)\tAcc@1  83.59 ( 80.55)\tAcc@5 100.00 ( 99.17)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [4][200/468]\tTime  0.671 ( 0.322)\tData  0.502 ( 0.133)\tLoss 7.0930e-01 (5.3655e-01)\tAcc@1  78.12 ( 80.79)\tAcc@5 100.00 ( 99.16)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [4][300/468]\tTime  0.742 ( 0.319)\tData  0.590 ( 0.129)\tLoss 6.6605e-01 (5.2997e-01)\tAcc@1  76.56 ( 80.84)\tAcc@5  98.44 ( 99.22)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [4][400/468]\tTime  0.622 ( 0.319)\tData  0.485 ( 0.129)\tLoss 6.3646e-01 (5.2350e-01)\tAcc@1  78.12 ( 81.11)\tAcc@5  98.44 ( 99.22)\tLR 0.00050000 (0.00050000)\n",
            "Starting SWA Training..\n",
            "Test: [ 0/78]\tTime  1.712 ( 1.712)\tLoss 5.1335e-01 (5.1335e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  99.22 ( 99.22)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [5][  0/468]\tTime  1.877 ( 1.877)\tData  1.739 ( 1.739)\tLoss 5.3472e-01 (5.3472e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  96.88 ( 96.88)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [5][100/468]\tTime  0.689 ( 0.324)\tData  0.564 ( 0.130)\tLoss 5.7092e-01 (4.7954e-01)\tAcc@1  81.25 ( 82.73)\tAcc@5  99.22 ( 99.25)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [5][200/468]\tTime  0.414 ( 0.312)\tData  0.292 ( 0.119)\tLoss 6.0791e-01 (4.8066e-01)\tAcc@1  76.56 ( 82.61)\tAcc@5  99.22 ( 99.30)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [5][300/468]\tTime  0.224 ( 0.312)\tData  0.000 ( 0.123)\tLoss 3.3874e-01 (4.7772e-01)\tAcc@1  87.50 ( 82.79)\tAcc@5 100.00 ( 99.31)\tLR 0.00050000 (0.00050000)\n",
            "Epoch: [5][400/468]\tTime  0.233 ( 0.310)\tData  0.000 ( 0.124)\tLoss 4.6084e-01 (4.7585e-01)\tAcc@1  85.94 ( 82.92)\tAcc@5  99.22 ( 99.36)\tLR 0.00050000 (0.00050000)\n",
            "Test: [ 0/78]\tTime  1.523 ( 1.523)\tLoss 3.4898e-01 (3.4898e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5 100.00 (100.00)\tLR 0.00050000 (0.00050000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qJnCdfJ-Z2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}