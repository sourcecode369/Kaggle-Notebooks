{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Pytorch Training] TPU ConvNets and MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+pgck94VzrN8njO6/G57K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/Kaggle-Notebooks/blob/master/Tutorials/tpu/pytorch/%5BPytorch_Training%5D_TPU_ConvNets_and_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UESL5AQ1wfGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MkFOdPwwsQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0622905d-2d79-4ce0-e24f-b2bd58f8cc61"
      },
      "source": [
        "VERSION = \"20200516\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4139  100  4139    0     0  23787      0 --:--:-- --:--:-- --:--:-- 23787\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200516 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (47.3.1)\n",
            "Uninstalling torch-1.5.1+cu101:\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.5.1+cu101\n",
            "Uninstalling torchvision-0.6.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200516-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200516-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][119.8 MiB/119.8 MiB]                                                \n",
            "Operation completed over 1 objects/119.8 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200516-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200516-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200516) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200516) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+83df3be\n",
            "Processing ./torch_xla-nightly+20200516-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2191422\n",
            "Processing ./torchvision-nightly+20200516-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200516) (1.6.0a0+83df3be)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200516) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200516) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200516) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+348dd5a\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (359 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX1FWIQ8wwsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/tmp/test_result.png'\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize((-0.1307/0.3081,), (1/0.3081,))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(11, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.squeeze() # [1,Y,X] -> [Y,X]\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(label, prediction), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMMSDHB-zTMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['datadir'] = \"/tmp/mnist\"\n",
        "FLAGS['batch_size'] = 128\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['learning_rate'] = 0.01\n",
        "FLAGS['momentum'] = 0.5\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['num_cores'] = 8\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kM7Pxuj6tTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "\n",
        "class MNIST(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNIST, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.bn1 = nn.BatchNorm2d(10)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.bn2 = nn.BatchNorm2d(20)\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = self.bn1(x)\n",
        "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "    x = self.bn2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Only instantiate model weights once in memory.\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(MNIST())\n",
        "\n",
        "def train_mnist():\n",
        "  torch.manual_seed(1)\n",
        "  \n",
        "  def get_dataset():\n",
        "    norm = transforms.Normalize((0.1307,), (0.3081,))\n",
        "    train_dataset = datasets.MNIST(\n",
        "        FLAGS['datadir'],\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(), norm]))\n",
        "    test_dataset = datasets.MNIST(\n",
        "        FLAGS['datadir'],\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(), norm]))\n",
        "    \n",
        "    return train_dataset, test_dataset\n",
        "  \n",
        "  # Using the serial executor avoids multiple processes to\n",
        "  # download the same data.\n",
        "  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n",
        "\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      shuffle=False,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  # Scale learning rate to world size\n",
        "  lr = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  # Get loss function, optimizer, and model\n",
        "  device = xm.xla_device()\n",
        "  model = WRAPPED_MODEL.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=FLAGS['momentum'])\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_loop_fn(loader):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for x, (data, target) in enumerate(loader):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer)\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "      if x % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
        "            tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "  def test_loop_fn(loader):\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    data, pred, target = None, None, None\n",
        "    for data, target in loader:\n",
        "      output = model(data)\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      total_samples += data.size()[0]\n",
        "\n",
        "    accuracy = 100.0 * correct / total_samples\n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, data, pred, target\n",
        "\n",
        "  # Train and eval loops\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
        "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
        "    if FLAGS['metrics_debug']:\n",
        "      xm.master_print(met.metrics_report(), flush=True)\n",
        "\n",
        "  return accuracy, data, pred, target"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GisX16q_7-xi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "693d5745-7ddd-4956-8359-494d236928be"
      },
      "source": [
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target = train_mnist()\n",
        "  if rank == 0:\n",
        "    # Retrieve tensors that are on TPU core 0 and plot.\n",
        "    plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "          start_method='fork')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[xla:0](0) Loss=2.34535 Rate=455.29 GlobalRate=455.26 Time=Sat Jul  4 13:49:05 2020\n",
            "[xla:2](0) Loss=2.30091 Rate=395.12 GlobalRate=395.08 Time=Sat Jul  4 13:49:05 2020\n",
            "[xla:6](0) Loss=2.34904 Rate=230.73 GlobalRate=230.72 Time=Sat Jul  4 13:49:06 2020\n",
            "[xla:4](0) Loss=2.39988 Rate=179.55 GlobalRate=179.54 Time=Sat Jul  4 13:49:06 2020\n",
            "[xla:3](0) Loss=2.33181 Rate=172.86 GlobalRate=172.85 Time=Sat Jul  4 13:49:07 2020\n",
            "[xla:1](0) Loss=2.39467 Rate=418.41 GlobalRate=418.39 Time=Sat Jul  4 13:49:12 2020\n",
            "[xla:5](0) Loss=2.32317 Rate=429.43 GlobalRate=429.39 Time=Sat Jul  4 13:49:15 2020\n",
            "[xla:7](0) Loss=2.35452 Rate=477.01 GlobalRate=476.97 Time=Sat Jul  4 13:49:17 2020\n",
            "[xla:1](20) Loss=0.28624 Rate=395.03 GlobalRate=381.13 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:6](20) Loss=0.26764 Rate=211.59 GlobalRate=200.15 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:0](20) Loss=0.38768 Rate=291.76 GlobalRate=188.10 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:5](20) Loss=0.35496 Rate=596.34 GlobalRate=686.44 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:2](20) Loss=0.37553 Rate=272.74 GlobalRate=195.97 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:4](20) Loss=0.36608 Rate=196.99 GlobalRate=207.02 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:3](20) Loss=0.35720 Rate=196.89 GlobalRate=210.58 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:7](20) Loss=0.35845 Rate=772.32 GlobalRate=923.80 Time=Sat Jul  4 13:49:19 2020\n",
            "[xla:4](40) Loss=0.13095 Rate=541.26 GlobalRate=321.86 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:1](40) Loss=0.14552 Rate=617.95 GlobalRate=504.99 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:3](40) Loss=0.22176 Rate=548.63 GlobalRate=327.31 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:0](40) Loss=0.22692 Rate=576.96 GlobalRate=297.71 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:5](40) Loss=0.16239 Rate=699.21 GlobalRate=723.85 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:2](40) Loss=0.17247 Rate=570.36 GlobalRate=307.87 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:6](40) Loss=0.23731 Rate=545.60 GlobalRate=313.09 Time=Sat Jul  4 13:49:22 2020\n",
            "[xla:7](40) Loss=0.17517 Rate=794.68 GlobalRate=864.32 Time=Sat Jul  4 13:49:22 2020\n",
            "Finished training epoch 1\n",
            "[xla:2] Accuracy=96.65%\n",
            "[xla:7] Accuracy=96.63%\n",
            "[xla:4] Accuracy=96.62%\n",
            "[xla:3] Accuracy=96.63%\n",
            "[xla:1] Accuracy=96.63%\n",
            "[xla:0] Accuracy=96.61%\n",
            "[xla:6] Accuracy=96.62%\n",
            "[xla:5] Accuracy=96.61%\n",
            "[xla:2](0) Loss=0.12689 Rate=129.71 GlobalRate=129.71 Time=Sat Jul  4 13:49:37 2020\n",
            "[xla:4](0) Loss=0.11761 Rate=114.50 GlobalRate=114.50 Time=Sat Jul  4 13:49:37 2020\n",
            "[xla:7](0) Loss=0.21391 Rate=102.93 GlobalRate=102.93 Time=Sat Jul  4 13:49:37 2020\n",
            "[xla:3](0) Loss=0.08599 Rate=102.41 GlobalRate=102.40 Time=Sat Jul  4 13:49:37 2020\n",
            "[xla:6](0) Loss=0.15646 Rate=103.36 GlobalRate=103.35 Time=Sat Jul  4 13:49:37 2020\n",
            "[xla:1](0) Loss=0.16557 Rate=99.65 GlobalRate=99.65 Time=Sat Jul  4 13:49:37 2020\n",
            "[xla:0](0) Loss=0.15350 Rate=86.21 GlobalRate=86.21 Time=Sat Jul  4 13:49:38 2020\n",
            "[xla:5](0) Loss=0.12134 Rate=79.02 GlobalRate=79.01 Time=Sat Jul  4 13:49:38 2020\n",
            "[xla:2](20) Loss=0.14545 Rate=514.19 GlobalRate=623.76 Time=Sat Jul  4 13:49:40 2020\n",
            "[xla:4](20) Loss=0.14167 Rate=525.67 GlobalRate=622.40 Time=Sat Jul  4 13:49:40 2020\n",
            "[xla:7](20) Loss=0.14876 Rate=511.71 GlobalRate=596.29 Time=Sat Jul  4 13:49:41 2020\n",
            "[xla:3](20) Loss=0.15593 Rate=509.91 GlobalRate=593.99 Time=Sat Jul  4 13:49:41 2020\n",
            "[xla:6](20) Loss=0.06242 Rate=499.92 GlobalRate=585.89 Time=Sat Jul  4 13:49:41 2020\n",
            "[xla:1](20) Loss=0.05655 Rate=498.01 GlobalRate=579.67 Time=Sat Jul  4 13:49:41 2020\n",
            "[xla:5](20) Loss=0.12244 Rate=523.21 GlobalRate=566.56 Time=Sat Jul  4 13:49:41 2020\n",
            "[xla:0](20) Loss=0.12231 Rate=501.44 GlobalRate=563.04 Time=Sat Jul  4 13:49:41 2020\n",
            "[xla:4](40) Loss=0.05343 Rate=669.65 GlobalRate=684.90 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:2](40) Loss=0.07939 Rate=656.78 GlobalRate=680.30 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:7](40) Loss=0.08428 Rate=663.89 GlobalRate=668.29 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:3](40) Loss=0.11856 Rate=650.33 GlobalRate=658.76 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:1](40) Loss=0.07215 Rate=659.22 GlobalRate=657.96 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:0](40) Loss=0.14569 Rate=667.75 GlobalRate=650.95 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:6](40) Loss=0.10748 Rate=648.41 GlobalRate=654.93 Time=Sat Jul  4 13:49:44 2020\n",
            "[xla:5](40) Loss=0.06653 Rate=657.72 GlobalRate=642.38 Time=Sat Jul  4 13:49:44 2020\n",
            "Finished training epoch 2\n",
            "[xla:2] Accuracy=97.66%\n",
            "[xla:6] Accuracy=97.63%\n",
            "[xla:0] Accuracy=97.65%\n",
            "[xla:1] Accuracy=97.62%\n",
            "[xla:4] Accuracy=97.66%\n",
            "[xla:7] Accuracy=97.64%\n",
            "[xla:3] Accuracy=97.66%\n",
            "[xla:5] Accuracy=97.64%\n",
            "[xla:2](0) Loss=0.08806 Rate=119.01 GlobalRate=119.01 Time=Sat Jul  4 13:50:00 2020\n",
            "[xla:6](0) Loss=0.08957 Rate=111.15 GlobalRate=111.15 Time=Sat Jul  4 13:50:00 2020\n",
            "[xla:4](0) Loss=0.06510 Rate=107.60 GlobalRate=107.60 Time=Sat Jul  4 13:50:00 2020\n",
            "[xla:0](0) Loss=0.11970 Rate=104.60 GlobalRate=104.59 Time=Sat Jul  4 13:50:00 2020\n",
            "[xla:1](0) Loss=0.09358 Rate=98.65 GlobalRate=98.65 Time=Sat Jul  4 13:50:01 2020\n",
            "[xla:7](0) Loss=0.16538 Rate=92.47 GlobalRate=92.47 Time=Sat Jul  4 13:50:01 2020\n",
            "[xla:5](0) Loss=0.05957 Rate=93.33 GlobalRate=93.33 Time=Sat Jul  4 13:50:01 2020\n",
            "[xla:3](0) Loss=0.04508 Rate=88.17 GlobalRate=88.17 Time=Sat Jul  4 13:50:01 2020\n",
            "[xla:2](20) Loss=0.10860 Rate=507.95 GlobalRate=609.23 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:1](20) Loss=0.03182 Rate=519.97 GlobalRate=598.12 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:6](20) Loss=0.03392 Rate=500.10 GlobalRate=594.34 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:0](20) Loss=0.09330 Rate=491.35 GlobalRate=579.21 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:7](20) Loss=0.11196 Rate=505.92 GlobalRate=576.85 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:4](20) Loss=0.09835 Rate=479.50 GlobalRate=570.84 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:5](20) Loss=0.08548 Rate=492.50 GlobalRate=566.36 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:3](20) Loss=0.13082 Rate=495.17 GlobalRate=560.99 Time=Sat Jul  4 13:50:04 2020\n",
            "[xla:2](40) Loss=0.05109 Rate=645.75 GlobalRate=665.75 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:6](40) Loss=0.06231 Rate=652.03 GlobalRate=662.54 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:1](40) Loss=0.04850 Rate=655.71 GlobalRate=662.23 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:4](40) Loss=0.03470 Rate=652.17 GlobalRate=652.31 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:0](40) Loss=0.12692 Rate=643.05 GlobalRate=649.44 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:7](40) Loss=0.05760 Rate=655.63 GlobalRate=652.04 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:5](40) Loss=0.03810 Rate=644.84 GlobalRate=641.88 Time=Sat Jul  4 13:50:07 2020\n",
            "[xla:3](40) Loss=0.08395 Rate=650.27 GlobalRate=640.92 Time=Sat Jul  4 13:50:07 2020\n",
            "Finished training epoch 3\n",
            "[xla:4] Accuracy=97.96%\n",
            "[xla:5] Accuracy=97.92%\n",
            "[xla:3] Accuracy=97.95%\n",
            "[xla:0] Accuracy=97.97%\n",
            "[xla:2] Accuracy=97.96%\n",
            "[xla:1] Accuracy=97.92%\n",
            "[xla:7] Accuracy=97.95%\n",
            "[xla:6] Accuracy=97.95%\n",
            "[xla:5](0) Loss=0.03939 Rate=111.32 GlobalRate=111.32 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:4](0) Loss=0.04585 Rate=114.58 GlobalRate=114.58 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:3](0) Loss=0.03799 Rate=115.48 GlobalRate=115.48 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:2](0) Loss=0.07304 Rate=103.51 GlobalRate=103.51 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:1](0) Loss=0.06627 Rate=94.24 GlobalRate=94.24 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:7](0) Loss=0.13777 Rate=91.94 GlobalRate=91.94 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:0](0) Loss=0.10135 Rate=86.43 GlobalRate=86.43 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:6](0) Loss=0.06673 Rate=86.03 GlobalRate=86.03 Time=Sat Jul  4 13:50:24 2020\n",
            "[xla:4](20) Loss=0.07155 Rate=509.49 GlobalRate=606.79 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:3](20) Loss=0.11724 Rate=496.71 GlobalRate=594.98 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:2](20) Loss=0.08647 Rate=491.46 GlobalRate=578.12 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:7](20) Loss=0.09387 Rate=507.06 GlobalRate=577.03 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:5](20) Loss=0.06491 Rate=475.11 GlobalRate=569.84 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:1](20) Loss=0.02195 Rate=501.64 GlobalRate=575.71 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:0](20) Loss=0.07800 Rate=503.64 GlobalRate=565.23 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:6](20) Loss=0.02070 Rate=498.81 GlobalRate=560.54 Time=Sat Jul  4 13:50:27 2020\n",
            "[xla:4](40) Loss=0.02495 Rate=659.69 GlobalRate=672.90 Time=Sat Jul  4 13:50:30 2020\n",
            "[xla:1](40) Loss=0.03498 Rate=679.84 GlobalRate=666.46 Time=Sat Jul  4 13:50:30 2020\n",
            "[xla:7](40) Loss=0.04509 Rate=670.77 GlobalRate=660.90 Time=Sat Jul  4 13:50:30 2020\n",
            "[xla:3](40) Loss=0.06524 Rate=642.67 GlobalRate=657.86 Time=Sat Jul  4 13:50:30 2020\n",
            "[xla:2](40) Loss=0.03812 Rate=644.23 GlobalRate=649.44 Time=Sat Jul  4 13:50:31 2020\n",
            "[xla:0](40) Loss=0.11634 Rate=662.41 GlobalRate=648.87 Time=Sat Jul  4 13:50:31 2020\n",
            "[xla:6](40) Loss=0.04488 Rate=665.01 GlobalRate=648.29 Time=Sat Jul  4 13:50:31 2020\n",
            "[xla:5](40) Loss=0.02559 Rate=636.19 GlobalRate=643.14 Time=Sat Jul  4 13:50:31 2020\n",
            "Finished training epoch 4\n",
            "[xla:1] Accuracy=98.30%\n",
            "[xla:6] Accuracy=98.32%\n",
            "[xla:5] Accuracy=98.29%\n",
            "[xla:3] Accuracy=98.31%\n",
            "[xla:4] Accuracy=98.30%\n",
            "[xla:7] Accuracy=98.31%\n",
            "[xla:2] Accuracy=98.32%\n",
            "[xla:0] Accuracy=98.31%\n",
            "[xla:1](0) Loss=0.05326 Rate=114.79 GlobalRate=114.79 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:3](0) Loss=0.03690 Rate=124.48 GlobalRate=124.47 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:6](0) Loss=0.05495 Rate=103.62 GlobalRate=103.61 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:4](0) Loss=0.03492 Rate=106.41 GlobalRate=106.41 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:5](0) Loss=0.02883 Rate=100.98 GlobalRate=100.98 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:7](0) Loss=0.11869 Rate=91.52 GlobalRate=91.52 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:2](0) Loss=0.06594 Rate=86.59 GlobalRate=86.59 Time=Sat Jul  4 13:50:47 2020\n",
            "[xla:0](0) Loss=0.08898 Rate=76.91 GlobalRate=76.91 Time=Sat Jul  4 13:50:48 2020\n",
            "[xla:6](20) Loss=0.01373 Rate=500.47 GlobalRate=586.70 Time=Sat Jul  4 13:50:50 2020\n",
            "[xla:1](20) Loss=0.01722 Rate=474.47 GlobalRate=572.01 Time=Sat Jul  4 13:50:50 2020\n",
            "[xla:4](20) Loss=0.05404 Rate=502.48 GlobalRate=591.72 Time=Sat Jul  4 13:50:50 2020\n",
            "[xla:3](20) Loss=0.10671 Rate=483.89 GlobalRate=588.61 Time=Sat Jul  4 13:50:50 2020\n",
            "[xla:5](20) Loss=0.05545 Rate=504.72 GlobalRate=587.47 Time=Sat Jul  4 13:50:50 2020\n",
            "[xla:7](20) Loss=0.08115 Rate=495.85 GlobalRate=566.69 Time=Sat Jul  4 13:50:51 2020\n",
            "[xla:2](20) Loss=0.07601 Rate=489.19 GlobalRate=553.39 Time=Sat Jul  4 13:50:51 2020\n",
            "[xla:0](20) Loss=0.07090 Rate=492.43 GlobalRate=538.53 Time=Sat Jul  4 13:50:51 2020\n",
            "[xla:4](40) Loss=0.01905 Rate=648.53 GlobalRate=658.07 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:1](40) Loss=0.02767 Rate=636.68 GlobalRate=645.01 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:6](40) Loss=0.03683 Rate=646.70 GlobalRate=654.24 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:3](40) Loss=0.05019 Rate=640.64 GlobalRate=655.81 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:5](40) Loss=0.01897 Rate=656.28 GlobalRate=659.63 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:7](40) Loss=0.03720 Rate=658.14 GlobalRate=649.19 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:2](40) Loss=0.03201 Rate=663.26 GlobalRate=644.53 Time=Sat Jul  4 13:50:54 2020\n",
            "[xla:0](40) Loss=0.10955 Rate=653.02 GlobalRate=627.80 Time=Sat Jul  4 13:50:54 2020\n",
            "Finished training epoch 5\n",
            "[xla:2] Accuracy=98.51%\n",
            "[xla:1] Accuracy=98.51%\n",
            "[xla:4] Accuracy=98.52%\n",
            "[xla:3] Accuracy=98.48%\n",
            "[xla:0] Accuracy=98.50%\n",
            "[xla:7] Accuracy=98.47%\n",
            "[xla:5] Accuracy=98.47%\n",
            "[xla:6] Accuracy=98.51%\n",
            "[xla:2](0) Loss=0.06044 Rate=176.02 GlobalRate=176.01 Time=Sat Jul  4 13:51:10 2020\n",
            "[xla:1](0) Loss=0.04754 Rate=113.85 GlobalRate=113.84 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:4](0) Loss=0.02774 Rate=101.96 GlobalRate=101.96 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:5](0) Loss=0.02239 Rate=90.82 GlobalRate=90.82 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:6](0) Loss=0.04836 Rate=85.25 GlobalRate=85.24 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:7](0) Loss=0.10236 Rate=85.97 GlobalRate=85.97 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:3](0) Loss=0.03626 Rate=85.79 GlobalRate=85.79 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:0](0) Loss=0.07906 Rate=77.89 GlobalRate=77.89 Time=Sat Jul  4 13:51:11 2020\n",
            "[xla:2](20) Loss=0.06883 Rate=476.78 GlobalRate=596.41 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:4](20) Loss=0.04259 Rate=507.92 GlobalRate=591.61 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:1](20) Loss=0.01317 Rate=487.67 GlobalRate=584.55 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:3](20) Loss=0.09687 Rate=524.37 GlobalRate=581.02 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:6](20) Loss=0.00971 Rate=503.42 GlobalRate=563.01 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:5](20) Loss=0.04899 Rate=489.98 GlobalRate=560.56 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:7](20) Loss=0.07212 Rate=489.10 GlobalRate=552.31 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:0](20) Loss=0.06421 Rate=500.66 GlobalRate=546.91 Time=Sat Jul  4 13:51:14 2020\n",
            "[xla:2](40) Loss=0.02647 Rate=648.96 GlobalRate=667.78 Time=Sat Jul  4 13:51:17 2020\n",
            "[xla:1](40) Loss=0.02439 Rate=650.79 GlobalRate=658.56 Time=Sat Jul  4 13:51:17 2020\n",
            "[xla:4](40) Loss=0.01594 Rate=654.26 GlobalRate=660.24 Time=Sat Jul  4 13:51:17 2020\n",
            "[xla:5](40) Loss=0.01568 Rate=672.25 GlobalRate=654.33 Time=Sat Jul  4 13:51:18 2020\n",
            "[xla:3](40) Loss=0.04233 Rate=667.62 GlobalRate=657.56 Time=Sat Jul  4 13:51:18 2020\n",
            "[xla:6](40) Loss=0.03264 Rate=662.11 GlobalRate=647.25 Time=Sat Jul  4 13:51:18 2020\n",
            "[xla:7](40) Loss=0.03183 Rate=663.03 GlobalRate=643.67 Time=Sat Jul  4 13:51:18 2020\n",
            "[xla:0](40) Loss=0.10320 Rate=656.45 GlobalRate=633.67 Time=Sat Jul  4 13:51:18 2020\n",
            "Finished training epoch 6\n",
            "[xla:2] Accuracy=98.59%\n",
            "[xla:4] Accuracy=98.59%\n",
            "[xla:5] Accuracy=98.59%\n",
            "[xla:1] Accuracy=98.63%\n",
            "[xla:6] Accuracy=98.62%\n",
            "[xla:0] Accuracy=98.59%\n",
            "[xla:3] Accuracy=98.58%\n",
            "[xla:7] Accuracy=98.60%\n",
            "[xla:2](0) Loss=0.05992 Rate=125.99 GlobalRate=125.99 Time=Sat Jul  4 13:51:34 2020\n",
            "[xla:4](0) Loss=0.02420 Rate=114.19 GlobalRate=114.19 Time=Sat Jul  4 13:51:34 2020\n",
            "[xla:5](0) Loss=0.01903 Rate=94.65 GlobalRate=94.65 Time=Sat Jul  4 13:51:34 2020\n",
            "[xla:1](0) Loss=0.04101 Rate=99.41 GlobalRate=99.41 Time=Sat Jul  4 13:51:34 2020\n",
            "[xla:6](0) Loss=0.04297 Rate=95.31 GlobalRate=95.30 Time=Sat Jul  4 13:51:34 2020\n",
            "[xla:3](0) Loss=0.03413 Rate=85.27 GlobalRate=85.27 Time=Sat Jul  4 13:51:35 2020\n",
            "[xla:0](0) Loss=0.07146 Rate=83.52 GlobalRate=83.52 Time=Sat Jul  4 13:51:35 2020\n",
            "[xla:7](0) Loss=0.08958 Rate=77.53 GlobalRate=77.53 Time=Sat Jul  4 13:51:35 2020\n",
            "[xla:2](20) Loss=0.06055 Rate=491.08 GlobalRate=597.15 Time=Sat Jul  4 13:51:37 2020\n",
            "[xla:4](20) Loss=0.03498 Rate=487.25 GlobalRate=584.42 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:1](20) Loss=0.01043 Rate=491.61 GlobalRate=573.50 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:5](20) Loss=0.04427 Rate=491.09 GlobalRate=566.93 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:6](20) Loss=0.00762 Rate=492.05 GlobalRate=568.67 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:0](20) Loss=0.05914 Rate=502.17 GlobalRate=558.92 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:3](20) Loss=0.08634 Rate=495.50 GlobalRate=556.49 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:7](20) Loss=0.06287 Rate=505.71 GlobalRate=550.07 Time=Sat Jul  4 13:51:38 2020\n",
            "[xla:2](40) Loss=0.02361 Rate=657.42 GlobalRate=669.95 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:4](40) Loss=0.01422 Rate=650.05 GlobalRate=658.13 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:6](40) Loss=0.03014 Rate=664.42 GlobalRate=655.04 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:1](40) Loss=0.02169 Rate=650.73 GlobalRate=650.34 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:5](40) Loss=0.01327 Rate=652.62 GlobalRate=647.23 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:3](40) Loss=0.03596 Rate=664.45 GlobalRate=645.94 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:0](40) Loss=0.09592 Rate=657.25 GlobalRate=641.96 Time=Sat Jul  4 13:51:41 2020\n",
            "[xla:7](40) Loss=0.02767 Rate=658.15 GlobalRate=635.66 Time=Sat Jul  4 13:51:41 2020\n",
            "Finished training epoch 7\n",
            "[xla:7] Accuracy=98.70%\n",
            "[xla:3] Accuracy=98.69%\n",
            "[xla:6] Accuracy=98.70%\n",
            "[xla:4] Accuracy=98.70%\n",
            "[xla:2] Accuracy=98.71%\n",
            "[xla:0] Accuracy=98.70%\n",
            "[xla:5] Accuracy=98.68%\n",
            "[xla:1] Accuracy=98.70%\n",
            "[xla:3](0) Loss=0.03202 Rate=119.48 GlobalRate=119.47 Time=Sat Jul  4 13:51:57 2020\n",
            "[xla:7](0) Loss=0.08099 Rate=126.31 GlobalRate=126.30 Time=Sat Jul  4 13:51:57 2020\n",
            "[xla:2](0) Loss=0.05900 Rate=94.96 GlobalRate=94.96 Time=Sat Jul  4 13:51:58 2020\n",
            "[xla:6](0) Loss=0.03714 Rate=93.21 GlobalRate=93.21 Time=Sat Jul  4 13:51:58 2020\n",
            "[xla:4](0) Loss=0.02077 Rate=95.26 GlobalRate=95.26 Time=Sat Jul  4 13:51:58 2020\n",
            "[xla:5](0) Loss=0.01508 Rate=87.21 GlobalRate=87.21 Time=Sat Jul  4 13:51:58 2020\n",
            "[xla:0](0) Loss=0.06516 Rate=79.91 GlobalRate=79.91 Time=Sat Jul  4 13:51:58 2020\n",
            "[xla:1](0) Loss=0.03630 Rate=80.33 GlobalRate=80.32 Time=Sat Jul  4 13:51:58 2020\n",
            "[xla:7](20) Loss=0.05713 Rate=503.06 GlobalRate=609.86 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:3](20) Loss=0.07738 Rate=489.95 GlobalRate=591.39 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:6](20) Loss=0.00587 Rate=518.57 GlobalRate=588.86 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:4](20) Loss=0.03000 Rate=506.36 GlobalRate=581.32 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:2](20) Loss=0.05484 Rate=499.02 GlobalRate=574.41 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:0](20) Loss=0.05448 Rate=520.51 GlobalRate=566.39 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:5](20) Loss=0.03968 Rate=511.48 GlobalRate=573.07 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:1](20) Loss=0.00868 Rate=517.50 GlobalRate=564.93 Time=Sat Jul  4 13:52:01 2020\n",
            "[xla:3](40) Loss=0.03108 Rate=638.57 GlobalRate=654.71 Time=Sat Jul  4 13:52:04 2020\n",
            "[xla:7](40) Loss=0.02518 Rate=628.67 GlobalRate=655.92 Time=Sat Jul  4 13:52:04 2020\n",
            "[xla:5](40) Loss=0.01160 Rate=674.52 GlobalRate=659.37 Time=Sat Jul  4 13:52:05 2020\n",
            "[xla:6](40) Loss=0.02773 Rate=647.74 GlobalRate=651.67 Time=Sat Jul  4 13:52:05 2020\n",
            "[xla:4](40) Loss=0.01152 Rate=643.95 GlobalRate=647.60 Time=Sat Jul  4 13:52:05 2020\n",
            "[xla:2](40) Loss=0.02190 Rate=646.35 GlobalRate=646.48 Time=Sat Jul  4 13:52:05 2020\n",
            "[xla:0](40) Loss=0.08825 Rate=648.41 GlobalRate=637.27 Time=Sat Jul  4 13:52:05 2020\n",
            "[xla:1](40) Loss=0.02011 Rate=654.20 GlobalRate=640.56 Time=Sat Jul  4 13:52:05 2020\n",
            "Finished training epoch 8\n",
            "[xla:5] Accuracy=98.71%\n",
            "[xla:4] Accuracy=98.74%\n",
            "[xla:2] Accuracy=98.72%\n",
            "[xla:3] Accuracy=98.71%\n",
            "[xla:6] Accuracy=98.71%\n",
            "[xla:1] Accuracy=98.70%\n",
            "[xla:0] Accuracy=98.71%\n",
            "[xla:7] Accuracy=98.73%\n",
            "[xla:5](0) Loss=0.01255 Rate=143.16 GlobalRate=143.16 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:3](0) Loss=0.02906 Rate=103.27 GlobalRate=103.26 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:4](0) Loss=0.01863 Rate=103.97 GlobalRate=103.97 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:6](0) Loss=0.03212 Rate=98.11 GlobalRate=98.11 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:2](0) Loss=0.05720 Rate=93.32 GlobalRate=93.32 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:1](0) Loss=0.03251 Rate=97.35 GlobalRate=97.35 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:7](0) Loss=0.07189 Rate=88.76 GlobalRate=88.76 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:0](0) Loss=0.06150 Rate=81.46 GlobalRate=81.46 Time=Sat Jul  4 13:52:21 2020\n",
            "[xla:5](20) Loss=0.03729 Rate=507.33 GlobalRate=624.11 Time=Sat Jul  4 13:52:24 2020\n",
            "[xla:3](20) Loss=0.06976 Rate=509.30 GlobalRate=594.48 Time=Sat Jul  4 13:52:24 2020\n",
            "[xla:4](20) Loss=0.02635 Rate=510.76 GlobalRate=596.68 Time=Sat Jul  4 13:52:24 2020\n",
            "[xla:6](20) Loss=0.00466 Rate=509.90 GlobalRate=588.42 Time=Sat Jul  4 13:52:24 2020\n",
            "[xla:2](20) Loss=0.05123 Rate=499.30 GlobalRate=572.34 Time=Sat Jul  4 13:52:24 2020\n",
            "[xla:1](20) Loss=0.00711 Rate=504.56 GlobalRate=582.62 Time=Sat Jul  4 13:52:24 2020\n",
            "[xla:7](20) Loss=0.05209 Rate=511.38 GlobalRate=575.61 Time=Sat Jul  4 13:52:25 2020\n",
            "[xla:0](20) Loss=0.04917 Rate=502.58 GlobalRate=555.44 Time=Sat Jul  4 13:52:25 2020\n",
            "[xla:5](40) Loss=0.00998 Rate=654.14 GlobalRate=680.58 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:4](40) Loss=0.01070 Rate=655.03 GlobalRate=663.23 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:3](40) Loss=0.02868 Rate=651.05 GlobalRate=659.68 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:1](40) Loss=0.01906 Rate=670.37 GlobalRate=664.99 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:2](40) Loss=0.02047 Rate=656.81 GlobalRate=651.37 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:6](40) Loss=0.02686 Rate=646.55 GlobalRate=652.85 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:7](40) Loss=0.02390 Rate=671.84 GlobalRate=659.56 Time=Sat Jul  4 13:52:28 2020\n",
            "[xla:0](40) Loss=0.07968 Rate=654.10 GlobalRate=637.70 Time=Sat Jul  4 13:52:28 2020\n",
            "Finished training epoch 9\n",
            "[xla:5] Accuracy=98.72%\n",
            "[xla:0] Accuracy=98.75%\n",
            "[xla:4] Accuracy=98.78%\n",
            "[xla:6] Accuracy=98.77%\n",
            "[xla:2] Accuracy=98.79%\n",
            "[xla:1] Accuracy=98.77%\n",
            "[xla:3] Accuracy=98.79%\n",
            "[xla:7] Accuracy=98.74%\n",
            "[xla:5](0) Loss=0.01132 Rate=111.02 GlobalRate=111.01 Time=Sat Jul  4 13:52:44 2020\n",
            "[xla:4](0) Loss=0.01652 Rate=105.97 GlobalRate=105.96 Time=Sat Jul  4 13:52:44 2020\n",
            "[xla:6](0) Loss=0.02843 Rate=103.99 GlobalRate=103.99 Time=Sat Jul  4 13:52:44 2020\n",
            "[xla:0](0) Loss=0.05697 Rate=94.72 GlobalRate=94.71 Time=Sat Jul  4 13:52:45 2020\n",
            "[xla:2](0) Loss=0.05570 Rate=95.50 GlobalRate=95.49 Time=Sat Jul  4 13:52:45 2020\n",
            "[xla:1](0) Loss=0.02825 Rate=99.28 GlobalRate=99.28 Time=Sat Jul  4 13:52:45 2020\n",
            "[xla:3](0) Loss=0.02759 Rate=88.93 GlobalRate=88.93 Time=Sat Jul  4 13:52:45 2020\n",
            "[xla:7](0) Loss=0.06567 Rate=90.81 GlobalRate=90.81 Time=Sat Jul  4 13:52:45 2020\n",
            "[xla:5](20) Loss=0.03453 Rate=493.40 GlobalRate=587.67 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:6](20) Loss=0.00377 Rate=487.78 GlobalRate=575.17 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:4](20) Loss=0.02226 Rate=481.19 GlobalRate=570.90 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:1](20) Loss=0.00602 Rate=498.56 GlobalRate=579.71 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:2](20) Loss=0.04737 Rate=492.09 GlobalRate=568.96 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:0](20) Loss=0.04386 Rate=489.43 GlobalRate=565.53 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:3](20) Loss=0.06239 Rate=498.50 GlobalRate=565.03 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:7](20) Loss=0.04854 Rate=488.34 GlobalRate=559.10 Time=Sat Jul  4 13:52:48 2020\n",
            "[xla:5](40) Loss=0.00884 Rate=662.76 GlobalRate=666.47 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:1](40) Loss=0.01686 Rate=664.44 GlobalRate=660.96 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:2](40) Loss=0.01853 Rate=662.61 GlobalRate=654.18 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:4](40) Loss=0.00900 Rate=654.22 GlobalRate=653.15 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:6](40) Loss=0.02504 Rate=648.51 GlobalRate=651.00 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:0](40) Loss=0.07179 Rate=650.22 GlobalRate=645.27 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:7](40) Loss=0.02188 Rate=655.04 GlobalRate=644.00 Time=Sat Jul  4 13:52:51 2020\n",
            "[xla:3](40) Loss=0.02662 Rate=641.32 GlobalRate=637.43 Time=Sat Jul  4 13:52:52 2020\n",
            "Finished training epoch 10\n",
            "[xla:2] Accuracy=98.82%\n",
            "[xla:3] Accuracy=98.83%\n",
            "[xla:7] Accuracy=98.81%\n",
            "[xla:1] Accuracy=98.82%\n",
            "[xla:6] Accuracy=98.81%\n",
            "[xla:0] Accuracy=98.83%\n",
            "[xla:5] Accuracy=98.79%\n",
            "[xla:4] Accuracy=98.84%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpikXEqdDasR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}