{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch on Cloud TPUs: MultiCore Training AlexNet on Fashion MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXvV7GbQJgy5mHM3LGC/jN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/Kaggle-Notebooks/blob/master/Tutorials/tpu/pytorch/PyTorch_on_Cloud_TPUs_MultiCore_Training_AlexNet_on_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3PpYRZtN6lO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d018785-486e-487d-a36a-a0dc0f8377b7"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4139  100  4139    0     0  54460      0 --:--:-- --:--:-- --:--:-- 54460\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (47.3.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Uninstalling torch-1.5.1+cu101:\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.9)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.5.1+cu101\n",
            "Uninstalling torchvision-0.6.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (383 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbKdF8zVOAMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.core.xla_env_vars as xla_env_vars\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from torch_xla import os as xla_os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yExPA4x1OawT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b192e668-3630-471e-ffee-97cc825cefdd"
      },
      "source": [
        "print(torch_xla.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6+e788e5b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjba_Ue3PBZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "5dcda9f4-5c63-4e55-c57f-ecbc2a677b02"
      },
      "source": [
        "def simple_map_fn(index, flags):\n",
        "  torch.manual_seed(1234)\n",
        "  device = xm.xla_device()  \n",
        "  t = torch.randn((2, 2), device=device)\n",
        "  print(\"Process\", index ,\"is using\", xm.xla_real_devices([str(device)])[0])\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "flags = {}\n",
        "xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 0 is using TPU:0\n",
            "Process 6 is using TPU:6\n",
            "Process 2 is using TPU:2\n",
            "Process 5 is using TPU:5\n",
            "Process 7 is using TPU:7\n",
            "Process 4 is using TPU:4\n",
            "Process 1 is using TPU:1\n",
            "Process 3 is using TPU:3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FJnyCJBRbCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "70a09944-16ae-4f61-a406-fc909fb4e37c"
      },
      "source": [
        "def simple_map_fn(index, flags):\n",
        "  torch.manual_seed(1234)\n",
        "  device = xm.xla_device()  \n",
        "\n",
        "  t = torch.randn((2, 2), device=device)  # Common Cloud TPU computation\n",
        "  out = str(t)  # Each process uses the XLA tensors the same way\n",
        "\n",
        "  if xm.is_master_ordinal():  # Divergent CPU-only computation (no XLA tensors beyond this point!)\n",
        "    print(out)\n",
        "\n",
        "  # Barrier to prevent master from exiting before workers connect.\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "\n",
        "xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.3916,  0.4222],\n",
            "        [ 1.0496, -0.4849]], device='xla:1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdvvKBmwP1BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision \n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.data_parallel as data_parallel\n",
        "import torch_xla.distributed.parallel_loader as parallel_loader\n",
        "import time"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM44nNsJQsC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    torch.manual_seed(flags['seed'])\n",
        "    device = xm.xla_device()\n",
        "    \n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "    to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
        "    resize = transforms.Resize((224, 224))\n",
        "    my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
        "\n",
        "    if not xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "    \n",
        "    train_dataset = datasets.FashionMNIST('/tmp/fashionmnist',\n",
        "                                          train=True,\n",
        "                                          download=True,\n",
        "                                          transform = my_transform)\n",
        "    test_dataset = datasets.FashionMNIST('/tmp/fashionmnist',\n",
        "                                         train=False,\n",
        "                                         download=True,\n",
        "                                         transform=my_transform)\n",
        "    \n",
        "    if xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "    \n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
        "                                                                    num_replicas=xm.xrt_world_size(),\n",
        "                                                                    rank=xm.get_ordinal(),\n",
        "                                                                    shuffle=True\n",
        "                                                                    )\n",
        "    \n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset,\n",
        "                                                                    num_replicas=xm.xrt_world_size(),\n",
        "                                                                    rank=xm.get_ordinal(),\n",
        "                                                                    shuffle=False\n",
        "                                                                    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=flags['batch_size'],\n",
        "                                               sampler=train_sampler,\n",
        "                                               num_workers=flags['num_workers'],\n",
        "                                               drop_last=True\n",
        "                                               )\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                               batch_size=flags['batch_size'],\n",
        "                                               sampler=test_sampler,\n",
        "                                               num_workers=flags['num_workers'],\n",
        "                                               drop_last=True\n",
        "                                               )\n",
        "    \n",
        "    net = torchvision.models.alexnet(num_classes=10).to(device).train()\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "    train_start = time.time()\n",
        "    for epoch in range(flags['num_epochs']):\n",
        "        para_train_loader = parallel_loader.ParallelLoader(train_loader, devices=[device]).per_device_loader(device)\n",
        "        for batch_num, batch in enumerate(para_train_loader):\n",
        "            data, targets = batch\n",
        "\n",
        "            output = net(data)\n",
        "            loss = loss_fn(output, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer=optimizer)\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) \n",
        "\n",
        "    net.eval()\n",
        "    eval_start = time.time()\n",
        "    with torch.no_grad():\n",
        "        num_correct = 0\n",
        "        total_guesses = 0\n",
        "        para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "        for batch_num, batch in enumerate(para_train_loader):\n",
        "            data, targets = batch\n",
        "\n",
        "            output = net(data)\n",
        "            best_guesses = torch.argmax(output, 1)\n",
        "\n",
        "            num_correct += torch.eq(targets, best_guesses).sum().item()\n",
        "            total_guesses += flags['batch_size']\n",
        "\n",
        "    elapsed_eval_time = time.time() - eval_start\n",
        "    print(\"Process\", index, \"finished evaluation. Evaluation time was:\", elapsed_eval_time)\n",
        "    print(\"Process\", index, \"guessed\", num_correct, \"of\", total_guesses, \"correctly for\", num_correct/total_guesses * 100, \"% accuracy.\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CvFEe_RVs5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f74fc373-292b-4c8f-e98a-51d22d0390c8"
      },
      "source": [
        "flags['batch_size'] = 32\n",
        "flags['num_workers'] = 8\n",
        "flags['num_epochs'] = 1\n",
        "flags['seed'] = 1234\n",
        "\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 1 finished training. Train time was: 328.7308475971222\n",
            "Process 0 finished training. Train time was: 327.9923770427704\n",
            "Process 3 finished training. Train time was: 328.0882017612457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in device=TPU:0: name 'pl' is not defined\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Process 5 finished training. Train time was: 327.4206635951996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in device=TPU:3: name 'pl' is not defined\n",
            "Exception in device=TPU:5: name 'pl' is not defined\n",
            "Exception in device=TPU:1: name 'pl' is not defined\n",
            "Traceback (most recent call last):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Process 4 finished training. Train time was: 327.65316939353943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Process 7 finished training. Train time was: 333.198184967041\n",
            "Process 6 finished training. Train time was: 332.0990159511566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Process 2 finished training. Train time was: 333.13902592658997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "Exception in device=TPU:4: name 'pl' is not defined\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "Exception in device=TPU:6: name 'pl' is not defined\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "Exception in device=TPU:7: name 'pl' is not defined\n",
            "NameError: name 'pl' is not defined\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "Traceback (most recent call last):\n",
            "Exception in device=TPU:2: name 'pl' is not defined\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "NameError: name 'pl' is not defined\n",
            "NameError: name 'pl' is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "NameError: name 'pl' is not defined\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "NameError: name 'pl' is not defined\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "  File \"<ipython-input-12-4a3924915d56>\", line 75, in map_fn\n",
            "    para_train_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
            "NameError: name 'pl' is not defined\n",
            "NameError: name 'pl' is not defined\n",
            "NameError: name 'pl' is not defined\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-980d29c04af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1234\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         start_method=start_method)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 raise Exception(\n\u001b[1;32m    112\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 )\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: process 0 terminated with exit code 17"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvz0q2ehV07f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}