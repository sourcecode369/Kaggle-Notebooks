{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar 100 with Optimizers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNH/4fX689EI6Qnf1Vmf0ZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99f08c15c5b24a5c97f984ecf7e76ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6501a68b76f84b0f86047c176678494f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_47f9378de8cb42498b5d9ae80d70dd1a",
              "IPY_MODEL_690f6deee50545b8b708a9036f37cbcd"
            ]
          }
        },
        "6501a68b76f84b0f86047c176678494f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47f9378de8cb42498b5d9ae80d70dd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac8179009e6c4c19bf19b94dae546b6c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5d8a0da7454467ca1a34a504343ce29"
          }
        },
        "690f6deee50545b8b708a9036f37cbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7711d82b0c704e9b820e64f44b586683",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:30&lt;00:00, 16625839.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e25f859c35b4b729d8f3f7369eb9511"
          }
        },
        "ac8179009e6c4c19bf19b94dae546b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5d8a0da7454467ca1a34a504343ce29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7711d82b0c704e9b820e64f44b586683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e25f859c35b4b729d8f3f7369eb9511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55bbc9d4229147a0a5e83dbbb1264f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_26a108f4b83840dc8b662d2b8bb4c0c5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8d832ffd1baf49c689655ac212d9dde0",
              "IPY_MODEL_a498014dbba744728d78a91bebb3dbb2"
            ]
          }
        },
        "26a108f4b83840dc8b662d2b8bb4c0c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d832ffd1baf49c689655ac212d9dde0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63f507daf04a438c97602c7cd90a08f4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d015be366a31417883644aa530eb0f93"
          }
        },
        "a498014dbba744728d78a91bebb3dbb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a544f8172d174c6c96cce3cdc24ac081",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [01:49&lt;00:00, 936kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d837306d5dd14efbbbae6040569730a5"
          }
        },
        "63f507daf04a438c97602c7cd90a08f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d015be366a31417883644aa530eb0f93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a544f8172d174c6c96cce3cdc24ac081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d837306d5dd14efbbbae6040569730a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/Kaggle-Notebooks/blob/master/Tutorials/Cifar_100_with_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_NJww17Ey6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "3111c003-7e9e-4a8f-abc6-661c62259cc7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 19 14:32:17 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s_G1PYQFFwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "818c62f6-3d53-464b-e310-02b62a79a451"
      },
      "source": [
        "!pip install torch_optimizer\n",
        "!pip install pretrainedmodels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_optimizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/1a/a3f86e67c4f650916cb7d16849331bda302bf0b155f7c8240607cc97664b/torch_optimizer-0.0.1a14-py3-none-any.whl (40kB)\n",
            "\r\u001b[K     |████████▏                       | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.8MB/s \n",
            "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a14\n",
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.6.1+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.12.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=bd19400f4b66b40a74f79c7eab42c4426d5fcae8ddddf0d4351093f8baede9fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeOhdZ4IFAIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "import torch.optim as to\n",
        "import torch.nn.functional as F\n",
        "import torch_optimizer as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ysGCLgTHJD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = {\n",
        "'cifar10': (0.4914, 0.4822, 0.4465),\n",
        "'cifar100': (0.5071, 0.4867, 0.4408),\n",
        "}\n",
        "\n",
        "std = {\n",
        "'cifar10': (0.2023, 0.1994, 0.2010),\n",
        "'cifar100': (0.2675, 0.2565, 0.2761),\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QuXWbbKFc9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "99f08c15c5b24a5c97f984ecf7e76ffa",
            "6501a68b76f84b0f86047c176678494f",
            "47f9378de8cb42498b5d9ae80d70dd1a",
            "690f6deee50545b8b708a9036f37cbcd",
            "ac8179009e6c4c19bf19b94dae546b6c",
            "e5d8a0da7454467ca1a34a504343ce29",
            "7711d82b0c704e9b820e64f44b586683",
            "7e25f859c35b4b729d8f3f7369eb9511"
          ]
        },
        "outputId": "8f98f85c-9a91-4436-b580-c0476f0e8c0e"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean['cifar100'], std['cifar100'])\n",
        "    ])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = DataLoader(\n",
        "    dataset_train, shuffle=True, num_workers=4, batch_size=64)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99f08c15c5b24a5c97f984ecf7e76ffa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gp228zKHd5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, pretrained=None, classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.classes = classes\n",
        "        self.pretrained = pretrained\n",
        "        self.resnet = pretrainedmodels.__dict__['resnet50'](pretrained=self.pretrained)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.ln = nn.LayerNorm(2048)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.classifier = nn.Linear(2048,self.classes)\n",
        "    def forward(self, images):\n",
        "        features = self.resnet.features(images)\n",
        "        avg_pool = self.avg_pool(features).squeeze(-1).squeeze(-1)\n",
        "        layer_norm = self.ln(avg_pool)\n",
        "        dropout = self.dropout(layer_norm)\n",
        "        logits = self.classifier(dropout)\n",
        "        return logits"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNh5iXeyO0pR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "55bbc9d4229147a0a5e83dbbb1264f71",
            "26a108f4b83840dc8b662d2b8bb4c0c5",
            "8d832ffd1baf49c689655ac212d9dde0",
            "a498014dbba744728d78a91bebb3dbb2",
            "63f507daf04a438c97602c7cd90a08f4",
            "d015be366a31417883644aa530eb0f93",
            "a544f8172d174c6c96cce3cdc24ac081",
            "d837306d5dd14efbbbae6040569730a5"
          ]
        },
        "outputId": "64fb018b-854f-4a8d-db3b-3474bc01f064"
      },
      "source": [
        "model = ResNet(pretrained='imagenet')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55bbc9d4229147a0a5e83dbbb1264f71",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Qrh6KZOYIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fa90355-7c7c-4671-e9b9-3750a0c09533"
      },
      "source": [
        "model(torch.rand(5,3,512,512)).shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhFgjEYHJIgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, gamma=2, logits=False, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fi5g4jtJL4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy().argmax(axis=1)\n",
        "        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n",
        "        self.y_true = np.hstack((self.y_true, y_true))\n",
        "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
        "        self.score = metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n",
        "    \n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00pkBHCkKdwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet(pretrained='imagenet').to(\"cuda\")\n",
        "model = nn.DataParallel(model)\n",
        "\n",
        "#criterion = FocalLoss(alpha=0.5, gamma=2).to(\"cuda\")\n",
        "criterion = nn.CrossEntropyLoss().to(\"cuda\")\n",
        "\n",
        "# optimizer = optim.QHAdam(model.parameters(), \n",
        "                        #  lr=1e-3, \n",
        "                        #  betas=(0.995, 0.999), \n",
        "                        #  nus=(0.7, 1.0), \n",
        "                        #  weight_decay=0.0, \n",
        "                        #  eps=1e-8)\n",
        "\n",
        "optimizer = optim.Ranger(model.parameters(), \n",
        "                         lr=1e-3, \n",
        "                         alpha=0.5, \n",
        "                         k=6, \n",
        "                         N_sma_threshhold=5, \n",
        "                         betas=(.95, 0.999), \n",
        "                         eps=1e-5,\n",
        "                         weight_decay=0)\n",
        "\n",
        "# optimizer = optim.Lamb(model.parameters(), \n",
        "#                        lr=1e-3, \n",
        "#                        betas=(0.9, 0.999), \n",
        "#                        eps=1e-6, \n",
        "#                        weight_decay=0, \n",
        "#                        adam=False)\n",
        "\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **dict(mode='max',\n",
        "                                                                         factor=0.8,\n",
        "                                                                         patience=2,\n",
        "                                                                         verbose=False, \n",
        "                                                                         threshold=0.0001,\n",
        "                                                                         threshold_mode='abs',\n",
        "                                                                         cooldown=0, \n",
        "                                                                         min_lr=1e-8,\n",
        "                                                                         eps=1e-08\n",
        "                                                                         ))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBfH41QwLKlm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78a27d52-00c1-49ce-a25c-2dbc2ef660e6"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "model.train()\n",
        "for e in range(10):\n",
        "    total_loss = AverageMeter()\n",
        "    total_score = RocAucMeter()\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        batch_size, _, _, _ = images.shape\n",
        "        images = images.cuda()\n",
        "        targets = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        # print(outputs.shape, targets.shape)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss = loss.detach().item()\n",
        "\n",
        "        # total_score.update(targets, outputs)\n",
        "        total_loss.update(loss, batch_size)\n",
        "\n",
        "        \n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.10f}'.format(\n",
        "                total_loss.avg,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                epoch=e,\n",
        "                trained_samples=step*batch_size+len(images),\n",
        "                total_samples=len(train_loader.dataset)\n",
        "                ))\n",
        "            # print(f'Step={step}, loss={total_loss.avg:<8.4f}, auc={total_score.avg:<8.4f} {time.time()-start_time:<2.2f}')\n",
        "        \n",
        "    if scheduler is not None:\n",
        "        scheduler.step(metrics=total_loss.avg)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Epoch: 0 [64/50000]\tLoss: 4.7445\tLR: 0.0010000000\n",
            "Training Epoch: 0 [704/50000]\tLoss: 4.7721\tLR: 0.0010000000\n",
            "Training Epoch: 0 [1344/50000]\tLoss: 4.7800\tLR: 0.0010000000\n",
            "Training Epoch: 0 [1984/50000]\tLoss: 4.7629\tLR: 0.0010000000\n",
            "Training Epoch: 0 [2624/50000]\tLoss: 4.7534\tLR: 0.0010000000\n",
            "Training Epoch: 0 [3264/50000]\tLoss: 4.7242\tLR: 0.0010000000\n",
            "Training Epoch: 0 [3904/50000]\tLoss: 4.7013\tLR: 0.0010000000\n",
            "Training Epoch: 0 [4544/50000]\tLoss: 4.6801\tLR: 0.0010000000\n",
            "Training Epoch: 0 [5184/50000]\tLoss: 4.6518\tLR: 0.0010000000\n",
            "Training Epoch: 0 [5824/50000]\tLoss: 4.6179\tLR: 0.0010000000\n",
            "Training Epoch: 0 [6464/50000]\tLoss: 4.5859\tLR: 0.0010000000\n",
            "Training Epoch: 0 [7104/50000]\tLoss: 4.5527\tLR: 0.0010000000\n",
            "Training Epoch: 0 [7744/50000]\tLoss: 4.5127\tLR: 0.0010000000\n",
            "Training Epoch: 0 [8384/50000]\tLoss: 4.4728\tLR: 0.0010000000\n",
            "Training Epoch: 0 [9024/50000]\tLoss: 4.4361\tLR: 0.0010000000\n",
            "Training Epoch: 0 [9664/50000]\tLoss: 4.3994\tLR: 0.0010000000\n",
            "Training Epoch: 0 [10304/50000]\tLoss: 4.3621\tLR: 0.0010000000\n",
            "Training Epoch: 0 [10944/50000]\tLoss: 4.3227\tLR: 0.0010000000\n",
            "Training Epoch: 0 [11584/50000]\tLoss: 4.2870\tLR: 0.0010000000\n",
            "Training Epoch: 0 [12224/50000]\tLoss: 4.2467\tLR: 0.0010000000\n",
            "Training Epoch: 0 [12864/50000]\tLoss: 4.2115\tLR: 0.0010000000\n",
            "Training Epoch: 0 [13504/50000]\tLoss: 4.1771\tLR: 0.0010000000\n",
            "Training Epoch: 0 [14144/50000]\tLoss: 4.1427\tLR: 0.0010000000\n",
            "Training Epoch: 0 [14784/50000]\tLoss: 4.1112\tLR: 0.0010000000\n",
            "Training Epoch: 0 [15424/50000]\tLoss: 4.0751\tLR: 0.0010000000\n",
            "Training Epoch: 0 [16064/50000]\tLoss: 4.0461\tLR: 0.0010000000\n",
            "Training Epoch: 0 [16704/50000]\tLoss: 4.0132\tLR: 0.0010000000\n",
            "Training Epoch: 0 [17344/50000]\tLoss: 3.9841\tLR: 0.0010000000\n",
            "Training Epoch: 0 [17984/50000]\tLoss: 3.9585\tLR: 0.0010000000\n",
            "Training Epoch: 0 [18624/50000]\tLoss: 3.9352\tLR: 0.0010000000\n",
            "Training Epoch: 0 [19264/50000]\tLoss: 3.9083\tLR: 0.0010000000\n",
            "Training Epoch: 0 [19904/50000]\tLoss: 3.8816\tLR: 0.0010000000\n",
            "Training Epoch: 0 [20544/50000]\tLoss: 3.8544\tLR: 0.0010000000\n",
            "Training Epoch: 0 [21184/50000]\tLoss: 3.8283\tLR: 0.0010000000\n",
            "Training Epoch: 0 [21824/50000]\tLoss: 3.8050\tLR: 0.0010000000\n",
            "Training Epoch: 0 [22464/50000]\tLoss: 3.7794\tLR: 0.0010000000\n",
            "Training Epoch: 0 [23104/50000]\tLoss: 3.7567\tLR: 0.0010000000\n",
            "Training Epoch: 0 [23744/50000]\tLoss: 3.7364\tLR: 0.0010000000\n",
            "Training Epoch: 0 [24384/50000]\tLoss: 3.7187\tLR: 0.0010000000\n",
            "Training Epoch: 0 [25024/50000]\tLoss: 3.6945\tLR: 0.0010000000\n",
            "Training Epoch: 0 [25664/50000]\tLoss: 3.6727\tLR: 0.0010000000\n",
            "Training Epoch: 0 [26304/50000]\tLoss: 3.6536\tLR: 0.0010000000\n",
            "Training Epoch: 0 [26944/50000]\tLoss: 3.6339\tLR: 0.0010000000\n",
            "Training Epoch: 0 [27584/50000]\tLoss: 3.6166\tLR: 0.0010000000\n",
            "Training Epoch: 0 [28224/50000]\tLoss: 3.6000\tLR: 0.0010000000\n",
            "Training Epoch: 0 [28864/50000]\tLoss: 3.5825\tLR: 0.0010000000\n",
            "Training Epoch: 0 [29504/50000]\tLoss: 3.5667\tLR: 0.0010000000\n",
            "Training Epoch: 0 [30144/50000]\tLoss: 3.5497\tLR: 0.0010000000\n",
            "Training Epoch: 0 [30784/50000]\tLoss: 3.5332\tLR: 0.0010000000\n",
            "Training Epoch: 0 [31424/50000]\tLoss: 3.5170\tLR: 0.0010000000\n",
            "Training Epoch: 0 [32064/50000]\tLoss: 3.5019\tLR: 0.0010000000\n",
            "Training Epoch: 0 [32704/50000]\tLoss: 3.4892\tLR: 0.0010000000\n",
            "Training Epoch: 0 [33344/50000]\tLoss: 3.4722\tLR: 0.0010000000\n",
            "Training Epoch: 0 [33984/50000]\tLoss: 3.4568\tLR: 0.0010000000\n",
            "Training Epoch: 0 [34624/50000]\tLoss: 3.4449\tLR: 0.0010000000\n",
            "Training Epoch: 0 [35264/50000]\tLoss: 3.4308\tLR: 0.0010000000\n",
            "Training Epoch: 0 [35904/50000]\tLoss: 3.4171\tLR: 0.0010000000\n",
            "Training Epoch: 0 [36544/50000]\tLoss: 3.4045\tLR: 0.0010000000\n",
            "Training Epoch: 0 [37184/50000]\tLoss: 3.3908\tLR: 0.0010000000\n",
            "Training Epoch: 0 [37824/50000]\tLoss: 3.3785\tLR: 0.0010000000\n",
            "Training Epoch: 0 [38464/50000]\tLoss: 3.3679\tLR: 0.0010000000\n",
            "Training Epoch: 0 [39104/50000]\tLoss: 3.3565\tLR: 0.0010000000\n",
            "Training Epoch: 0 [39744/50000]\tLoss: 3.3432\tLR: 0.0010000000\n",
            "Training Epoch: 0 [40384/50000]\tLoss: 3.3299\tLR: 0.0010000000\n",
            "Training Epoch: 0 [41024/50000]\tLoss: 3.3180\tLR: 0.0010000000\n",
            "Training Epoch: 0 [41664/50000]\tLoss: 3.3079\tLR: 0.0010000000\n",
            "Training Epoch: 0 [42304/50000]\tLoss: 3.2951\tLR: 0.0010000000\n",
            "Training Epoch: 0 [42944/50000]\tLoss: 3.2830\tLR: 0.0010000000\n",
            "Training Epoch: 0 [43584/50000]\tLoss: 3.2720\tLR: 0.0010000000\n",
            "Training Epoch: 0 [44224/50000]\tLoss: 3.2619\tLR: 0.0010000000\n",
            "Training Epoch: 0 [44864/50000]\tLoss: 3.2518\tLR: 0.0010000000\n",
            "Training Epoch: 0 [45504/50000]\tLoss: 3.2417\tLR: 0.0010000000\n",
            "Training Epoch: 0 [46144/50000]\tLoss: 3.2312\tLR: 0.0010000000\n",
            "Training Epoch: 0 [46784/50000]\tLoss: 3.2200\tLR: 0.0010000000\n",
            "Training Epoch: 0 [47424/50000]\tLoss: 3.2103\tLR: 0.0010000000\n",
            "Training Epoch: 0 [48064/50000]\tLoss: 3.2006\tLR: 0.0010000000\n",
            "Training Epoch: 0 [48704/50000]\tLoss: 3.1902\tLR: 0.0010000000\n",
            "Training Epoch: 0 [49344/50000]\tLoss: 3.1810\tLR: 0.0010000000\n",
            "Training Epoch: 0 [49984/50000]\tLoss: 3.1721\tLR: 0.0010000000\n",
            "Training Epoch: 1 [64/50000]\tLoss: 1.9909\tLR: 0.0010000000\n",
            "Training Epoch: 1 [704/50000]\tLoss: 2.3804\tLR: 0.0010000000\n",
            "Training Epoch: 1 [1344/50000]\tLoss: 2.3064\tLR: 0.0010000000\n",
            "Training Epoch: 1 [1984/50000]\tLoss: 2.3280\tLR: 0.0010000000\n",
            "Training Epoch: 1 [2624/50000]\tLoss: 2.3451\tLR: 0.0010000000\n",
            "Training Epoch: 1 [3264/50000]\tLoss: 2.3492\tLR: 0.0010000000\n",
            "Training Epoch: 1 [3904/50000]\tLoss: 2.3510\tLR: 0.0010000000\n",
            "Training Epoch: 1 [4544/50000]\tLoss: 2.3548\tLR: 0.0010000000\n",
            "Training Epoch: 1 [5184/50000]\tLoss: 2.3540\tLR: 0.0010000000\n",
            "Training Epoch: 1 [5824/50000]\tLoss: 2.3625\tLR: 0.0010000000\n",
            "Training Epoch: 1 [6464/50000]\tLoss: 2.3740\tLR: 0.0010000000\n",
            "Training Epoch: 1 [7104/50000]\tLoss: 2.3506\tLR: 0.0010000000\n",
            "Training Epoch: 1 [7744/50000]\tLoss: 2.3516\tLR: 0.0010000000\n",
            "Training Epoch: 1 [8384/50000]\tLoss: 2.3455\tLR: 0.0010000000\n",
            "Training Epoch: 1 [9024/50000]\tLoss: 2.3329\tLR: 0.0010000000\n",
            "Training Epoch: 1 [9664/50000]\tLoss: 2.3340\tLR: 0.0010000000\n",
            "Training Epoch: 1 [10304/50000]\tLoss: 2.3369\tLR: 0.0010000000\n",
            "Training Epoch: 1 [10944/50000]\tLoss: 2.3397\tLR: 0.0010000000\n",
            "Training Epoch: 1 [11584/50000]\tLoss: 2.3425\tLR: 0.0010000000\n",
            "Training Epoch: 1 [12224/50000]\tLoss: 2.3435\tLR: 0.0010000000\n",
            "Training Epoch: 1 [12864/50000]\tLoss: 2.3428\tLR: 0.0010000000\n",
            "Training Epoch: 1 [13504/50000]\tLoss: 2.3457\tLR: 0.0010000000\n",
            "Training Epoch: 1 [14144/50000]\tLoss: 2.3478\tLR: 0.0010000000\n",
            "Training Epoch: 1 [14784/50000]\tLoss: 2.3526\tLR: 0.0010000000\n",
            "Training Epoch: 1 [15424/50000]\tLoss: 2.3625\tLR: 0.0010000000\n",
            "Training Epoch: 1 [16064/50000]\tLoss: 2.3648\tLR: 0.0010000000\n",
            "Training Epoch: 1 [16704/50000]\tLoss: 2.3712\tLR: 0.0010000000\n",
            "Training Epoch: 1 [17344/50000]\tLoss: 2.3747\tLR: 0.0010000000\n",
            "Training Epoch: 1 [17984/50000]\tLoss: 2.3764\tLR: 0.0010000000\n",
            "Training Epoch: 1 [18624/50000]\tLoss: 2.3792\tLR: 0.0010000000\n",
            "Training Epoch: 1 [19264/50000]\tLoss: 2.3786\tLR: 0.0010000000\n",
            "Training Epoch: 1 [19904/50000]\tLoss: 2.3782\tLR: 0.0010000000\n",
            "Training Epoch: 1 [20544/50000]\tLoss: 2.3792\tLR: 0.0010000000\n",
            "Training Epoch: 1 [21184/50000]\tLoss: 2.3800\tLR: 0.0010000000\n",
            "Training Epoch: 1 [21824/50000]\tLoss: 2.3800\tLR: 0.0010000000\n",
            "Training Epoch: 1 [22464/50000]\tLoss: 2.3821\tLR: 0.0010000000\n",
            "Training Epoch: 1 [23104/50000]\tLoss: 2.3836\tLR: 0.0010000000\n",
            "Training Epoch: 1 [23744/50000]\tLoss: 2.3834\tLR: 0.0010000000\n",
            "Training Epoch: 1 [24384/50000]\tLoss: 2.3837\tLR: 0.0010000000\n",
            "Training Epoch: 1 [25024/50000]\tLoss: 2.3828\tLR: 0.0010000000\n",
            "Training Epoch: 1 [25664/50000]\tLoss: 2.3806\tLR: 0.0010000000\n",
            "Training Epoch: 1 [26304/50000]\tLoss: 2.3819\tLR: 0.0010000000\n",
            "Training Epoch: 1 [26944/50000]\tLoss: 2.3821\tLR: 0.0010000000\n",
            "Training Epoch: 1 [27584/50000]\tLoss: 2.3816\tLR: 0.0010000000\n",
            "Training Epoch: 1 [28224/50000]\tLoss: 2.3790\tLR: 0.0010000000\n",
            "Training Epoch: 1 [28864/50000]\tLoss: 2.3768\tLR: 0.0010000000\n",
            "Training Epoch: 1 [29504/50000]\tLoss: 2.3765\tLR: 0.0010000000\n",
            "Training Epoch: 1 [30144/50000]\tLoss: 2.3762\tLR: 0.0010000000\n",
            "Training Epoch: 1 [30784/50000]\tLoss: 2.3758\tLR: 0.0010000000\n",
            "Training Epoch: 1 [31424/50000]\tLoss: 2.3728\tLR: 0.0010000000\n",
            "Training Epoch: 1 [32064/50000]\tLoss: 2.3708\tLR: 0.0010000000\n",
            "Training Epoch: 1 [32704/50000]\tLoss: 2.3699\tLR: 0.0010000000\n",
            "Training Epoch: 1 [33344/50000]\tLoss: 2.3682\tLR: 0.0010000000\n",
            "Training Epoch: 1 [33984/50000]\tLoss: 2.3647\tLR: 0.0010000000\n",
            "Training Epoch: 1 [34624/50000]\tLoss: 2.3615\tLR: 0.0010000000\n",
            "Training Epoch: 1 [35264/50000]\tLoss: 2.3624\tLR: 0.0010000000\n",
            "Training Epoch: 1 [35904/50000]\tLoss: 2.3611\tLR: 0.0010000000\n",
            "Training Epoch: 1 [36544/50000]\tLoss: 2.3595\tLR: 0.0010000000\n",
            "Training Epoch: 1 [37184/50000]\tLoss: 2.3575\tLR: 0.0010000000\n",
            "Training Epoch: 1 [37824/50000]\tLoss: 2.3580\tLR: 0.0010000000\n",
            "Training Epoch: 1 [38464/50000]\tLoss: 2.3567\tLR: 0.0010000000\n",
            "Training Epoch: 1 [39104/50000]\tLoss: 2.3590\tLR: 0.0010000000\n",
            "Training Epoch: 1 [39744/50000]\tLoss: 2.3575\tLR: 0.0010000000\n",
            "Training Epoch: 1 [40384/50000]\tLoss: 2.3553\tLR: 0.0010000000\n",
            "Training Epoch: 1 [41024/50000]\tLoss: 2.3561\tLR: 0.0010000000\n",
            "Training Epoch: 1 [41664/50000]\tLoss: 2.3543\tLR: 0.0010000000\n",
            "Training Epoch: 1 [42304/50000]\tLoss: 2.3538\tLR: 0.0010000000\n",
            "Training Epoch: 1 [42944/50000]\tLoss: 2.3539\tLR: 0.0010000000\n",
            "Training Epoch: 1 [43584/50000]\tLoss: 2.3520\tLR: 0.0010000000\n",
            "Training Epoch: 1 [44224/50000]\tLoss: 2.3529\tLR: 0.0010000000\n",
            "Training Epoch: 1 [44864/50000]\tLoss: 2.3534\tLR: 0.0010000000\n",
            "Training Epoch: 1 [45504/50000]\tLoss: 2.3534\tLR: 0.0010000000\n",
            "Training Epoch: 1 [46144/50000]\tLoss: 2.3536\tLR: 0.0010000000\n",
            "Training Epoch: 1 [46784/50000]\tLoss: 2.3548\tLR: 0.0010000000\n",
            "Training Epoch: 1 [47424/50000]\tLoss: 2.3543\tLR: 0.0010000000\n",
            "Training Epoch: 1 [48064/50000]\tLoss: 2.3532\tLR: 0.0010000000\n",
            "Training Epoch: 1 [48704/50000]\tLoss: 2.3519\tLR: 0.0010000000\n",
            "Training Epoch: 1 [49344/50000]\tLoss: 2.3508\tLR: 0.0010000000\n",
            "Training Epoch: 1 [49984/50000]\tLoss: 2.3499\tLR: 0.0010000000\n",
            "Training Epoch: 2 [64/50000]\tLoss: 2.2402\tLR: 0.0010000000\n",
            "Training Epoch: 2 [704/50000]\tLoss: 2.0657\tLR: 0.0010000000\n",
            "Training Epoch: 2 [1344/50000]\tLoss: 2.1696\tLR: 0.0010000000\n",
            "Training Epoch: 2 [1984/50000]\tLoss: 2.1958\tLR: 0.0010000000\n",
            "Training Epoch: 2 [2624/50000]\tLoss: 2.1836\tLR: 0.0010000000\n",
            "Training Epoch: 2 [3264/50000]\tLoss: 2.1831\tLR: 0.0010000000\n",
            "Training Epoch: 2 [3904/50000]\tLoss: 2.1576\tLR: 0.0010000000\n",
            "Training Epoch: 2 [4544/50000]\tLoss: 2.1624\tLR: 0.0010000000\n",
            "Training Epoch: 2 [5184/50000]\tLoss: 2.1740\tLR: 0.0010000000\n",
            "Training Epoch: 2 [5824/50000]\tLoss: 2.1713\tLR: 0.0010000000\n",
            "Training Epoch: 2 [6464/50000]\tLoss: 2.1730\tLR: 0.0010000000\n",
            "Training Epoch: 2 [7104/50000]\tLoss: 2.1537\tLR: 0.0010000000\n",
            "Training Epoch: 2 [7744/50000]\tLoss: 2.1547\tLR: 0.0010000000\n",
            "Training Epoch: 2 [8384/50000]\tLoss: 2.1673\tLR: 0.0010000000\n",
            "Training Epoch: 2 [9024/50000]\tLoss: 2.1627\tLR: 0.0010000000\n",
            "Training Epoch: 2 [9664/50000]\tLoss: 2.1626\tLR: 0.0010000000\n",
            "Training Epoch: 2 [10304/50000]\tLoss: 2.1686\tLR: 0.0010000000\n",
            "Training Epoch: 2 [10944/50000]\tLoss: 2.1739\tLR: 0.0010000000\n",
            "Training Epoch: 2 [11584/50000]\tLoss: 2.1766\tLR: 0.0010000000\n",
            "Training Epoch: 2 [12224/50000]\tLoss: 2.1738\tLR: 0.0010000000\n",
            "Training Epoch: 2 [12864/50000]\tLoss: 2.1773\tLR: 0.0010000000\n",
            "Training Epoch: 2 [13504/50000]\tLoss: 2.1795\tLR: 0.0010000000\n",
            "Training Epoch: 2 [14144/50000]\tLoss: 2.1812\tLR: 0.0010000000\n",
            "Training Epoch: 2 [14784/50000]\tLoss: 2.1810\tLR: 0.0010000000\n",
            "Training Epoch: 2 [15424/50000]\tLoss: 2.1842\tLR: 0.0010000000\n",
            "Training Epoch: 2 [16064/50000]\tLoss: 2.1851\tLR: 0.0010000000\n",
            "Training Epoch: 2 [16704/50000]\tLoss: 2.1847\tLR: 0.0010000000\n",
            "Training Epoch: 2 [17344/50000]\tLoss: 2.1883\tLR: 0.0010000000\n",
            "Training Epoch: 2 [17984/50000]\tLoss: 2.1915\tLR: 0.0010000000\n",
            "Training Epoch: 2 [18624/50000]\tLoss: 2.1914\tLR: 0.0010000000\n",
            "Training Epoch: 2 [19264/50000]\tLoss: 2.1919\tLR: 0.0010000000\n",
            "Training Epoch: 2 [19904/50000]\tLoss: 2.1961\tLR: 0.0010000000\n",
            "Training Epoch: 2 [20544/50000]\tLoss: 2.1954\tLR: 0.0010000000\n",
            "Training Epoch: 2 [21184/50000]\tLoss: 2.1948\tLR: 0.0010000000\n",
            "Training Epoch: 2 [21824/50000]\tLoss: 2.1929\tLR: 0.0010000000\n",
            "Training Epoch: 2 [22464/50000]\tLoss: 2.1918\tLR: 0.0010000000\n",
            "Training Epoch: 2 [23104/50000]\tLoss: 2.1887\tLR: 0.0010000000\n",
            "Training Epoch: 2 [23744/50000]\tLoss: 2.1854\tLR: 0.0010000000\n",
            "Training Epoch: 2 [24384/50000]\tLoss: 2.1807\tLR: 0.0010000000\n",
            "Training Epoch: 2 [25024/50000]\tLoss: 2.1785\tLR: 0.0010000000\n",
            "Training Epoch: 2 [25664/50000]\tLoss: 2.1812\tLR: 0.0010000000\n",
            "Training Epoch: 2 [26304/50000]\tLoss: 2.1837\tLR: 0.0010000000\n",
            "Training Epoch: 2 [26944/50000]\tLoss: 2.1876\tLR: 0.0010000000\n",
            "Training Epoch: 2 [27584/50000]\tLoss: 2.1913\tLR: 0.0010000000\n",
            "Training Epoch: 2 [28224/50000]\tLoss: 2.1932\tLR: 0.0010000000\n",
            "Training Epoch: 2 [28864/50000]\tLoss: 2.1944\tLR: 0.0010000000\n",
            "Training Epoch: 2 [29504/50000]\tLoss: 2.1950\tLR: 0.0010000000\n",
            "Training Epoch: 2 [30144/50000]\tLoss: 2.1972\tLR: 0.0010000000\n",
            "Training Epoch: 2 [30784/50000]\tLoss: 2.1976\tLR: 0.0010000000\n",
            "Training Epoch: 2 [31424/50000]\tLoss: 2.2002\tLR: 0.0010000000\n",
            "Training Epoch: 2 [32064/50000]\tLoss: 2.2043\tLR: 0.0010000000\n",
            "Training Epoch: 2 [32704/50000]\tLoss: 2.2096\tLR: 0.0010000000\n",
            "Training Epoch: 2 [33344/50000]\tLoss: 2.2182\tLR: 0.0010000000\n",
            "Training Epoch: 2 [33984/50000]\tLoss: 2.2240\tLR: 0.0010000000\n",
            "Training Epoch: 2 [34624/50000]\tLoss: 2.2247\tLR: 0.0010000000\n",
            "Training Epoch: 2 [35264/50000]\tLoss: 2.2301\tLR: 0.0010000000\n",
            "Training Epoch: 2 [35904/50000]\tLoss: 2.2315\tLR: 0.0010000000\n",
            "Training Epoch: 2 [36544/50000]\tLoss: 2.2342\tLR: 0.0010000000\n",
            "Training Epoch: 2 [37184/50000]\tLoss: 2.2358\tLR: 0.0010000000\n",
            "Training Epoch: 2 [37824/50000]\tLoss: 2.2381\tLR: 0.0010000000\n",
            "Training Epoch: 2 [38464/50000]\tLoss: 2.2396\tLR: 0.0010000000\n",
            "Training Epoch: 2 [39104/50000]\tLoss: 2.2391\tLR: 0.0010000000\n",
            "Training Epoch: 2 [39744/50000]\tLoss: 2.2407\tLR: 0.0010000000\n",
            "Training Epoch: 2 [40384/50000]\tLoss: 2.2426\tLR: 0.0010000000\n",
            "Training Epoch: 2 [41024/50000]\tLoss: 2.2425\tLR: 0.0010000000\n",
            "Training Epoch: 2 [41664/50000]\tLoss: 2.2424\tLR: 0.0010000000\n",
            "Training Epoch: 2 [42304/50000]\tLoss: 2.2439\tLR: 0.0010000000\n",
            "Training Epoch: 2 [42944/50000]\tLoss: 2.2444\tLR: 0.0010000000\n",
            "Training Epoch: 2 [43584/50000]\tLoss: 2.2460\tLR: 0.0010000000\n",
            "Training Epoch: 2 [44224/50000]\tLoss: 2.2487\tLR: 0.0010000000\n",
            "Training Epoch: 2 [44864/50000]\tLoss: 2.2473\tLR: 0.0010000000\n",
            "Training Epoch: 2 [45504/50000]\tLoss: 2.2499\tLR: 0.0010000000\n",
            "Training Epoch: 2 [46144/50000]\tLoss: 2.2512\tLR: 0.0010000000\n",
            "Training Epoch: 2 [46784/50000]\tLoss: 2.2513\tLR: 0.0010000000\n",
            "Training Epoch: 2 [47424/50000]\tLoss: 2.2514\tLR: 0.0010000000\n",
            "Training Epoch: 2 [48064/50000]\tLoss: 2.2512\tLR: 0.0010000000\n",
            "Training Epoch: 2 [48704/50000]\tLoss: 2.2522\tLR: 0.0010000000\n",
            "Training Epoch: 2 [49344/50000]\tLoss: 2.2549\tLR: 0.0010000000\n",
            "Training Epoch: 2 [49984/50000]\tLoss: 2.2558\tLR: 0.0010000000\n",
            "Training Epoch: 3 [64/50000]\tLoss: 2.1344\tLR: 0.0010000000\n",
            "Training Epoch: 3 [704/50000]\tLoss: 2.1351\tLR: 0.0010000000\n",
            "Training Epoch: 3 [1344/50000]\tLoss: 2.1335\tLR: 0.0010000000\n",
            "Training Epoch: 3 [1984/50000]\tLoss: 2.1383\tLR: 0.0010000000\n",
            "Training Epoch: 3 [2624/50000]\tLoss: 2.1451\tLR: 0.0010000000\n",
            "Training Epoch: 3 [3264/50000]\tLoss: 2.1585\tLR: 0.0010000000\n",
            "Training Epoch: 3 [3904/50000]\tLoss: 2.1843\tLR: 0.0010000000\n",
            "Training Epoch: 3 [4544/50000]\tLoss: 2.1945\tLR: 0.0010000000\n",
            "Training Epoch: 3 [5184/50000]\tLoss: 2.2003\tLR: 0.0010000000\n",
            "Training Epoch: 3 [5824/50000]\tLoss: 2.2051\tLR: 0.0010000000\n",
            "Training Epoch: 3 [6464/50000]\tLoss: 2.2123\tLR: 0.0010000000\n",
            "Training Epoch: 3 [7104/50000]\tLoss: 2.2121\tLR: 0.0010000000\n",
            "Training Epoch: 3 [7744/50000]\tLoss: 2.2035\tLR: 0.0010000000\n",
            "Training Epoch: 3 [8384/50000]\tLoss: 2.1973\tLR: 0.0010000000\n",
            "Training Epoch: 3 [9024/50000]\tLoss: 2.1900\tLR: 0.0010000000\n",
            "Training Epoch: 3 [9664/50000]\tLoss: 2.1831\tLR: 0.0010000000\n",
            "Training Epoch: 3 [10304/50000]\tLoss: 2.1854\tLR: 0.0010000000\n",
            "Training Epoch: 3 [10944/50000]\tLoss: 2.1801\tLR: 0.0010000000\n",
            "Training Epoch: 3 [11584/50000]\tLoss: 2.1746\tLR: 0.0010000000\n",
            "Training Epoch: 3 [12224/50000]\tLoss: 2.1750\tLR: 0.0010000000\n",
            "Training Epoch: 3 [12864/50000]\tLoss: 2.1748\tLR: 0.0010000000\n",
            "Training Epoch: 3 [13504/50000]\tLoss: 2.1790\tLR: 0.0010000000\n",
            "Training Epoch: 3 [14144/50000]\tLoss: 2.1916\tLR: 0.0010000000\n",
            "Training Epoch: 3 [14784/50000]\tLoss: 2.2032\tLR: 0.0010000000\n",
            "Training Epoch: 3 [15424/50000]\tLoss: 2.2131\tLR: 0.0010000000\n",
            "Training Epoch: 3 [16064/50000]\tLoss: 2.2200\tLR: 0.0010000000\n",
            "Training Epoch: 3 [16704/50000]\tLoss: 2.2263\tLR: 0.0010000000\n",
            "Training Epoch: 3 [17344/50000]\tLoss: 2.2291\tLR: 0.0010000000\n",
            "Training Epoch: 3 [17984/50000]\tLoss: 2.2306\tLR: 0.0010000000\n",
            "Training Epoch: 3 [18624/50000]\tLoss: 2.2308\tLR: 0.0010000000\n",
            "Training Epoch: 3 [19264/50000]\tLoss: 2.2346\tLR: 0.0010000000\n",
            "Training Epoch: 3 [19904/50000]\tLoss: 2.2342\tLR: 0.0010000000\n",
            "Training Epoch: 3 [20544/50000]\tLoss: 2.2354\tLR: 0.0010000000\n",
            "Training Epoch: 3 [21184/50000]\tLoss: 2.2338\tLR: 0.0010000000\n",
            "Training Epoch: 3 [21824/50000]\tLoss: 2.2323\tLR: 0.0010000000\n",
            "Training Epoch: 3 [22464/50000]\tLoss: 2.2315\tLR: 0.0010000000\n",
            "Training Epoch: 3 [23104/50000]\tLoss: 2.2309\tLR: 0.0010000000\n",
            "Training Epoch: 3 [23744/50000]\tLoss: 2.2278\tLR: 0.0010000000\n",
            "Training Epoch: 3 [24384/50000]\tLoss: 2.2286\tLR: 0.0010000000\n",
            "Training Epoch: 3 [25024/50000]\tLoss: 2.2296\tLR: 0.0010000000\n",
            "Training Epoch: 3 [25664/50000]\tLoss: 2.2271\tLR: 0.0010000000\n",
            "Training Epoch: 3 [26304/50000]\tLoss: 2.2256\tLR: 0.0010000000\n",
            "Training Epoch: 3 [26944/50000]\tLoss: 2.2272\tLR: 0.0010000000\n",
            "Training Epoch: 3 [27584/50000]\tLoss: 2.2256\tLR: 0.0010000000\n",
            "Training Epoch: 3 [28224/50000]\tLoss: 2.2219\tLR: 0.0010000000\n",
            "Training Epoch: 3 [28864/50000]\tLoss: 2.2207\tLR: 0.0010000000\n",
            "Training Epoch: 3 [29504/50000]\tLoss: 2.2188\tLR: 0.0010000000\n",
            "Training Epoch: 3 [30144/50000]\tLoss: 2.2171\tLR: 0.0010000000\n",
            "Training Epoch: 3 [30784/50000]\tLoss: 2.2183\tLR: 0.0010000000\n",
            "Training Epoch: 3 [31424/50000]\tLoss: 2.2184\tLR: 0.0010000000\n",
            "Training Epoch: 3 [32064/50000]\tLoss: 2.2158\tLR: 0.0010000000\n",
            "Training Epoch: 3 [32704/50000]\tLoss: 2.2137\tLR: 0.0010000000\n",
            "Training Epoch: 3 [33344/50000]\tLoss: 2.2123\tLR: 0.0010000000\n",
            "Training Epoch: 3 [33984/50000]\tLoss: 2.2104\tLR: 0.0010000000\n",
            "Training Epoch: 3 [34624/50000]\tLoss: 2.2100\tLR: 0.0010000000\n",
            "Training Epoch: 3 [35264/50000]\tLoss: 2.2075\tLR: 0.0010000000\n",
            "Training Epoch: 3 [35904/50000]\tLoss: 2.2071\tLR: 0.0010000000\n",
            "Training Epoch: 3 [36544/50000]\tLoss: 2.2051\tLR: 0.0010000000\n",
            "Training Epoch: 3 [37184/50000]\tLoss: 2.2075\tLR: 0.0010000000\n",
            "Training Epoch: 3 [37824/50000]\tLoss: 2.2073\tLR: 0.0010000000\n",
            "Training Epoch: 3 [38464/50000]\tLoss: 2.2080\tLR: 0.0010000000\n",
            "Training Epoch: 3 [39104/50000]\tLoss: 2.2084\tLR: 0.0010000000\n",
            "Training Epoch: 3 [39744/50000]\tLoss: 2.2088\tLR: 0.0010000000\n",
            "Training Epoch: 3 [40384/50000]\tLoss: 2.2079\tLR: 0.0010000000\n",
            "Training Epoch: 3 [41024/50000]\tLoss: 2.2070\tLR: 0.0010000000\n",
            "Training Epoch: 3 [41664/50000]\tLoss: 2.2069\tLR: 0.0010000000\n",
            "Training Epoch: 3 [42304/50000]\tLoss: 2.2087\tLR: 0.0010000000\n",
            "Training Epoch: 3 [42944/50000]\tLoss: 2.2117\tLR: 0.0010000000\n",
            "Training Epoch: 3 [43584/50000]\tLoss: 2.2164\tLR: 0.0010000000\n",
            "Training Epoch: 3 [44224/50000]\tLoss: 2.2177\tLR: 0.0010000000\n",
            "Training Epoch: 3 [44864/50000]\tLoss: 2.2181\tLR: 0.0010000000\n",
            "Training Epoch: 3 [45504/50000]\tLoss: 2.2187\tLR: 0.0010000000\n",
            "Training Epoch: 3 [46144/50000]\tLoss: 2.2199\tLR: 0.0010000000\n",
            "Training Epoch: 3 [46784/50000]\tLoss: 2.2203\tLR: 0.0010000000\n",
            "Training Epoch: 3 [47424/50000]\tLoss: 2.2178\tLR: 0.0010000000\n",
            "Training Epoch: 3 [48064/50000]\tLoss: 2.2191\tLR: 0.0010000000\n",
            "Training Epoch: 3 [48704/50000]\tLoss: 2.2191\tLR: 0.0010000000\n",
            "Training Epoch: 3 [49344/50000]\tLoss: 2.2169\tLR: 0.0010000000\n",
            "Training Epoch: 3 [49984/50000]\tLoss: 2.2157\tLR: 0.0010000000\n",
            "Training Epoch: 4 [64/50000]\tLoss: 2.3570\tLR: 0.0008000000\n",
            "Training Epoch: 4 [704/50000]\tLoss: 1.9440\tLR: 0.0008000000\n",
            "Training Epoch: 4 [1344/50000]\tLoss: 2.0019\tLR: 0.0008000000\n",
            "Training Epoch: 4 [1984/50000]\tLoss: 2.0251\tLR: 0.0008000000\n",
            "Training Epoch: 4 [2624/50000]\tLoss: 2.0706\tLR: 0.0008000000\n",
            "Training Epoch: 4 [3264/50000]\tLoss: 2.0451\tLR: 0.0008000000\n",
            "Training Epoch: 4 [3904/50000]\tLoss: 2.0283\tLR: 0.0008000000\n",
            "Training Epoch: 4 [4544/50000]\tLoss: 2.0298\tLR: 0.0008000000\n",
            "Training Epoch: 4 [5184/50000]\tLoss: 2.0324\tLR: 0.0008000000\n",
            "Training Epoch: 4 [5824/50000]\tLoss: 2.0263\tLR: 0.0008000000\n",
            "Training Epoch: 4 [6464/50000]\tLoss: 2.0234\tLR: 0.0008000000\n",
            "Training Epoch: 4 [7104/50000]\tLoss: 2.0232\tLR: 0.0008000000\n",
            "Training Epoch: 4 [7744/50000]\tLoss: 2.0224\tLR: 0.0008000000\n",
            "Training Epoch: 4 [8384/50000]\tLoss: 2.0179\tLR: 0.0008000000\n",
            "Training Epoch: 4 [9024/50000]\tLoss: 2.0150\tLR: 0.0008000000\n",
            "Training Epoch: 4 [9664/50000]\tLoss: 2.0176\tLR: 0.0008000000\n",
            "Training Epoch: 4 [10304/50000]\tLoss: 2.0180\tLR: 0.0008000000\n",
            "Training Epoch: 4 [10944/50000]\tLoss: 2.0231\tLR: 0.0008000000\n",
            "Training Epoch: 4 [11584/50000]\tLoss: 2.0227\tLR: 0.0008000000\n",
            "Training Epoch: 4 [12224/50000]\tLoss: 2.0209\tLR: 0.0008000000\n",
            "Training Epoch: 4 [12864/50000]\tLoss: 2.0194\tLR: 0.0008000000\n",
            "Training Epoch: 4 [13504/50000]\tLoss: 2.0174\tLR: 0.0008000000\n",
            "Training Epoch: 4 [14144/50000]\tLoss: 2.0151\tLR: 0.0008000000\n",
            "Training Epoch: 4 [14784/50000]\tLoss: 2.0101\tLR: 0.0008000000\n",
            "Training Epoch: 4 [15424/50000]\tLoss: 2.0046\tLR: 0.0008000000\n",
            "Training Epoch: 4 [16064/50000]\tLoss: 2.0070\tLR: 0.0008000000\n",
            "Training Epoch: 4 [16704/50000]\tLoss: 2.0065\tLR: 0.0008000000\n",
            "Training Epoch: 4 [17344/50000]\tLoss: 2.0083\tLR: 0.0008000000\n",
            "Training Epoch: 4 [17984/50000]\tLoss: 2.0069\tLR: 0.0008000000\n",
            "Training Epoch: 4 [18624/50000]\tLoss: 2.0051\tLR: 0.0008000000\n",
            "Training Epoch: 4 [19264/50000]\tLoss: 2.0074\tLR: 0.0008000000\n",
            "Training Epoch: 4 [19904/50000]\tLoss: 2.0075\tLR: 0.0008000000\n",
            "Training Epoch: 4 [20544/50000]\tLoss: 2.0093\tLR: 0.0008000000\n",
            "Training Epoch: 4 [21184/50000]\tLoss: 2.0108\tLR: 0.0008000000\n",
            "Training Epoch: 4 [21824/50000]\tLoss: 2.0148\tLR: 0.0008000000\n",
            "Training Epoch: 4 [22464/50000]\tLoss: 2.0179\tLR: 0.0008000000\n",
            "Training Epoch: 4 [23104/50000]\tLoss: 2.0210\tLR: 0.0008000000\n",
            "Training Epoch: 4 [23744/50000]\tLoss: 2.0238\tLR: 0.0008000000\n",
            "Training Epoch: 4 [24384/50000]\tLoss: 2.0267\tLR: 0.0008000000\n",
            "Training Epoch: 4 [25024/50000]\tLoss: 2.0264\tLR: 0.0008000000\n",
            "Training Epoch: 4 [25664/50000]\tLoss: 2.0278\tLR: 0.0008000000\n",
            "Training Epoch: 4 [26304/50000]\tLoss: 2.0238\tLR: 0.0008000000\n",
            "Training Epoch: 4 [26944/50000]\tLoss: 2.0219\tLR: 0.0008000000\n",
            "Training Epoch: 4 [27584/50000]\tLoss: 2.0197\tLR: 0.0008000000\n",
            "Training Epoch: 4 [28224/50000]\tLoss: 2.0203\tLR: 0.0008000000\n",
            "Training Epoch: 4 [28864/50000]\tLoss: 2.0158\tLR: 0.0008000000\n",
            "Training Epoch: 4 [29504/50000]\tLoss: 2.0138\tLR: 0.0008000000\n",
            "Training Epoch: 4 [30144/50000]\tLoss: 2.0104\tLR: 0.0008000000\n",
            "Training Epoch: 4 [30784/50000]\tLoss: 2.0074\tLR: 0.0008000000\n",
            "Training Epoch: 4 [31424/50000]\tLoss: 2.0069\tLR: 0.0008000000\n",
            "Training Epoch: 4 [32064/50000]\tLoss: 2.0043\tLR: 0.0008000000\n",
            "Training Epoch: 4 [32704/50000]\tLoss: 2.0018\tLR: 0.0008000000\n",
            "Training Epoch: 4 [33344/50000]\tLoss: 1.9993\tLR: 0.0008000000\n",
            "Training Epoch: 4 [33984/50000]\tLoss: 2.0003\tLR: 0.0008000000\n",
            "Training Epoch: 4 [34624/50000]\tLoss: 1.9984\tLR: 0.0008000000\n",
            "Training Epoch: 4 [35264/50000]\tLoss: 1.9946\tLR: 0.0008000000\n",
            "Training Epoch: 4 [35904/50000]\tLoss: 1.9927\tLR: 0.0008000000\n",
            "Training Epoch: 4 [36544/50000]\tLoss: 1.9902\tLR: 0.0008000000\n",
            "Training Epoch: 4 [37184/50000]\tLoss: 1.9885\tLR: 0.0008000000\n",
            "Training Epoch: 4 [37824/50000]\tLoss: 1.9881\tLR: 0.0008000000\n",
            "Training Epoch: 4 [38464/50000]\tLoss: 1.9877\tLR: 0.0008000000\n",
            "Training Epoch: 4 [39104/50000]\tLoss: 1.9885\tLR: 0.0008000000\n",
            "Training Epoch: 4 [39744/50000]\tLoss: 1.9880\tLR: 0.0008000000\n",
            "Training Epoch: 4 [40384/50000]\tLoss: 1.9887\tLR: 0.0008000000\n",
            "Training Epoch: 4 [41024/50000]\tLoss: 1.9887\tLR: 0.0008000000\n",
            "Training Epoch: 4 [41664/50000]\tLoss: 1.9881\tLR: 0.0008000000\n",
            "Training Epoch: 4 [42304/50000]\tLoss: 1.9905\tLR: 0.0008000000\n",
            "Training Epoch: 4 [42944/50000]\tLoss: 1.9898\tLR: 0.0008000000\n",
            "Training Epoch: 4 [43584/50000]\tLoss: 1.9886\tLR: 0.0008000000\n",
            "Training Epoch: 4 [44224/50000]\tLoss: 1.9883\tLR: 0.0008000000\n",
            "Training Epoch: 4 [44864/50000]\tLoss: 1.9880\tLR: 0.0008000000\n",
            "Training Epoch: 4 [45504/50000]\tLoss: 1.9874\tLR: 0.0008000000\n",
            "Training Epoch: 4 [46144/50000]\tLoss: 1.9860\tLR: 0.0008000000\n",
            "Training Epoch: 4 [46784/50000]\tLoss: 1.9851\tLR: 0.0008000000\n",
            "Training Epoch: 4 [47424/50000]\tLoss: 1.9848\tLR: 0.0008000000\n",
            "Training Epoch: 4 [48064/50000]\tLoss: 1.9836\tLR: 0.0008000000\n",
            "Training Epoch: 4 [48704/50000]\tLoss: 1.9830\tLR: 0.0008000000\n",
            "Training Epoch: 4 [49344/50000]\tLoss: 1.9804\tLR: 0.0008000000\n",
            "Training Epoch: 4 [49984/50000]\tLoss: 1.9781\tLR: 0.0008000000\n",
            "Training Epoch: 5 [64/50000]\tLoss: 1.9494\tLR: 0.0008000000\n",
            "Training Epoch: 5 [704/50000]\tLoss: 1.6801\tLR: 0.0008000000\n",
            "Training Epoch: 5 [1344/50000]\tLoss: 1.6911\tLR: 0.0008000000\n",
            "Training Epoch: 5 [1984/50000]\tLoss: 1.6868\tLR: 0.0008000000\n",
            "Training Epoch: 5 [2624/50000]\tLoss: 1.7207\tLR: 0.0008000000\n",
            "Training Epoch: 5 [3264/50000]\tLoss: 1.7130\tLR: 0.0008000000\n",
            "Training Epoch: 5 [3904/50000]\tLoss: 1.7049\tLR: 0.0008000000\n",
            "Training Epoch: 5 [4544/50000]\tLoss: 1.7094\tLR: 0.0008000000\n",
            "Training Epoch: 5 [5184/50000]\tLoss: 1.7268\tLR: 0.0008000000\n",
            "Training Epoch: 5 [5824/50000]\tLoss: 1.7206\tLR: 0.0008000000\n",
            "Training Epoch: 5 [6464/50000]\tLoss: 1.7176\tLR: 0.0008000000\n",
            "Training Epoch: 5 [7104/50000]\tLoss: 1.7197\tLR: 0.0008000000\n",
            "Training Epoch: 5 [7744/50000]\tLoss: 1.7303\tLR: 0.0008000000\n",
            "Training Epoch: 5 [8384/50000]\tLoss: 1.7361\tLR: 0.0008000000\n",
            "Training Epoch: 5 [9024/50000]\tLoss: 1.7284\tLR: 0.0008000000\n",
            "Training Epoch: 5 [9664/50000]\tLoss: 1.7271\tLR: 0.0008000000\n",
            "Training Epoch: 5 [10304/50000]\tLoss: 1.7288\tLR: 0.0008000000\n",
            "Training Epoch: 5 [10944/50000]\tLoss: 1.7308\tLR: 0.0008000000\n",
            "Training Epoch: 5 [11584/50000]\tLoss: 1.7338\tLR: 0.0008000000\n",
            "Training Epoch: 5 [12224/50000]\tLoss: 1.7327\tLR: 0.0008000000\n",
            "Training Epoch: 5 [12864/50000]\tLoss: 1.7283\tLR: 0.0008000000\n",
            "Training Epoch: 5 [13504/50000]\tLoss: 1.7322\tLR: 0.0008000000\n",
            "Training Epoch: 5 [14144/50000]\tLoss: 1.7336\tLR: 0.0008000000\n",
            "Training Epoch: 5 [14784/50000]\tLoss: 1.7360\tLR: 0.0008000000\n",
            "Training Epoch: 5 [15424/50000]\tLoss: 1.7328\tLR: 0.0008000000\n",
            "Training Epoch: 5 [16064/50000]\tLoss: 1.7283\tLR: 0.0008000000\n",
            "Training Epoch: 5 [16704/50000]\tLoss: 1.7325\tLR: 0.0008000000\n",
            "Training Epoch: 5 [17344/50000]\tLoss: 1.7351\tLR: 0.0008000000\n",
            "Training Epoch: 5 [17984/50000]\tLoss: 1.7442\tLR: 0.0008000000\n",
            "Training Epoch: 5 [18624/50000]\tLoss: 1.7548\tLR: 0.0008000000\n",
            "Training Epoch: 5 [19264/50000]\tLoss: 1.7689\tLR: 0.0008000000\n",
            "Training Epoch: 5 [19904/50000]\tLoss: 1.7768\tLR: 0.0008000000\n",
            "Training Epoch: 5 [20544/50000]\tLoss: 1.7827\tLR: 0.0008000000\n",
            "Training Epoch: 5 [21184/50000]\tLoss: 1.7882\tLR: 0.0008000000\n",
            "Training Epoch: 5 [21824/50000]\tLoss: 1.7931\tLR: 0.0008000000\n",
            "Training Epoch: 5 [22464/50000]\tLoss: 1.7969\tLR: 0.0008000000\n",
            "Training Epoch: 5 [23104/50000]\tLoss: 1.8050\tLR: 0.0008000000\n",
            "Training Epoch: 5 [23744/50000]\tLoss: 1.8126\tLR: 0.0008000000\n",
            "Training Epoch: 5 [24384/50000]\tLoss: 1.8142\tLR: 0.0008000000\n",
            "Training Epoch: 5 [25024/50000]\tLoss: 1.8208\tLR: 0.0008000000\n",
            "Training Epoch: 5 [25664/50000]\tLoss: 1.8224\tLR: 0.0008000000\n",
            "Training Epoch: 5 [26304/50000]\tLoss: 1.8295\tLR: 0.0008000000\n",
            "Training Epoch: 5 [26944/50000]\tLoss: 1.8339\tLR: 0.0008000000\n",
            "Training Epoch: 5 [27584/50000]\tLoss: 1.8393\tLR: 0.0008000000\n",
            "Training Epoch: 5 [28224/50000]\tLoss: 1.8434\tLR: 0.0008000000\n",
            "Training Epoch: 5 [28864/50000]\tLoss: 1.8506\tLR: 0.0008000000\n",
            "Training Epoch: 5 [29504/50000]\tLoss: 1.8542\tLR: 0.0008000000\n",
            "Training Epoch: 5 [30144/50000]\tLoss: 1.8582\tLR: 0.0008000000\n",
            "Training Epoch: 5 [30784/50000]\tLoss: 1.8630\tLR: 0.0008000000\n",
            "Training Epoch: 5 [31424/50000]\tLoss: 1.8667\tLR: 0.0008000000\n",
            "Training Epoch: 5 [32064/50000]\tLoss: 1.8685\tLR: 0.0008000000\n",
            "Training Epoch: 5 [32704/50000]\tLoss: 1.8712\tLR: 0.0008000000\n",
            "Training Epoch: 5 [33344/50000]\tLoss: 1.8748\tLR: 0.0008000000\n",
            "Training Epoch: 5 [33984/50000]\tLoss: 1.8772\tLR: 0.0008000000\n",
            "Training Epoch: 5 [34624/50000]\tLoss: 1.8781\tLR: 0.0008000000\n",
            "Training Epoch: 5 [35264/50000]\tLoss: 1.8787\tLR: 0.0008000000\n",
            "Training Epoch: 5 [35904/50000]\tLoss: 1.8791\tLR: 0.0008000000\n",
            "Training Epoch: 5 [36544/50000]\tLoss: 1.8803\tLR: 0.0008000000\n",
            "Training Epoch: 5 [37184/50000]\tLoss: 1.8797\tLR: 0.0008000000\n",
            "Training Epoch: 5 [37824/50000]\tLoss: 1.8830\tLR: 0.0008000000\n",
            "Training Epoch: 5 [38464/50000]\tLoss: 1.8818\tLR: 0.0008000000\n",
            "Training Epoch: 5 [39104/50000]\tLoss: 1.8820\tLR: 0.0008000000\n",
            "Training Epoch: 5 [39744/50000]\tLoss: 1.8802\tLR: 0.0008000000\n",
            "Training Epoch: 5 [40384/50000]\tLoss: 1.8791\tLR: 0.0008000000\n",
            "Training Epoch: 5 [41024/50000]\tLoss: 1.8806\tLR: 0.0008000000\n",
            "Training Epoch: 5 [41664/50000]\tLoss: 1.8794\tLR: 0.0008000000\n",
            "Training Epoch: 5 [42304/50000]\tLoss: 1.8779\tLR: 0.0008000000\n",
            "Training Epoch: 5 [42944/50000]\tLoss: 1.8782\tLR: 0.0008000000\n",
            "Training Epoch: 5 [43584/50000]\tLoss: 1.8783\tLR: 0.0008000000\n",
            "Training Epoch: 5 [44224/50000]\tLoss: 1.8798\tLR: 0.0008000000\n",
            "Training Epoch: 5 [44864/50000]\tLoss: 1.8791\tLR: 0.0008000000\n",
            "Training Epoch: 5 [45504/50000]\tLoss: 1.8804\tLR: 0.0008000000\n",
            "Training Epoch: 5 [46144/50000]\tLoss: 1.8790\tLR: 0.0008000000\n",
            "Training Epoch: 5 [46784/50000]\tLoss: 1.8779\tLR: 0.0008000000\n",
            "Training Epoch: 5 [47424/50000]\tLoss: 1.8779\tLR: 0.0008000000\n",
            "Training Epoch: 5 [48064/50000]\tLoss: 1.8773\tLR: 0.0008000000\n",
            "Training Epoch: 5 [48704/50000]\tLoss: 1.8779\tLR: 0.0008000000\n",
            "Training Epoch: 5 [49344/50000]\tLoss: 1.8779\tLR: 0.0008000000\n",
            "Training Epoch: 5 [49984/50000]\tLoss: 1.8778\tLR: 0.0008000000\n",
            "Training Epoch: 6 [64/50000]\tLoss: 1.7749\tLR: 0.0008000000\n",
            "Training Epoch: 6 [704/50000]\tLoss: 1.7048\tLR: 0.0008000000\n",
            "Training Epoch: 6 [1344/50000]\tLoss: 1.7079\tLR: 0.0008000000\n",
            "Training Epoch: 6 [1984/50000]\tLoss: 1.7052\tLR: 0.0008000000\n",
            "Training Epoch: 6 [2624/50000]\tLoss: 1.7290\tLR: 0.0008000000\n",
            "Training Epoch: 6 [3264/50000]\tLoss: 1.7438\tLR: 0.0008000000\n",
            "Training Epoch: 6 [3904/50000]\tLoss: 1.7536\tLR: 0.0008000000\n",
            "Training Epoch: 6 [4544/50000]\tLoss: 1.7395\tLR: 0.0008000000\n",
            "Training Epoch: 6 [5184/50000]\tLoss: 1.7380\tLR: 0.0008000000\n",
            "Training Epoch: 6 [5824/50000]\tLoss: 1.7148\tLR: 0.0008000000\n",
            "Training Epoch: 6 [6464/50000]\tLoss: 1.7195\tLR: 0.0008000000\n",
            "Training Epoch: 6 [7104/50000]\tLoss: 1.7258\tLR: 0.0008000000\n",
            "Training Epoch: 6 [7744/50000]\tLoss: 1.7374\tLR: 0.0008000000\n",
            "Training Epoch: 6 [8384/50000]\tLoss: 1.7339\tLR: 0.0008000000\n",
            "Training Epoch: 6 [9024/50000]\tLoss: 1.7384\tLR: 0.0008000000\n",
            "Training Epoch: 6 [9664/50000]\tLoss: 1.7393\tLR: 0.0008000000\n",
            "Training Epoch: 6 [10304/50000]\tLoss: 1.7456\tLR: 0.0008000000\n",
            "Training Epoch: 6 [10944/50000]\tLoss: 1.7480\tLR: 0.0008000000\n",
            "Training Epoch: 6 [11584/50000]\tLoss: 1.7486\tLR: 0.0008000000\n",
            "Training Epoch: 6 [12224/50000]\tLoss: 1.7441\tLR: 0.0008000000\n",
            "Training Epoch: 6 [12864/50000]\tLoss: 1.7447\tLR: 0.0008000000\n",
            "Training Epoch: 6 [13504/50000]\tLoss: 1.7404\tLR: 0.0008000000\n",
            "Training Epoch: 6 [14144/50000]\tLoss: 1.7388\tLR: 0.0008000000\n",
            "Training Epoch: 6 [14784/50000]\tLoss: 1.7412\tLR: 0.0008000000\n",
            "Training Epoch: 6 [15424/50000]\tLoss: 1.7378\tLR: 0.0008000000\n",
            "Training Epoch: 6 [16064/50000]\tLoss: 1.7334\tLR: 0.0008000000\n",
            "Training Epoch: 6 [16704/50000]\tLoss: 1.7338\tLR: 0.0008000000\n",
            "Training Epoch: 6 [17344/50000]\tLoss: 1.7377\tLR: 0.0008000000\n",
            "Training Epoch: 6 [17984/50000]\tLoss: 1.7392\tLR: 0.0008000000\n",
            "Training Epoch: 6 [18624/50000]\tLoss: 1.7389\tLR: 0.0008000000\n",
            "Training Epoch: 6 [19264/50000]\tLoss: 1.7382\tLR: 0.0008000000\n",
            "Training Epoch: 6 [19904/50000]\tLoss: 1.7368\tLR: 0.0008000000\n",
            "Training Epoch: 6 [20544/50000]\tLoss: 1.7370\tLR: 0.0008000000\n",
            "Training Epoch: 6 [21184/50000]\tLoss: 1.7360\tLR: 0.0008000000\n",
            "Training Epoch: 6 [21824/50000]\tLoss: 1.7344\tLR: 0.0008000000\n",
            "Training Epoch: 6 [22464/50000]\tLoss: 1.7372\tLR: 0.0008000000\n",
            "Training Epoch: 6 [23104/50000]\tLoss: 1.7346\tLR: 0.0008000000\n",
            "Training Epoch: 6 [23744/50000]\tLoss: 1.7327\tLR: 0.0008000000\n",
            "Training Epoch: 6 [24384/50000]\tLoss: 1.7335\tLR: 0.0008000000\n",
            "Training Epoch: 6 [25024/50000]\tLoss: 1.7334\tLR: 0.0008000000\n",
            "Training Epoch: 6 [25664/50000]\tLoss: 1.7312\tLR: 0.0008000000\n",
            "Training Epoch: 6 [26304/50000]\tLoss: 1.7299\tLR: 0.0008000000\n",
            "Training Epoch: 6 [26944/50000]\tLoss: 1.7297\tLR: 0.0008000000\n",
            "Training Epoch: 6 [27584/50000]\tLoss: 1.7325\tLR: 0.0008000000\n",
            "Training Epoch: 6 [28224/50000]\tLoss: 1.7344\tLR: 0.0008000000\n",
            "Training Epoch: 6 [28864/50000]\tLoss: 1.7351\tLR: 0.0008000000\n",
            "Training Epoch: 6 [29504/50000]\tLoss: 1.7366\tLR: 0.0008000000\n",
            "Training Epoch: 6 [30144/50000]\tLoss: 1.7366\tLR: 0.0008000000\n",
            "Training Epoch: 6 [30784/50000]\tLoss: 1.7347\tLR: 0.0008000000\n",
            "Training Epoch: 6 [31424/50000]\tLoss: 1.7344\tLR: 0.0008000000\n",
            "Training Epoch: 6 [32064/50000]\tLoss: 1.7352\tLR: 0.0008000000\n",
            "Training Epoch: 6 [32704/50000]\tLoss: 1.7369\tLR: 0.0008000000\n",
            "Training Epoch: 6 [33344/50000]\tLoss: 1.7402\tLR: 0.0008000000\n",
            "Training Epoch: 6 [33984/50000]\tLoss: 1.7430\tLR: 0.0008000000\n",
            "Training Epoch: 6 [34624/50000]\tLoss: 1.7483\tLR: 0.0008000000\n",
            "Training Epoch: 6 [35264/50000]\tLoss: 1.7507\tLR: 0.0008000000\n",
            "Training Epoch: 6 [35904/50000]\tLoss: 1.7537\tLR: 0.0008000000\n",
            "Training Epoch: 6 [36544/50000]\tLoss: 1.7550\tLR: 0.0008000000\n",
            "Training Epoch: 6 [37184/50000]\tLoss: 1.7583\tLR: 0.0008000000\n",
            "Training Epoch: 6 [37824/50000]\tLoss: 1.7579\tLR: 0.0008000000\n",
            "Training Epoch: 6 [38464/50000]\tLoss: 1.7598\tLR: 0.0008000000\n",
            "Training Epoch: 6 [39104/50000]\tLoss: 1.7604\tLR: 0.0008000000\n",
            "Training Epoch: 6 [39744/50000]\tLoss: 1.7629\tLR: 0.0008000000\n",
            "Training Epoch: 6 [40384/50000]\tLoss: 1.7642\tLR: 0.0008000000\n",
            "Training Epoch: 6 [41024/50000]\tLoss: 1.7669\tLR: 0.0008000000\n",
            "Training Epoch: 6 [41664/50000]\tLoss: 1.7684\tLR: 0.0008000000\n",
            "Training Epoch: 6 [42304/50000]\tLoss: 1.7696\tLR: 0.0008000000\n",
            "Training Epoch: 6 [42944/50000]\tLoss: 1.7690\tLR: 0.0008000000\n",
            "Training Epoch: 6 [43584/50000]\tLoss: 1.7692\tLR: 0.0008000000\n",
            "Training Epoch: 6 [44224/50000]\tLoss: 1.7705\tLR: 0.0008000000\n",
            "Training Epoch: 6 [44864/50000]\tLoss: 1.7729\tLR: 0.0008000000\n",
            "Training Epoch: 6 [45504/50000]\tLoss: 1.7742\tLR: 0.0008000000\n",
            "Training Epoch: 6 [46144/50000]\tLoss: 1.7748\tLR: 0.0008000000\n",
            "Training Epoch: 6 [46784/50000]\tLoss: 1.7750\tLR: 0.0008000000\n",
            "Training Epoch: 6 [47424/50000]\tLoss: 1.7749\tLR: 0.0008000000\n",
            "Training Epoch: 6 [48064/50000]\tLoss: 1.7757\tLR: 0.0008000000\n",
            "Training Epoch: 6 [48704/50000]\tLoss: 1.7773\tLR: 0.0008000000\n",
            "Training Epoch: 6 [49344/50000]\tLoss: 1.7784\tLR: 0.0008000000\n",
            "Training Epoch: 6 [49984/50000]\tLoss: 1.7812\tLR: 0.0008000000\n",
            "Training Epoch: 7 [64/50000]\tLoss: 1.7855\tLR: 0.0006400000\n",
            "Training Epoch: 7 [704/50000]\tLoss: 1.7292\tLR: 0.0006400000\n",
            "Training Epoch: 7 [1344/50000]\tLoss: 1.6342\tLR: 0.0006400000\n",
            "Training Epoch: 7 [1984/50000]\tLoss: 1.6301\tLR: 0.0006400000\n",
            "Training Epoch: 7 [2624/50000]\tLoss: 1.6447\tLR: 0.0006400000\n",
            "Training Epoch: 7 [3264/50000]\tLoss: 1.6602\tLR: 0.0006400000\n",
            "Training Epoch: 7 [3904/50000]\tLoss: 1.6598\tLR: 0.0006400000\n",
            "Training Epoch: 7 [4544/50000]\tLoss: 1.6406\tLR: 0.0006400000\n",
            "Training Epoch: 7 [5184/50000]\tLoss: 1.6257\tLR: 0.0006400000\n",
            "Training Epoch: 7 [5824/50000]\tLoss: 1.6272\tLR: 0.0006400000\n",
            "Training Epoch: 7 [6464/50000]\tLoss: 1.6285\tLR: 0.0006400000\n",
            "Training Epoch: 7 [7104/50000]\tLoss: 1.6273\tLR: 0.0006400000\n",
            "Training Epoch: 7 [7744/50000]\tLoss: 1.6340\tLR: 0.0006400000\n",
            "Training Epoch: 7 [8384/50000]\tLoss: 1.6338\tLR: 0.0006400000\n",
            "Training Epoch: 7 [9024/50000]\tLoss: 1.6407\tLR: 0.0006400000\n",
            "Training Epoch: 7 [9664/50000]\tLoss: 1.6353\tLR: 0.0006400000\n",
            "Training Epoch: 7 [10304/50000]\tLoss: 1.6383\tLR: 0.0006400000\n",
            "Training Epoch: 7 [10944/50000]\tLoss: 1.6462\tLR: 0.0006400000\n",
            "Training Epoch: 7 [11584/50000]\tLoss: 1.6484\tLR: 0.0006400000\n",
            "Training Epoch: 7 [12224/50000]\tLoss: 1.6574\tLR: 0.0006400000\n",
            "Training Epoch: 7 [12864/50000]\tLoss: 1.6664\tLR: 0.0006400000\n",
            "Training Epoch: 7 [13504/50000]\tLoss: 1.6645\tLR: 0.0006400000\n",
            "Training Epoch: 7 [14144/50000]\tLoss: 1.6640\tLR: 0.0006400000\n",
            "Training Epoch: 7 [14784/50000]\tLoss: 1.6581\tLR: 0.0006400000\n",
            "Training Epoch: 7 [15424/50000]\tLoss: 1.6594\tLR: 0.0006400000\n",
            "Training Epoch: 7 [16064/50000]\tLoss: 1.6605\tLR: 0.0006400000\n",
            "Training Epoch: 7 [16704/50000]\tLoss: 1.6528\tLR: 0.0006400000\n",
            "Training Epoch: 7 [17344/50000]\tLoss: 1.6486\tLR: 0.0006400000\n",
            "Training Epoch: 7 [17984/50000]\tLoss: 1.6482\tLR: 0.0006400000\n",
            "Training Epoch: 7 [18624/50000]\tLoss: 1.6500\tLR: 0.0006400000\n",
            "Training Epoch: 7 [19264/50000]\tLoss: 1.6495\tLR: 0.0006400000\n",
            "Training Epoch: 7 [19904/50000]\tLoss: 1.6531\tLR: 0.0006400000\n",
            "Training Epoch: 7 [20544/50000]\tLoss: 1.6589\tLR: 0.0006400000\n",
            "Training Epoch: 7 [21184/50000]\tLoss: 1.6584\tLR: 0.0006400000\n",
            "Training Epoch: 7 [21824/50000]\tLoss: 1.6593\tLR: 0.0006400000\n",
            "Training Epoch: 7 [22464/50000]\tLoss: 1.6585\tLR: 0.0006400000\n",
            "Training Epoch: 7 [23104/50000]\tLoss: 1.6571\tLR: 0.0006400000\n",
            "Training Epoch: 7 [23744/50000]\tLoss: 1.6590\tLR: 0.0006400000\n",
            "Training Epoch: 7 [24384/50000]\tLoss: 1.6593\tLR: 0.0006400000\n",
            "Training Epoch: 7 [25024/50000]\tLoss: 1.6602\tLR: 0.0006400000\n",
            "Training Epoch: 7 [25664/50000]\tLoss: 1.6575\tLR: 0.0006400000\n",
            "Training Epoch: 7 [26304/50000]\tLoss: 1.6589\tLR: 0.0006400000\n",
            "Training Epoch: 7 [26944/50000]\tLoss: 1.6589\tLR: 0.0006400000\n",
            "Training Epoch: 7 [27584/50000]\tLoss: 1.6591\tLR: 0.0006400000\n",
            "Training Epoch: 7 [28224/50000]\tLoss: 1.6571\tLR: 0.0006400000\n",
            "Training Epoch: 7 [28864/50000]\tLoss: 1.6592\tLR: 0.0006400000\n",
            "Training Epoch: 7 [29504/50000]\tLoss: 1.6586\tLR: 0.0006400000\n",
            "Training Epoch: 7 [30144/50000]\tLoss: 1.6563\tLR: 0.0006400000\n",
            "Training Epoch: 7 [30784/50000]\tLoss: 1.6551\tLR: 0.0006400000\n",
            "Training Epoch: 7 [31424/50000]\tLoss: 1.6547\tLR: 0.0006400000\n",
            "Training Epoch: 7 [32064/50000]\tLoss: 1.6534\tLR: 0.0006400000\n",
            "Training Epoch: 7 [32704/50000]\tLoss: 1.6507\tLR: 0.0006400000\n",
            "Training Epoch: 7 [33344/50000]\tLoss: 1.6525\tLR: 0.0006400000\n",
            "Training Epoch: 7 [33984/50000]\tLoss: 1.6527\tLR: 0.0006400000\n",
            "Training Epoch: 7 [34624/50000]\tLoss: 1.6519\tLR: 0.0006400000\n",
            "Training Epoch: 7 [35264/50000]\tLoss: 1.6522\tLR: 0.0006400000\n",
            "Training Epoch: 7 [35904/50000]\tLoss: 1.6531\tLR: 0.0006400000\n",
            "Training Epoch: 7 [36544/50000]\tLoss: 1.6533\tLR: 0.0006400000\n",
            "Training Epoch: 7 [37184/50000]\tLoss: 1.6563\tLR: 0.0006400000\n",
            "Training Epoch: 7 [37824/50000]\tLoss: 1.6579\tLR: 0.0006400000\n",
            "Training Epoch: 7 [38464/50000]\tLoss: 1.6565\tLR: 0.0006400000\n",
            "Training Epoch: 7 [39104/50000]\tLoss: 1.6574\tLR: 0.0006400000\n",
            "Training Epoch: 7 [39744/50000]\tLoss: 1.6569\tLR: 0.0006400000\n",
            "Training Epoch: 7 [40384/50000]\tLoss: 1.6581\tLR: 0.0006400000\n",
            "Training Epoch: 7 [41024/50000]\tLoss: 1.6598\tLR: 0.0006400000\n",
            "Training Epoch: 7 [41664/50000]\tLoss: 1.6602\tLR: 0.0006400000\n",
            "Training Epoch: 7 [42304/50000]\tLoss: 1.6592\tLR: 0.0006400000\n",
            "Training Epoch: 7 [42944/50000]\tLoss: 1.6603\tLR: 0.0006400000\n",
            "Training Epoch: 7 [43584/50000]\tLoss: 1.6584\tLR: 0.0006400000\n",
            "Training Epoch: 7 [44224/50000]\tLoss: 1.6601\tLR: 0.0006400000\n",
            "Training Epoch: 7 [44864/50000]\tLoss: 1.6620\tLR: 0.0006400000\n",
            "Training Epoch: 7 [45504/50000]\tLoss: 1.6613\tLR: 0.0006400000\n",
            "Training Epoch: 7 [46144/50000]\tLoss: 1.6604\tLR: 0.0006400000\n",
            "Training Epoch: 7 [46784/50000]\tLoss: 1.6610\tLR: 0.0006400000\n",
            "Training Epoch: 7 [47424/50000]\tLoss: 1.6615\tLR: 0.0006400000\n",
            "Training Epoch: 7 [48064/50000]\tLoss: 1.6620\tLR: 0.0006400000\n",
            "Training Epoch: 7 [48704/50000]\tLoss: 1.6623\tLR: 0.0006400000\n",
            "Training Epoch: 7 [49344/50000]\tLoss: 1.6628\tLR: 0.0006400000\n",
            "Training Epoch: 7 [49984/50000]\tLoss: 1.6650\tLR: 0.0006400000\n",
            "Training Epoch: 8 [64/50000]\tLoss: 1.6397\tLR: 0.0006400000\n",
            "Training Epoch: 8 [704/50000]\tLoss: 1.6547\tLR: 0.0006400000\n",
            "Training Epoch: 8 [1344/50000]\tLoss: 1.6483\tLR: 0.0006400000\n",
            "Training Epoch: 8 [1984/50000]\tLoss: 1.6203\tLR: 0.0006400000\n",
            "Training Epoch: 8 [2624/50000]\tLoss: 1.6187\tLR: 0.0006400000\n",
            "Training Epoch: 8 [3264/50000]\tLoss: 1.6345\tLR: 0.0006400000\n",
            "Training Epoch: 8 [3904/50000]\tLoss: 1.6552\tLR: 0.0006400000\n",
            "Training Epoch: 8 [4544/50000]\tLoss: 1.6718\tLR: 0.0006400000\n",
            "Training Epoch: 8 [5184/50000]\tLoss: 1.6946\tLR: 0.0006400000\n",
            "Training Epoch: 8 [5824/50000]\tLoss: 1.6984\tLR: 0.0006400000\n",
            "Training Epoch: 8 [6464/50000]\tLoss: 1.7000\tLR: 0.0006400000\n",
            "Training Epoch: 8 [7104/50000]\tLoss: 1.7107\tLR: 0.0006400000\n",
            "Training Epoch: 8 [7744/50000]\tLoss: 1.7199\tLR: 0.0006400000\n",
            "Training Epoch: 8 [8384/50000]\tLoss: 1.7294\tLR: 0.0006400000\n",
            "Training Epoch: 8 [9024/50000]\tLoss: 1.7248\tLR: 0.0006400000\n",
            "Training Epoch: 8 [9664/50000]\tLoss: 1.7143\tLR: 0.0006400000\n",
            "Training Epoch: 8 [10304/50000]\tLoss: 1.7077\tLR: 0.0006400000\n",
            "Training Epoch: 8 [10944/50000]\tLoss: 1.7052\tLR: 0.0006400000\n",
            "Training Epoch: 8 [11584/50000]\tLoss: 1.7010\tLR: 0.0006400000\n",
            "Training Epoch: 8 [12224/50000]\tLoss: 1.7002\tLR: 0.0006400000\n",
            "Training Epoch: 8 [12864/50000]\tLoss: 1.6967\tLR: 0.0006400000\n",
            "Training Epoch: 8 [13504/50000]\tLoss: 1.6897\tLR: 0.0006400000\n",
            "Training Epoch: 8 [14144/50000]\tLoss: 1.6833\tLR: 0.0006400000\n",
            "Training Epoch: 8 [14784/50000]\tLoss: 1.6806\tLR: 0.0006400000\n",
            "Training Epoch: 8 [15424/50000]\tLoss: 1.6773\tLR: 0.0006400000\n",
            "Training Epoch: 8 [16064/50000]\tLoss: 1.6727\tLR: 0.0006400000\n",
            "Training Epoch: 8 [16704/50000]\tLoss: 1.6753\tLR: 0.0006400000\n",
            "Training Epoch: 8 [17344/50000]\tLoss: 1.6747\tLR: 0.0006400000\n",
            "Training Epoch: 8 [17984/50000]\tLoss: 1.6708\tLR: 0.0006400000\n",
            "Training Epoch: 8 [18624/50000]\tLoss: 1.6699\tLR: 0.0006400000\n",
            "Training Epoch: 8 [19264/50000]\tLoss: 1.6701\tLR: 0.0006400000\n",
            "Training Epoch: 8 [19904/50000]\tLoss: 1.6705\tLR: 0.0006400000\n",
            "Training Epoch: 8 [20544/50000]\tLoss: 1.6686\tLR: 0.0006400000\n",
            "Training Epoch: 8 [21184/50000]\tLoss: 1.6721\tLR: 0.0006400000\n",
            "Training Epoch: 8 [21824/50000]\tLoss: 1.6729\tLR: 0.0006400000\n",
            "Training Epoch: 8 [22464/50000]\tLoss: 1.6772\tLR: 0.0006400000\n",
            "Training Epoch: 8 [23104/50000]\tLoss: 1.6764\tLR: 0.0006400000\n",
            "Training Epoch: 8 [23744/50000]\tLoss: 1.6757\tLR: 0.0006400000\n",
            "Training Epoch: 8 [24384/50000]\tLoss: 1.6745\tLR: 0.0006400000\n",
            "Training Epoch: 8 [25024/50000]\tLoss: 1.6775\tLR: 0.0006400000\n",
            "Training Epoch: 8 [25664/50000]\tLoss: 1.6761\tLR: 0.0006400000\n",
            "Training Epoch: 8 [26304/50000]\tLoss: 1.6751\tLR: 0.0006400000\n",
            "Training Epoch: 8 [26944/50000]\tLoss: 1.6754\tLR: 0.0006400000\n",
            "Training Epoch: 8 [27584/50000]\tLoss: 1.6751\tLR: 0.0006400000\n",
            "Training Epoch: 8 [28224/50000]\tLoss: 1.6752\tLR: 0.0006400000\n",
            "Training Epoch: 8 [28864/50000]\tLoss: 1.6739\tLR: 0.0006400000\n",
            "Training Epoch: 8 [29504/50000]\tLoss: 1.6755\tLR: 0.0006400000\n",
            "Training Epoch: 8 [30144/50000]\tLoss: 1.6751\tLR: 0.0006400000\n",
            "Training Epoch: 8 [30784/50000]\tLoss: 1.6741\tLR: 0.0006400000\n",
            "Training Epoch: 8 [31424/50000]\tLoss: 1.6725\tLR: 0.0006400000\n",
            "Training Epoch: 8 [32064/50000]\tLoss: 1.6732\tLR: 0.0006400000\n",
            "Training Epoch: 8 [32704/50000]\tLoss: 1.6732\tLR: 0.0006400000\n",
            "Training Epoch: 8 [33344/50000]\tLoss: 1.6715\tLR: 0.0006400000\n",
            "Training Epoch: 8 [33984/50000]\tLoss: 1.6732\tLR: 0.0006400000\n",
            "Training Epoch: 8 [34624/50000]\tLoss: 1.6750\tLR: 0.0006400000\n",
            "Training Epoch: 8 [35264/50000]\tLoss: 1.6757\tLR: 0.0006400000\n",
            "Training Epoch: 8 [35904/50000]\tLoss: 1.6777\tLR: 0.0006400000\n",
            "Training Epoch: 8 [36544/50000]\tLoss: 1.6781\tLR: 0.0006400000\n",
            "Training Epoch: 8 [37184/50000]\tLoss: 1.6777\tLR: 0.0006400000\n",
            "Training Epoch: 8 [37824/50000]\tLoss: 1.6785\tLR: 0.0006400000\n",
            "Training Epoch: 8 [38464/50000]\tLoss: 1.6790\tLR: 0.0006400000\n",
            "Training Epoch: 8 [39104/50000]\tLoss: 1.6782\tLR: 0.0006400000\n",
            "Training Epoch: 8 [39744/50000]\tLoss: 1.6783\tLR: 0.0006400000\n",
            "Training Epoch: 8 [40384/50000]\tLoss: 1.6780\tLR: 0.0006400000\n",
            "Training Epoch: 8 [41024/50000]\tLoss: 1.6793\tLR: 0.0006400000\n",
            "Training Epoch: 8 [41664/50000]\tLoss: 1.6793\tLR: 0.0006400000\n",
            "Training Epoch: 8 [42304/50000]\tLoss: 1.6793\tLR: 0.0006400000\n",
            "Training Epoch: 8 [42944/50000]\tLoss: 1.6780\tLR: 0.0006400000\n",
            "Training Epoch: 8 [43584/50000]\tLoss: 1.6772\tLR: 0.0006400000\n",
            "Training Epoch: 8 [44224/50000]\tLoss: 1.6778\tLR: 0.0006400000\n",
            "Training Epoch: 8 [44864/50000]\tLoss: 1.6800\tLR: 0.0006400000\n",
            "Training Epoch: 8 [45504/50000]\tLoss: 1.6820\tLR: 0.0006400000\n",
            "Training Epoch: 8 [46144/50000]\tLoss: 1.6818\tLR: 0.0006400000\n",
            "Training Epoch: 8 [46784/50000]\tLoss: 1.6812\tLR: 0.0006400000\n",
            "Training Epoch: 8 [47424/50000]\tLoss: 1.6817\tLR: 0.0006400000\n",
            "Training Epoch: 8 [48064/50000]\tLoss: 1.6822\tLR: 0.0006400000\n",
            "Training Epoch: 8 [48704/50000]\tLoss: 1.6822\tLR: 0.0006400000\n",
            "Training Epoch: 8 [49344/50000]\tLoss: 1.6840\tLR: 0.0006400000\n",
            "Training Epoch: 8 [49984/50000]\tLoss: 1.6850\tLR: 0.0006400000\n",
            "Training Epoch: 9 [64/50000]\tLoss: 1.7369\tLR: 0.0006400000\n",
            "Training Epoch: 9 [704/50000]\tLoss: 1.4940\tLR: 0.0006400000\n",
            "Training Epoch: 9 [1344/50000]\tLoss: 1.5252\tLR: 0.0006400000\n",
            "Training Epoch: 9 [1984/50000]\tLoss: 1.5087\tLR: 0.0006400000\n",
            "Training Epoch: 9 [2624/50000]\tLoss: 1.4970\tLR: 0.0006400000\n",
            "Training Epoch: 9 [3264/50000]\tLoss: 1.5092\tLR: 0.0006400000\n",
            "Training Epoch: 9 [3904/50000]\tLoss: 1.5121\tLR: 0.0006400000\n",
            "Training Epoch: 9 [4544/50000]\tLoss: 1.5279\tLR: 0.0006400000\n",
            "Training Epoch: 9 [5184/50000]\tLoss: 1.5177\tLR: 0.0006400000\n",
            "Training Epoch: 9 [5824/50000]\tLoss: 1.5159\tLR: 0.0006400000\n",
            "Training Epoch: 9 [6464/50000]\tLoss: 1.5045\tLR: 0.0006400000\n",
            "Training Epoch: 9 [7104/50000]\tLoss: 1.4986\tLR: 0.0006400000\n",
            "Training Epoch: 9 [7744/50000]\tLoss: 1.4952\tLR: 0.0006400000\n",
            "Training Epoch: 9 [8384/50000]\tLoss: 1.4949\tLR: 0.0006400000\n",
            "Training Epoch: 9 [9024/50000]\tLoss: 1.4921\tLR: 0.0006400000\n",
            "Training Epoch: 9 [9664/50000]\tLoss: 1.4927\tLR: 0.0006400000\n",
            "Training Epoch: 9 [10304/50000]\tLoss: 1.4876\tLR: 0.0006400000\n",
            "Training Epoch: 9 [10944/50000]\tLoss: 1.4884\tLR: 0.0006400000\n",
            "Training Epoch: 9 [11584/50000]\tLoss: 1.4922\tLR: 0.0006400000\n",
            "Training Epoch: 9 [12224/50000]\tLoss: 1.4911\tLR: 0.0006400000\n",
            "Training Epoch: 9 [12864/50000]\tLoss: 1.4969\tLR: 0.0006400000\n",
            "Training Epoch: 9 [13504/50000]\tLoss: 1.4989\tLR: 0.0006400000\n",
            "Training Epoch: 9 [14144/50000]\tLoss: 1.5008\tLR: 0.0006400000\n",
            "Training Epoch: 9 [14784/50000]\tLoss: 1.5016\tLR: 0.0006400000\n",
            "Training Epoch: 9 [15424/50000]\tLoss: 1.5000\tLR: 0.0006400000\n",
            "Training Epoch: 9 [16064/50000]\tLoss: 1.5000\tLR: 0.0006400000\n",
            "Training Epoch: 9 [16704/50000]\tLoss: 1.5000\tLR: 0.0006400000\n",
            "Training Epoch: 9 [17344/50000]\tLoss: 1.4995\tLR: 0.0006400000\n",
            "Training Epoch: 9 [17984/50000]\tLoss: 1.4995\tLR: 0.0006400000\n",
            "Training Epoch: 9 [18624/50000]\tLoss: 1.5040\tLR: 0.0006400000\n",
            "Training Epoch: 9 [19264/50000]\tLoss: 1.5093\tLR: 0.0006400000\n",
            "Training Epoch: 9 [19904/50000]\tLoss: 1.5091\tLR: 0.0006400000\n",
            "Training Epoch: 9 [20544/50000]\tLoss: 1.5086\tLR: 0.0006400000\n",
            "Training Epoch: 9 [21184/50000]\tLoss: 1.5092\tLR: 0.0006400000\n",
            "Training Epoch: 9 [21824/50000]\tLoss: 1.5083\tLR: 0.0006400000\n",
            "Training Epoch: 9 [22464/50000]\tLoss: 1.5100\tLR: 0.0006400000\n",
            "Training Epoch: 9 [23104/50000]\tLoss: 1.5114\tLR: 0.0006400000\n",
            "Training Epoch: 9 [23744/50000]\tLoss: 1.5133\tLR: 0.0006400000\n",
            "Training Epoch: 9 [24384/50000]\tLoss: 1.5140\tLR: 0.0006400000\n",
            "Training Epoch: 9 [25024/50000]\tLoss: 1.5185\tLR: 0.0006400000\n",
            "Training Epoch: 9 [25664/50000]\tLoss: 1.5236\tLR: 0.0006400000\n",
            "Training Epoch: 9 [26304/50000]\tLoss: 1.5241\tLR: 0.0006400000\n",
            "Training Epoch: 9 [26944/50000]\tLoss: 1.5262\tLR: 0.0006400000\n",
            "Training Epoch: 9 [27584/50000]\tLoss: 1.5277\tLR: 0.0006400000\n",
            "Training Epoch: 9 [28224/50000]\tLoss: 1.5309\tLR: 0.0006400000\n",
            "Training Epoch: 9 [28864/50000]\tLoss: 1.5332\tLR: 0.0006400000\n",
            "Training Epoch: 9 [29504/50000]\tLoss: 1.5336\tLR: 0.0006400000\n",
            "Training Epoch: 9 [30144/50000]\tLoss: 1.5332\tLR: 0.0006400000\n",
            "Training Epoch: 9 [30784/50000]\tLoss: 1.5358\tLR: 0.0006400000\n",
            "Training Epoch: 9 [31424/50000]\tLoss: 1.5376\tLR: 0.0006400000\n",
            "Training Epoch: 9 [32064/50000]\tLoss: 1.5383\tLR: 0.0006400000\n",
            "Training Epoch: 9 [32704/50000]\tLoss: 1.5405\tLR: 0.0006400000\n",
            "Training Epoch: 9 [33344/50000]\tLoss: 1.5452\tLR: 0.0006400000\n",
            "Training Epoch: 9 [33984/50000]\tLoss: 1.5487\tLR: 0.0006400000\n",
            "Training Epoch: 9 [34624/50000]\tLoss: 1.5526\tLR: 0.0006400000\n",
            "Training Epoch: 9 [35264/50000]\tLoss: 1.5532\tLR: 0.0006400000\n",
            "Training Epoch: 9 [35904/50000]\tLoss: 1.5549\tLR: 0.0006400000\n",
            "Training Epoch: 9 [36544/50000]\tLoss: 1.5582\tLR: 0.0006400000\n",
            "Training Epoch: 9 [37184/50000]\tLoss: 1.5592\tLR: 0.0006400000\n",
            "Training Epoch: 9 [37824/50000]\tLoss: 1.5601\tLR: 0.0006400000\n",
            "Training Epoch: 9 [38464/50000]\tLoss: 1.5598\tLR: 0.0006400000\n",
            "Training Epoch: 9 [39104/50000]\tLoss: 1.5600\tLR: 0.0006400000\n",
            "Training Epoch: 9 [39744/50000]\tLoss: 1.5597\tLR: 0.0006400000\n",
            "Training Epoch: 9 [40384/50000]\tLoss: 1.5584\tLR: 0.0006400000\n",
            "Training Epoch: 9 [41024/50000]\tLoss: 1.5614\tLR: 0.0006400000\n",
            "Training Epoch: 9 [41664/50000]\tLoss: 1.5663\tLR: 0.0006400000\n",
            "Training Epoch: 9 [42304/50000]\tLoss: 1.5702\tLR: 0.0006400000\n",
            "Training Epoch: 9 [42944/50000]\tLoss: 1.5721\tLR: 0.0006400000\n",
            "Training Epoch: 9 [43584/50000]\tLoss: 1.5750\tLR: 0.0006400000\n",
            "Training Epoch: 9 [44224/50000]\tLoss: 1.5773\tLR: 0.0006400000\n",
            "Training Epoch: 9 [44864/50000]\tLoss: 1.5779\tLR: 0.0006400000\n",
            "Training Epoch: 9 [45504/50000]\tLoss: 1.5775\tLR: 0.0006400000\n",
            "Training Epoch: 9 [46144/50000]\tLoss: 1.5776\tLR: 0.0006400000\n",
            "Training Epoch: 9 [46784/50000]\tLoss: 1.5781\tLR: 0.0006400000\n",
            "Training Epoch: 9 [47424/50000]\tLoss: 1.5820\tLR: 0.0006400000\n",
            "Training Epoch: 9 [48064/50000]\tLoss: 1.5832\tLR: 0.0006400000\n",
            "Training Epoch: 9 [48704/50000]\tLoss: 1.5830\tLR: 0.0006400000\n",
            "Training Epoch: 9 [49344/50000]\tLoss: 1.5841\tLR: 0.0006400000\n",
            "Training Epoch: 9 [49984/50000]\tLoss: 1.5853\tLR: 0.0006400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8baboRBrwZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd5VUp0enA9K",
        "colab_type": "text"
      },
      "source": [
        "# Lamb\n",
        "\n",
        "Training Epoch: 0 [64/50000]\tLoss: 4.6812\tLR: 0.0010000000\n",
        "Training Epoch: 0 [704/50000]\tLoss: 4.7955\tLR: 0.0010000000\n",
        "Training Epoch: 0 [1344/50000]\tLoss: 4.7391\tLR: 0.0010000000\n",
        "Training Epoch: 0 [1984/50000]\tLoss: 4.7414\tLR: 0.0010000000\n",
        "Training Epoch: 0 [2624/50000]\tLoss: 4.7256\tLR: 0.0010000000\n",
        "Training Epoch: 0 [3264/50000]\tLoss: 4.7075\tLR: 0.0010000000\n",
        "Training Epoch: 0 [3904/50000]\tLoss: 4.6920\tLR: 0.0010000000\n",
        "Training Epoch: 0 [4544/50000]\tLoss: 4.6770\tLR: 0.0010000000\n",
        "Training Epoch: 0 [5184/50000]\tLoss: 4.6490\tLR: 0.0010000000\n",
        "Training Epoch: 0 [5824/50000]\tLoss: 4.6201\tLR: 0.0010000000\n",
        "Training Epoch: 0 [6464/50000]\tLoss: 4.5994\tLR: 0.0010000000\n",
        "Training Epoch: 0 [7104/50000]\tLoss: 4.5767\tLR: 0.0010000000\n",
        "Training Epoch: 0 [7744/50000]\tLoss: 4.5514\tLR: 0.0010000000\n",
        "Training Epoch: 0 [8384/50000]\tLoss: 4.5291\tLR: 0.0010000000\n",
        "Training Epoch: 0 [9024/50000]\tLoss: 4.5068\tLR: 0.0010000000\n",
        "Training Epoch: 0 [9664/50000]\tLoss: 4.4816\tLR: 0.0010000000\n",
        "Training Epoch: 0 [10304/50000]\tLoss: 4.4519\tLR: 0.0010000000\n",
        "Training Epoch: 0 [10944/50000]\tLoss: 4.4259\tLR: 0.0010000000\n",
        "Training Epoch: 0 [11584/50000]\tLoss: 4.4006\tLR: 0.0010000000\n",
        "Training Epoch: 0 [12224/50000]\tLoss: 4.3780\tLR: 0.0010000000\n",
        "Training Epoch: 0 [12864/50000]\tLoss: 4.3509\tLR: 0.0010000000\n",
        "Training Epoch: 0 [13504/50000]\tLoss: 4.3255\tLR: 0.0010000000\n",
        "Training Epoch: 0 [14144/50000]\tLoss: 4.3014\tLR: 0.0010000000\n",
        "Training Epoch: 0 [14784/50000]\tLoss: 4.2764\tLR: 0.0010000000\n",
        "Training Epoch: 0 [15424/50000]\tLoss: 4.2493\tLR: 0.0010000000\n",
        "Training Epoch: 0 [16064/50000]\tLoss: 4.2287\tLR: 0.0010000000\n",
        "Training Epoch: 0 [16704/50000]\tLoss: 4.2010\tLR: 0.0010000000\n",
        "Training Epoch: 0 [17344/50000]\tLoss: 4.1778\tLR: 0.0010000000\n",
        "Training Epoch: 0 [17984/50000]\tLoss: 4.1552\tLR: 0.0010000000\n",
        "Training Epoch: 0 [18624/50000]\tLoss: 4.1326\tLR: 0.0010000000\n",
        "Training Epoch: 0 [19264/50000]\tLoss: 4.1070\tLR: 0.0010000000\n",
        "Training Epoch: 0 [19904/50000]\tLoss: 4.0861\tLR: 0.0010000000\n",
        "Training Epoch: 0 [20544/50000]\tLoss: 4.0647\tLR: 0.0010000000\n",
        "Training Epoch: 0 [21184/50000]\tLoss: 4.0414\tLR: 0.0010000000\n",
        "Training Epoch: 0 [21824/50000]\tLoss: 4.0211\tLR: 0.0010000000\n",
        "Training Epoch: 0 [22464/50000]\tLoss: 4.0006\tLR: 0.0010000000\n",
        "Training Epoch: 0 [23104/50000]\tLoss: 3.9794\tLR: 0.0010000000\n",
        "Training Epoch: 0 [23744/50000]\tLoss: 3.9577\tLR: 0.0010000000\n",
        "Training Epoch: 0 [24384/50000]\tLoss: 3.9362\tLR: 0.0010000000"
      ]
    }
  ]
}