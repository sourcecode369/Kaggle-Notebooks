{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels\n!pip install torchtoolbox\n!pip install torchviz\n!pip install efficientnet_pytorch","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pretrainedmodels\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 2.5 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (1.5.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (0.6.0a0+35d732a)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (4.45.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (1.18.1)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (5.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.14.0)\nBuilding wheels for collected packages: pretrainedmodels\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=d7240a236d5cf09ff92f038e8103f4ec3bb30d49789daa32ea0966da9784558d\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built pretrainedmodels\nInstalling collected packages: pretrainedmodels\nSuccessfully installed pretrainedmodels-0.7.4\nCollecting torchtoolbox\n  Downloading torchtoolbox-0.1.4.1-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 1.8 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.23.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.2.0.34)\nCollecting lmdb\n  Downloading lmdb-0.98.tar.gz (869 kB)\n\u001b[K     |████████████████████████████████| 869 kB 10.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (2.1.0)\nBuilding wheels for collected packages: lmdb\n  Building wheel for lmdb (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=273141 sha256=e4bbc8bd93448c8ab459a79bb1153b50bf89a7b31e4ddbe865c814dcc5c7ec27\n  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\nSuccessfully built lmdb\nInstalling collected packages: lmdb, torchtoolbox\nSuccessfully installed lmdb-0.98 torchtoolbox-0.1.4.1\nCollecting torchviz\n  Downloading torchviz-0.0.1.tar.gz (41 kB)\n\u001b[K     |████████████████████████████████| 41 kB 112 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchviz) (1.5.1)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from torchviz) (0.8.4)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchviz) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->torchviz) (1.18.1)\nBuilding wheels for collected packages: torchviz\n  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.1-py3-none-any.whl size=3521 sha256=eefe4d35804a0149c9c2cb89940da049b984b636edcc30c6d4b90fc912a917d9\n  Stored in directory: /root/.cache/pip/wheels/10/7b/c8/3af79ec02e294a832c01037bcb38302bbcee0bb020dcbbbd3e\nSuccessfully built torchviz\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.1\nCollecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.18.1)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=c644618045082e323aab59227a635d653e3e01711dfdeb20f9a6cdab03293287\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.6.3\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%autosave 30\nimport os\nimport gc\ngc.enable()\nimport time\nimport glob\nimport random\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport tqdm as tqdm\nfrom PIL import Image\n\nimport torch\nimport torchvision\nfrom torchvision import transforms, models\nimport pretrainedmodels\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler, BatchSampler, RandomSampler\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torchviz import make_dot\nfrom efficientnet_pytorch import EfficientNet\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.model_selection import GroupKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","execution_count":2,"outputs":[{"output_type":"display_data","data":{"application/javascript":"IPython.notebook.set_autosave_interval(30000)"},"metadata":{}},{"output_type":"stream","text":"Autosaving every 30 seconds\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nSEED = 2020\nseed_everything(SEED)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = []\n\nfor label, kind in enumerate(['Cover', 'JMiPOD', 'JUNIWARD', 'UERD']):\n    for path in glob.glob('../input/alaska2-image-steganalysis/Cover/*.jpg'):\n        dataset.append({\n            'kind': kind,\n            'image_name': path.split('/')[-1],\n            'label': label\n        })\n        \nrandom.shuffle(dataset)\ndataset = pd.DataFrame(dataset)\ngkf = GroupKFold(n_splits=5)\ndataset.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['image_name'])):\n    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        ToTensorV2(p=1.0)\n    ], p=1.0,\n    additional_targets={\"image2\" : \"image\"})\n\ndef get_valid_transforms():\n    return A.Compose([\n        A.Resize(height=512, width=512, p=1.0),\n        ToTensorV2(p=1.0)\n    ], p=1.0)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT_PATH = '/kaggle/input/alaska2-image-steganalysis/'\n\ndef one_hot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, kinds, image_names, labels, transforms=None):\n        super().__init__()\n        self.kinds = kinds\n        self.image_names = image_names\n        self.labels = labels\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        image1 = cv2.resize(image, (512,512))\n        image2 = cv2.resize(image, (331,331))\n        if self.transforms:\n            sample = {'image':image1, 'image2':image2}\n            sample = self.transforms(**sample)\n            image1 = sample['image']\n            image2 = sample['image2']\n            \n        target = one_hot(4, label)\n        return image1, image2, target\n\n    def __len__(self) -> int:\n        return self.image_names.shape[0]\n\n    def get_labels(self):\n        return list(self.labels)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    kinds=dataset[dataset['fold'] != fold_number].kind.values,\n    image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n    labels=dataset[dataset['fold'] != fold_number].label.values,\n    transforms=get_train_transforms(),\n)\n\nvalidation_dataset = DatasetRetriever(\n    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n    labels=dataset[dataset['fold'] == fold_number].label.values,\n    transforms=get_valid_transforms(),\n)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image1, image2, target = train_dataset[0]","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \n        \ndef alaska_weighted_auc(y_true, y_valid):\n    \"\"\"\n    https://www.kaggle.com/anokas/weighted-auc-metric-updated\n    \"\"\"\n    tpr_thresholds = [0.0, 0.4, 1.0]\n    weights = [2, 1]\n\n    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n\n    # size of subsets\n    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n\n    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n    normalization = np.dot(areas, weights)\n\n    competition_metric = 0\n    for idx, weight in enumerate(weights):\n        y_min = tpr_thresholds[idx]\n        y_max = tpr_thresholds[idx + 1]\n        mask = (y_min < tpr) & (tpr < y_max)\n        # pdb.set_trace()\n\n        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n\n        x = np.concatenate([fpr[mask], x_padding])\n        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n        y = y - y_min  # normalize such that curve starts at y=0\n        score = metrics.auc(x, y)\n        submetric = score * weight\n        best_subscore = (y_max - y_min) * weight\n        competition_metric += submetric\n\n    return competition_metric / normalization\n        \nclass RocAucMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.y_true = np.array([0,1])\n        self.y_pred = np.array([0.5,0.5])\n        self.score = 0\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = alaska_weighted_auc(self.y_true, self.y_pred)\n    \n    @property\n    def avg(self):\n        return self.score","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelSmoothing(nn.Module):\n    def __init__(self, smoothing = 0.05):\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        \n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n            smooth_loss = -logprobs.mean(dim=-1)\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n            return loss.mean()\n        else:\n            return torch.nn.functional.cross_entropy(x, target)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dual_EfficientNets(nn.Module):\n    def __init__(self, pretrained='imagenet'):\n        super(Dual_EfficientNets, self).__init__()\n#         self.model1 = pretrainedmodels.__dict__['resnet34'](pretrained=pretrained)\n#         self.model1 = pretrainedmodels.__dict__['se_resnext50_32x4d'](pretrained=None)\n#         if pretrained is not None:\n#             self.model1.load_state_dict(\n#                 torch.load('../input/pretrained-model-weights-pytorch/se_resnext50_32x4d-a260b3a4.pth')\n#             )\n#             self.model2.load_state_dict(\n#                 torch.load('../input/pretrained-model-weights-pytorch/resnet34-333f7ec4.pth')\n#             )\n#         self.model1 = torchvision.models.resnet50(pretrained='imagenet')\n#         self.model2 = torchvision.models.resnet34(pretrained='imagenet')\n        self.model1 = EfficientNet.from_pretrained(model_name='efficientnet-b2')\n        self.model1._fc = nn.Linear(in_features=1408, out_features=4, bias=True)\n#         self.model2 = EfficientNet.from_pretrained(model_name='efficientnet-b0')\n#         self.flatten = nn.Flatten()\n#         self.adaptive_pooling = nn.AdaptiveAvgPool2d(1)\n#         self._fc = nn.Linear(in_features=524288, out_features=4, bias=True)\n    def forward(self, x1, x2):\n#         x2 = self.model2.extract_features(x2)\n#         x1 = self.adaptive_pooling(x1)\n#         x2 = self.adaptive_pooling(x2)\n#         x = torch.cat([x1, x2], 1).squeeze()\n#         x1 = self.flatten(x1)\n#         return self._fc(x1)\n        return self.model1(x1)\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Dual_EfficientNets(pretrained='imagenet')\noutput = model(image1.unsqueeze(0), image2.unsqueeze(0))\ndot = make_dot(output, dict(model.named_parameters()))\ndot.format='png'\ndot.render()","execution_count":12,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/checkpoints/efficientnet-b2-8bb594d6.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=36804509.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c5079af6a24f1aa2d8eb5e339f76f7"}},"metadata":{}},{"output_type":"stream","text":"\nLoaded pretrained weights for efficientnet-b2\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"'Digraph.gv.png'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Fitter:\n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n        self.base_dir = './'\n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n        \n        self.model = model\n        self.device = device\n        self.model.to(device)\n        self.log(f'Fitter prepared. Device is {self.device}')\n        \n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and '_fc' not in n], 'weight_decay': 0.001, 'lr':1e-3},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and '_fc' not in n], 'weight_decay': 0.0, 'lr':1e-3},\n            {'params': [p for n, p in param_optimizer if '_fc' in n], 'lr':3e-3, 'weight_decay':0.001}\n        ] \n        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-3)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.criterion = LabelSmoothing().to(self.device)\n    \n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        final_scores = RocAucMeter()\n        time1 = time.time()\n        for step, (images1, images2, targets) in enumerate(train_loader):\n            if self.config.verbose:\n                if step%self.config.verbose_step==0:\n                    print(f'Train step: {step}/{len(train_loader)}, \\\n                          Total Loss: {summary_loss.avg:.3f}, \\\n                          RoC Auc Score: {final_scores.avg:.3f}, \\\n                          Total Time: {time.time()-time1:.2f}secs.', \n                          end='\\r'\n                         )\n            targets = torch.tensor(targets, device=self.device, dtype=torch.float32)\n            images1 = torch.tensor(images1, device=self.device, dtype=torch.float32)\n            images2 = torch.tensor(images1, device=self.device, dtype=torch.float32)\n            batch_size = targets.shape[0]\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(images1, images2)\n            loss = self.criterion(outputs, targets)\n            loss.backward()\n            self.optimizer.step()\n            final_scores.update(targets, outputs)\n            summary_loss.update(loss.detach().item(), batch_size)\n            \n        return summary_loss, final_scores\n    \n    def fit(self, train_loader, validation_loader):\n        for epoch in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr1 = self.optimizer.param_groups[0][\"lr\"]\n                lr2 = self.optimizer.param_groups[-1][\"lr\"]\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f\"\\n{timestamp}\\nLR Backbone:{lr1}, LR Head:{lr2}\")\n            \n            time1 = time.time()\n            summary_loss, final_scores = self.train_one_epoch(train_loader)\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, \\\n                     Total Loss: {summary_loss.avg:.3f}, \\\n                     RoC Auc Score: {final_scores.avg:.3f}, \\\n                     Time: {(time.time() - time1):.2f} secs.')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n            \n            time1 = time.time()\n            summary_loss, final_scores = self.validation(validation_loader)\n            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, \\\n                     Total Loss: {summary_loss.avg:.3f}, \\\n                     RoC Auc Score: {final_scores.avg:.3f}, \\\n                     Time: {(time.time() - time1):.2f} secs.')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n                \n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n            \n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n            \n            self.epoch+=1\n        \n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        final_scores = RocAucMeter()\n        time1 = time.time()\n        for step, (images1, images2, targets) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(f'Validation step: {step}/{len(val_loader)}, \\\n                      Total Loss: {summary_loss.avg:.3f}, \\\n                      RoC Auc Score: {final_scores.avg:.3f}, \\\n                      Total Time: {time.time()-time1:.2f}secs.', \n                      end='\\r'\n                     )\n            with torch.no_grad():\n                targets = torch.tensor(targets, device=self.device, dtype=torch.float32)\n                images1 = torch.tensor(images1, device=self.device, dtype=torch.float32)\n                images2 = torch.tensor(images1, device=self.device, dtype=torch.float32)\n                batch_size = targets.shape[0]\n                outputs = self.model(images)\n                loss = self.criterion(outputs, targets)\n                final_scores.update(targets, outputs)\n                summary_loss.update(loss.detach().item(), batch_size)\n        return summary_loss, final_scores\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 4\n    batch_size = 16\n    n_epochs = 30\n    \n    verbose = True\n    verbose_step = 1\n    \n    step_scheduler = False  \n    validation_scheduler = True  \n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    ","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.data.sampler import BalanceClassSampler\n\ndef run_training():\n    device = torch.device('cuda:0')\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n        batch_size=TrainGlobalConfig.batch_size,\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n    )\n\n    fitter = Fitter(model=Dual_EfficientNets(pretrained='imagenet'), device=device, config=TrainGlobalConfig)\n    fitter.fit(train_loader, val_loader)","execution_count":15,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training()","execution_count":null,"outputs":[{"output_type":"stream","text":"Loaded pretrained weights for efficientnet-b2\nFitter prepared. Device is cuda:0\n\n2020-07-05T13:50:29.153194\nLR Backbone:0.001, LR Head:0.003\nTrain step: 0/15000,                           Total Loss: 0.000,                           RoC Auc Score: 0.000,                           Total Time: 1.09secs.\r","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n/opt/conda/lib/python3.7/site-packages/efficientnet_pytorch/utils.py:45: DeprecationWarning:\n\n'saved_variables' is deprecated; use 'saved_tensors'\n\n","name":"stderr"},{"output_type":"stream","text":"Train step: 10622/15000,                           Total Loss: 1.160,                           RoC Auc Score: 0.739,                           Total Time: 6196.44secs.\r","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}