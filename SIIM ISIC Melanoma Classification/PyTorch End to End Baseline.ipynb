{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install torchcontrib\n!pip install efficientnet_pytorch\n!pip install pretrainedmodels\n!pip install pytorch_toolbelt\n!pip install torch_optimizer\n!git clone https://github.com/4uiiurz1/pytorch-auto-augment > /dev/null\n# !conda install -c conda-forge nvidia-apex --yes\n# !conda install -c conda-forge/label/cf202003 nvidia-apex --yes\n!pip install --upgrade pillow","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport sys\nimport math\nsys.path.insert(0, './pytorch-auto-augment')\nimport time   \nimport random\nfrom glob import glob\nfrom datetime import datetime\nfrom typing import List\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torch import Tensor\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\n\n#import apex\nimport catalyst\nimport efficientnet_pytorch\nimport pytorch_toolbelt\nfrom pytorch_toolbelt.modules.pooling import (GlobalMaxPool2d, \n                                              GWAP,\n                                              GlobalAvgPool2d, \n                                              GlobalRankPooling, \n                                              GlobalWeightedAvgPool2d,\n                                             )\nimport torch_optimizer as optim\nimport torchcontrib\nimport albumentations as A\nfrom auto_augment import AutoAugment, Cutout\nimport pretrainedmodels\nfrom albumentations.augmentations.functional import brightness_contrast_adjust, elastic_transform\nimport json\nimport sklearn\nfrom pytorch_toolbelt.utils import count_parameters\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nSEED = 2020\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/melanoma-256x256/train.csv')\ntrain['kfold'] = -1\ntrain.sample(frac=1).reset_index(drop=True)\nskf = KFold(n_splits=5, shuffle=True, random_state=2020)\nfor fold, (idxT, idxV) in enumerate(skf.split(np.arange(15))):\n    train.loc[train.loc[train.tfrecord.isin(idxV)].index,'kfold'] = fold\ntrain.to_csv('train_folds.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augmentations Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdvancedHairAugmentation(A.ImageOnlyTransform):\n    \"\"\"\n    Impose an image of a hair to the target image\n\n    Args:\n        hairs (int): maximum number of hairs to impose\n        hairs_folder (str): path to the folder with hairs images\n    \"\"\"\n\n    def __init__(self, hairs: int = 5, hairs_folder: str = \"\", always_apply=False, p=0.5):\n        super(AdvancedHairAugmentation, self).__init__(always_apply, p)\n\n        self.hairs = hairs\n        self.hairs_folder = hairs_folder\n\n    def apply(self, img, **params):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to draw hairs on.\n\n        Returns:\n            PIL Image: Image with drawn hairs.\n        \"\"\"\n        n_hairs = random.randint(0, self.hairs)\n        \n        if not n_hairs:\n            return img\n        \n        height, width, _ = img.shape  # target image width and height\n        hair_images = [im for im in os.listdir(self.hairs_folder) if 'png' in im]\n        \n        for _ in range(n_hairs):\n            hair = cv2.imread(os.path.join(self.hairs_folder, random.choice(hair_images)))\n            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n\n            h_height, h_width, _ = hair.shape  # hair image width and height\n            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n            roi = img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width]\n\n            # Creating a mask and inverse mask\n            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n            mask_inv = cv2.bitwise_not(mask)\n\n            # Now black-out the area of hair in ROI\n            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n\n            # Take only region of hair from hair image.\n            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n\n            # Put hair in ROI and modify the target image\n            dst = cv2.add(img_bg, hair_fg)\n\n            img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width] = dst\n                \n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(hairs={self.hairs}, hairs_folder=\"{self.hairs_folder}\")'\n    \nclass DrawHair(A.ImageOnlyTransform):\n    \"\"\"\n    Draw a random number of pseudo hairs\n\n    Args:\n        hairs (int): maximum number of hairs to draw\n        width (tuple): possible width of the hair in pixels\n    \"\"\"\n\n    def __init__(self, hairs:int = 4, always_apply=False, width:tuple = (1, 2),p=0.5):\n        super(DrawHair, self).__init__(always_apply, p)\n        self.hairs = hairs\n        self.width = width\n\n    def apply(self, img, **params):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to draw hairs on.\n\n        Returns:\n            PIL Image: Image with drawn hairs.\n        \"\"\"\n        if not self.hairs:\n            return img\n        \n        width, height, _ = img.shape\n        \n        for _ in range(random.randint(0, self.hairs)):\n            # The origin point of the line will always be at the top half of the image\n            origin = (random.randint(0, width), random.randint(0, height // 2))\n            # The end of the line \n            end = (random.randint(0, width), random.randint(0, height))\n            color = (0, 0, 0)  # color of the hair. Black.\n            cv2.line(img, origin, end, color, random.randint(self.width[0], self.width[1]))\n        \n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(hairs={self.hairs}, width={self.width})'\n    \nclass Microscope(A.ImageOnlyTransform):\n    \"\"\"\n    Cutting out the edges around the center circle of the image\n    Imitating a picture, taken through the microscope\n\n    Args:\n        p (float): probability of applying an augmentation\n    \"\"\"\n\n    def __init__(self, always_apply=False, p: float = 0.5):\n        super(Microscope, self).__init__(always_apply, p)\n        self.p = p\n\n    def apply(self, img, **params):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to apply transformation to.\n\n        Returns:\n            PIL Image: Image with transformation.\n        \"\"\"\n        if random.random() < self.p:\n            circle = cv2.circle((np.ones(img.shape) * 255).astype(np.uint8), # image placeholder\n                        (img.shape[0]//2, img.shape[1]//2), # center point of circle\n                        random.randint(img.shape[0]//2 - 3, img.shape[0]//2 + 15), # radius\n                        (0, 0, 0), # color\n                        -1)\n\n            mask = circle - 255\n            img = np.multiply(img, mask)\n        \n        return img\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(p={self.p})'\n    \nclass IndependentRandomBrightnessContrast(A.ImageOnlyTransform):\n    \"\"\" Change brightness & contrast independently per channels \"\"\"\n\n    def __init__(self, brightness_limit=0.2, contrast_limit=0.2, always_apply=False, p=0.5):\n        super(IndependentRandomBrightnessContrast, self).__init__(always_apply, p)\n        self.brightness_limit = A.to_tuple(brightness_limit)\n        self.contrast_limit = A.to_tuple(contrast_limit)\n\n    def apply(self, img, **params):\n        img = img.copy()\n        for ch in range(img.shape[2]):\n            alpha = 1.0 + random.uniform(self.contrast_limit[0], self.contrast_limit[1])\n            beta = 0.0 + random.uniform(self.brightness_limit[0], self.brightness_limit[1])\n            img[..., ch] = brightness_contrast_adjust(img[..., ch], alpha, beta)\n\n        return img\n\nclass BrightnessContrastDestroy(A.ImageOnlyTransform):\n    def __init__(self, p=0.5):\n        super().__init__(p=p)\n\n    @property\n    def targets(self):\n        return {'image': self.apply, 'diagnosis': self.apply_to_diagnosis}\n\n    def apply_to_diagnosis(self, diagnosis, **params):\n        return 0\n\n    def apply(self, img, alpha=0, beta=0, **params):\n        from albumentations.augmentations.functional import brightness_contrast_adjust\n        img = brightness_contrast_adjust(img, alpha=alpha, beta=beta)\n        return img\n\n    def get_params(self):\n        return {'alpha': random.uniform(0.05, 0.25),\n                'beta': random.uniform(-0.5, -0.75)}\n\n\nclass MakeTooBlurry(A.ImageOnlyTransform):\n    def __init__(self, p=0.5):\n        super().__init__(p=p)\n\n    @property\n    def targets(self):\n        return {'image': self.apply, 'diagnosis': self.apply_to_diagnosis}\n\n    def apply_to_diagnosis(self, diagnosis, **params):\n        return 0\n\n    def apply(self, img, blur_ksize=3, **params):\n        img = cv2.boxFilter(img, ddepth=cv2.CV_8U, ksize=(blur_ksize, blur_ksize))\n        return img\n\n    def get_params(self):\n        return {'blur_ksize': int(random.uniform(21, 32)) * 2 + 1}\n\n\nclass MakeTooBlurryMedian(A.ImageOnlyTransform):\n    def __init__(self, p=0.5):\n        super().__init__(p=p)\n\n    @property\n    def targets(self):\n        return {'image': self.apply, 'diagnosis': self.apply_to_diagnosis}\n\n    def apply_to_diagnosis(self, diagnosis, **params):\n        return 0\n\n    def apply(self, img, blur_ksize=3, **params):\n        img = cv2.medianBlur(img, ksize=blur_ksize)\n        return img\n\n    def get_params(self):\n        return {'blur_ksize': int(random.uniform(8, 16)) * 2 + 1}\n\ndef fancy_pca(img, alpha=0.1):\n    orig_img = img.astype(float).copy()\n    img = img / 255.0\n    img_rs = img.reshape(-1, 3)\n    img_centered = img_rs - np.mean(img_rs, axis=0)\n    img_cov = np.cov(img_centered, rowvar=False)\n    eig_vals, eig_vecs = np.linalg.eigh(img_cov)\n    sort_perm = eig_vals[::-1].argsort()\n    eig_vals[::-1].sort()\n    eig_vecs = eig_vecs[:, sort_perm]\n    m1 = np.column_stack((eig_vecs))\n    m2 = np.zeros((3, 1))\n    m2[:, 0] = alpha * eig_vals[:]\n    add_vect = np.matrix(m1) * np.matrix(m2)\n    for idx in range(3):  # RGB\n        orig_img[..., idx] += add_vect[idx] * 255\n    orig_img = np.clip(orig_img, 0.0, 255.0)\n    orig_img = orig_img.astype(np.uint8)\n    return orig_img\n\nclass FancyPCA(A.ImageOnlyTransform):\n    def __init__(self, alpha_std=0.1, p=0.5):\n        super().__init__(p=p)\n        self.alpha_std = alpha_std\n\n    def apply(self, img, alpha=0.1, **params):\n        img = fancy_pca(img, alpha)\n        return img\n\n    def get_params(self):\n        return {'alpha': random.gauss(0, self.alpha_std)}\n    \ndef clahe_preprocessing(image, clip_limit=4.0, tile_grid_size=(18, 18)):\n    image_norm = image.copy()\n\n    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n    image_norm[:, :, 0] = clahe.apply(image[:, :, 0])\n    image_norm[:, :, 1] = clahe.apply(image[:, :, 1])\n    image_norm[:, :, 2] = clahe.apply(image[:, :, 2])\n\n    # image_norm = cv2.addWeighted(image, 0.5, image_norm, 0.5, 0)\n    return image_norm\n\nclass ChannelIndependentCLAHE(A.ImageOnlyTransform):\n    def __init__(self, p=1.0):\n        super().__init__(p=p)\n\n    def apply(self, img, **params):\n        return clahe_preprocessing(img)\n\n    def get_transform_init_args_names(self):\n        return tuple()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_none_augmentations(image_size):\n    return A.NoOp()\n\n\ndef get_light_augmentations(image_size):\n    return A.Compose([\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n                           rotate_limit=15,\n                           border_mode=cv2.BORDER_CONSTANT, value=0),\n        A.RandomSizedCrop(min_max_height=(int(image_size[0] * 0.85), image_size[0]),\n                          height=image_size[0],\n                          width=image_size[1], p=0.3),\n        Microscope(p=0.3),\n        # Brightness/contrast augmentations\n        A.OneOf([\n            A.RandomBrightnessContrast(brightness_limit=0.25,\n                                       contrast_limit=0.2),\n            IndependentRandomBrightnessContrast(brightness_limit=0.1,\n                                                contrast_limit=0.1),\n            A.RandomGamma(gamma_limit=(75, 125)),\n            A.NoOp()\n        ]),\n        A.OneOf([\n            ChannelIndependentCLAHE(p=0.5),\n            A.CLAHE(),\n            A.NoOp()\n        ]),\n        A.HorizontalFlip(p=0.5),\n    ])\n\n\ndef get_medium_augmentations(image_size):\n    return A.Compose([\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n                               rotate_limit=15,\n                               border_mode=cv2.BORDER_CONSTANT, value=0),\n            A.OpticalDistortion(distort_limit=0.11, shift_limit=0.15,\n                                border_mode=cv2.BORDER_CONSTANT,\n                                value=0),\n            A.NoOp()\n        ]),\n        Microscope(p=0.3),\n        A.RandomSizedCrop(min_max_height=(int(image_size[0] * 0.75), image_size[0]),\n                          height=image_size[0],\n                          width=image_size[1], p=0.3),\n        A.OneOf([\n            A.RandomBrightnessContrast(brightness_limit=0.5,\n                                       contrast_limit=0.4),\n            IndependentRandomBrightnessContrast(brightness_limit=0.25,\n                                                contrast_limit=0.24),\n            A.RandomGamma(gamma_limit=(50, 150)),\n            A.NoOp()\n        ]),\n        A.OneOf([\n            FancyPCA(alpha_std=4),\n            A.RGBShift(r_shift_limit=20, b_shift_limit=15, g_shift_limit=15),\n            A.HueSaturationValue(hue_shift_limit=5,\n                                 sat_shift_limit=5),\n            A.NoOp()\n        ]),\n        A.OneOf([\n            ChannelIndependentCLAHE(p=0.5),\n            A.CLAHE(),\n            A.NoOp()\n        ]),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5)\n    ])\n\ndef get_hard_augmentations(image_size):\n    return A.Compose([\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n                               rotate_limit=45,\n                               border_mode=cv2.BORDER_CONSTANT, value=0),\n            A.ElasticTransform(alpha_affine=0,\n                               alpha=35,\n                               sigma=5,\n                               border_mode=cv2.BORDER_CONSTANT,\n                               value=0),\n            A.OpticalDistortion(distort_limit=0.11, shift_limit=0.15,\n                                border_mode=cv2.BORDER_CONSTANT,\n                                value=0),\n            A.GridDistortion(border_mode=cv2.BORDER_CONSTANT,\n                             value=0),\n            A.NoOp()\n        ]),\n\n        A.OneOf([\n            Microscope(p=0.3),\n\n            A.RandomSizedCrop(min_max_height=(int(image_size[0] * 0.75), image_size[0]),\n                              height=image_size[0],\n                              width=image_size[1], p=0.3),\n            A.NoOp()\n        ]),\n\n        A.ISONoise(p=0.5),\n        A.JpegCompression(p=0.3, quality_lower=75),\n\n        # Brightness/contrast augmentations\n        A.OneOf([\n            A.RandomBrightnessContrast(brightness_limit=0.5,\n                                       contrast_limit=0.4),\n            IndependentRandomBrightnessContrast(brightness_limit=0.25,\n                                                contrast_limit=0.24),\n            A.RandomGamma(gamma_limit=(50, 150)),\n            A.NoOp()\n        ]),\n\n        A.OneOf([\n            FancyPCA(alpha_std=6),\n            A.RGBShift(r_shift_limit=40, b_shift_limit=30, g_shift_limit=30),\n            A.HueSaturationValue(hue_shift_limit=10,\n                                 sat_shift_limit=10),\n            A.ToGray(p=0.2),\n            A.NoOp()\n        ]),\n\n        # Add preprocessing method as an augmentation\n        ChannelIndependentCLAHE(p=0.5),\n\n        A.OneOf([\n            A.ChannelDropout(p=0.2),\n            A.CoarseDropout(p=0.1, max_holes=2, max_width=256, max_height=256, min_height=16, min_width=16),\n            A.RandomGridShuffle(p=0.3),\n            A.NoOp()\n        ]),\n\n        A.RandomGridShuffle(p=0.3),\n        DrawHair(p=0.3),\n\n        # D4\n        A.Compose([\n            A.RandomRotate90(),\n            A.Transpose()\n        ])\n    ])\n\n\ndef get_train_transform(image_size, augmentation=None):\n    if augmentation is None:\n        augmentation = 'none'\n\n    LEVELS = {\n        'none': get_none_augmentations,\n        'light': get_light_augmentations,\n        'medium': get_medium_augmentations,\n        'hard': get_hard_augmentations,\n    }\n\n    assert augmentation in LEVELS.keys(), f\"Augmentation {augmentation} not found.\"\n    augmentation = LEVELS[augmentation](image_size)\n\n    longest_size = max(image_size[0], image_size[1])\n    return A.Compose([\n        A.LongestMaxSize(longest_size, interpolation=cv2.INTER_CUBIC),\n        A.PadIfNeeded(image_size[0], image_size[1],\n                      border_mode=cv2.BORDER_CONSTANT, value=0),\n        augmentation,\n        A.Normalize()\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transforms = get_train_transform([512,512],'hard')\nimage = cv2.cvtColor(cv2.imread('../input/jpeg-isic2019-1024x1024/train/ISIC_0000010.jpg', cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\nimage_sample = {'image':image}\nimage_sample = train_transforms(**image_sample)\nimage = image_sample['image']\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Retriever Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_class_names():\n    CLASS_NAMES = [\n        'benign',\n        'malignant'\n    ]\n    return CLASS_NAMES\n\nclass MelanomaTrainDataset(Dataset):\n    def __init__(self, dataframe:pd.DataFrame, \n                 dataset_dir:str, \n                 classes:int,\n                 fold_idx:int=0,\n                 transforms:A.Compose=None,\n                 do_one_hot:bool=True,\n                 meta_features=False\n                ):\n        self.df = dataframe\n        self.image_ids = self.df.image_name.values\n        self.targets = self.df.target.values\n        if self.targets is not None:\n            targets = np.array(self.targets)\n            unique_targets = set(targets)\n            if len(unique_targets.difference({0, 1})):\n                raise ValueError('Unexpected targets in Y ' + str(unique_targets))\n        self.dataset_dir = dataset_dir\n        self.fold_idx = fold_idx\n        self.transforms = transforms\n        self.classes = classes\n        self.do_one_hot = do_one_hot\n        self.meta_features = meta_features\n        \n    def __len__(self):\n        return len(self.image_ids)\n        \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        target = self.targets[index]\n        if self.do_one_hot:\n            one_hot_target = torch.zeros(self.classes, dtype=torch.float32)\n            one_hot_target[target] = 1.\n            target = one_hot_target        \n        image_path = f'{self.dataset_dir}/{image_id}.jpg'\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        if image is None:\n            raise FileNotFoundError(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms is not None:\n            image_sample = {'image':image}\n            image_sample = self.transforms(**image_sample)\n            image = image_sample['image']\n        \n        data = {'images': image,\n                'targets': target}\n        \n        height, width = image.shape[:2]\n        if self.meta_features:\n            log_height = math.log(height)\n            log_width = math.log(width)\n            aspect_ratio = log_height / log_width\n            mean = np.mean(image, axis=(0, 1))\n            meta_features = np.array([\n                log_height,\n                log_width,\n                aspect_ratio,\n                mean[0],\n                mean[1],\n                mean[2]\n            ])\n            data['meta_features'] = meta_features\n        return data\n    \n    def get_labels(self):\n        return list(self.targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold = 0\n\nDIR = '../input/jpeg-melanoma-1024x1024/train'\n\ntrain_dataset = MelanomaTrainDataset(dataframe = train[train.kfold!=fold],\n                                     dataset_dir=DIR,\n                                     classes=len(get_class_names()),\n                                     fold_idx=fold,\n                                     do_one_hot=False,\n                                     transforms=get_train_transform([512,512],'medium'),\n                                     meta_features=True\n                                    )\n\ndata = train_dataset[2]\nprint(data[\"images\"].shape)\nprint(data['targets'])\nprint(data['meta_features'])\nplt.imshow(data[\"images\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n    \nclass LabelSmoothing(nn.Module):\n    def __init__(self, smoothing = 0.05):\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n            smooth_loss = -logprobs.mean(dim=-1)\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n            return loss.mean()\n        else:\n            return torch.nn.functional.cross_entropy(x, target)\n        \ndef soft_crossentropy(input: torch.Tensor,\n                      target: torch.Tensor,\n                      ignore_index=None,\n                      smooth_factor=0.01,\n                      reduction='mean'):\n    if ignore_index is not None:\n        mask = target != ignore_index\n        target = target[mask]\n        input = input[mask]\n\n    if not len(target):\n        return torch.tensor(0.).type_as(input).to(input.device)\n\n    n_class = input.size(1)\n    one_hot = torch.zeros_like(input).scatter(1, target.view(-1, 1), 1)\n    one_hot = one_hot * (1 - smooth_factor) + (1 - one_hot) * smooth_factor / (n_class - 1)\n    log_prb = F.log_softmax(input, dim=1)\n    loss = -(one_hot * log_prb).sum(dim=1)\n    if reduction == 'mean':\n        loss = loss.mean()\n    if reduction == 'sum':\n        loss = loss.sum()\n    return loss\n\nclass SoftCrossEntropyLoss(nn.Module):\n    def __init__(self, smooth_factor=0.01, ignore_index=None, reduction='mean'):\n        super().__init__()\n        self.smooth_factor = smooth_factor\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor):\n        return soft_crossentropy(input, target,\n                                 ignore_index=self.ignore_index,\n                                 smooth_factor=self.smooth_factor,\n                                 reduction=self.reduction)\n    \nclass SmoothLoss(nn.Module):\n    def __init__(self):\n       super(SmoothLoss, self).__init__()\n    def forward(self, pred, target):\n       pred = pred.log_softmax(dim=1)\n       return torch.mean(torch.sum(-target * pred, dim=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimizer Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def sgd(parameters, lr=0.001, momentum=0.9, weight_decay=0, nesterov=True, **_):\n  return optim.SGD(parameters, \n                   lr=lr, \n                   momentum=momentum, \n                   weight_decay=weight_decay,\n                   nesterov=nesterov)\n\ndef adam(parameters, lr=0.001, betas=(0.9, 0.999), weight_decay=0, amsgrad=False, **_):\n  if isinstance(betas, str):\n    betas = eval(betas)\n  return torch.optim.Adam(parameters, \n                          lr=lr, \n                          betas=betas, \n                          weight_decay=weight_decay,\n                          amsgrad=amsgrad)\n\ndef adamw(paraneters, lr=1e-3, betas=(0.9, 0.999), eps=1e-3, weight_decay=0, amsgrad=False):\n    if isinstance(betas, str):\n        betas = eval(betas)\n    return torch.optim.AdamW(parameters,\n                            lr=lr,\n                            betas=betas,\n                            eps=eps,\n                            weight_decay=weight_decay,\n                            amsgrad=amsgrad)\n\ndef qhadamw(paraneters, lr=1e-3, betas=(0.995, 0.999), nus=(0.7, 1.0), weight_decay=0.0, eps=1e-8):\n    if isinstance(betas, str):\n        betas = eval(betas)\n    if isinstance(nus, str):\n        nus = eval(nus)\n    return optim.QHAdam(parameters, \n                        lr=lr, \n                        betas=betas, \n                        nus=nus, \n                        weight_decay=weight_decay, \n                        eps=eps)\n\ndef radam(parameters, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n    if isinstance(betas, str):\n        betas = eval(betas)\n    return optim.RAdam(parameters,\n                      lr=lr,\n                      betas=betas,\n                      eps=eps,\n                      weight_decay=weight_decay)\n\ndef lamb(parameters, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0, adam=False):\n    if isinstance(betas, str):\n        betas = eval(betas)\n    return optim.Lamb(parameters,\n                     lr=lr,\n                     betas=betas,\n                     eps=eps,\n                     weight_decay=weight_decay,\n                     adam=adam)\n\ndef ranger(parameters, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95, 0.999), eps=1e-5, weight_decay=0):\n    if isinstance(betas, str):\n        betas = eval(betas)\n    return optim.Ranger(parameters,\n                       lr=lr,\n                       k=k,\n                       N_sma_threshhold=N_sma_threshhold,\n                       betas=betas,\n                       eps=eps,\n                       weight_decay=weight_decay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Model Factory","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Encoder Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderHeadModel(nn.Module):\n    def __init__(self, encoder, head):\n        super(EncoderHeadModel, self).__init__()\n        self.encoder = encoder\n        self.head = head\n\n    @property\n    def features_size(self):\n        return self.head.features_size\n\n    def forward(self, image):\n        feature_maps = self.encoder(image)\n        result = self.head(feature_maps)\n        return result \n    \nclass SEResNext50_32x4dEncoder(nn.Module):\n    def __init__(self, pretrained, path):\n        super(SEResNext50_32x4dEncoder, self).__init__()\n        self.base = pretrainedmodels.__dict__['se_resnext50_32x4d'](pretrained=None)\n        if pretrained:\n            self.base.load_state_dict(\n                torch.load(path)\n            )\n        self.output_filters = [2048]\n        \n    def forward(self, x):\n        x = self.base.features(x)\n        return [x]\n    \nclass SEResNext101_32x4dEncoder(nn.Module):\n    def __init__(self, pretrained, path):\n        self.base = pretrainedmodels.__dict__['se_resnext101_32x4d'](pretrained=None)\n        if pretrained:\n            self.base.load_state_dict(\n                torch.load(path)\n            )\n        super(SEResNext101_32x4dEncoder, self).__init__()\n        self.output_filters = [2048]\n    \n    def forward(self, x):\n        x = self.base.features(x)\n        return [x]\n\nclass InceptionV4Encoder(nn.Module):\n    def __init__(self, pretrained, path):\n        self.base = pretrainedmodels.__dict__['inceptionv4'](pretrained=pretrained)\n        super(InceptionV4Encoder, self).__init__()\n        self.output_filters = [1536]\n        \n    def forward(self, x):\n        x = self.base.features(x)\n        return [x]\n    \nclass InceptionResnetV2Encoder(nn.Module):\n    def __init__(self, pretrained, path):\n        self.base = pretrainedmodels.__dict__['inceptionresnetv2'](pretrained=pretrained)\n        super(InceptionResnetV2Encoder, self).__init__()\n        self.output_filters = [1536]\n        \n    def forward(self, x):\n        x = self.base.features(x)\n        return [x]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Head Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = Parameter(torch.ones(1)*p)\n        self.eps = eps\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)       \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n\nclass GeneralizedMeanPoolingHead(nn.Module):\n    def __init__(self, feature_maps, num_classes:int, dropout=0, multisample_dropout=False):\n        super(GeneralizedMeanPoolingHead, self).__init__()\n        self.features_size = feature_maps[-1]\n        self.multisample_dropout = multisample_dropout \n        self.meanpool = GeM()\n        self.dropout = nn.Dropout(dropout)\n        self.logits = nn.Linear(self.features_size, num_classes)\n    \n    def forward(self, feature_maps):\n        features = self.meanpool(feature_maps[-1])\n        features = features.view(features.size(0), features.size(1))\n        if self.multisample_dropout:\n            logits = torch.mean(\n                torch.stack(\n                    [self.logits(self.dropout(features)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            features = self.dropout(features)\n            logits = self.logits(features)\n        return {\n            'features': features,\n            'logits': logits\n        }\n    \nclass GlobalAvgPoolHead(nn.Module):\n    def __init__(self, feature_maps, num_classes: int, dropout=0., multisample_dropout=False):\n        super(GlobalAvgPoolHead, self).__init__()\n        self.features_size = feature_maps[-1]\n        self.multisample_dropout = multisample_dropout \n        self.avgpool = GlobalAvgPool2d()\n        self.dropout = nn.Dropout(dropout)\n        self.logits = nn.Linear(self.features_size, num_classes)\n\n    def forward(self, feature_maps):\n        features = self.avgpool(feature_maps[-1])\n        features = features.view(features.size(0), features.size(1))\n        if self.multisample_dropout:\n            logits = torch.mean(\n                torch.stack(\n                    [self.logits(self.dropout(features)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            features = self.dropout(features)\n            logits = self.logits(features)\n        return {\n            'features': features,\n            'logits': logits\n        }\n    \nclass GlobalWeightedAvgPoolHead(nn.Module):\n    def __init__(self, feature_maps, num_classes: int, dropout=0., multisample_dropout=False):\n        super(GlobalWeightedAvgPoolHead, self).__init__()\n        self.features_size = feature_maps[-1]\n        self.multisample_dropout = multisample_dropout \n        self.gwap = GWAP(self.features_size)\n        self.dropout = nn.Dropout(dropout)\n        self.logits = nn.Linear(self.features_size, num_classes)\n\n    def forward(self, feature_maps):\n        features = feature_maps[-1]\n        features = self.gwap(features)\n        features = features.view(features.size(0), features.size(1))\n        if self.multisample_dropout:\n            logits = torch.mean(\n                torch.stack(\n                    [self.logits(self.dropout(features)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            features = self.dropout(features)\n            logits = self.logits(features)\n\n        return {\n            'features': features,\n            'logits': logits\n        }\n\nclass GlobalMaxPoolHead(nn.Module):\n    def __init__(self, feature_maps, num_classes: int, dropout=0., reduction=8, multisample_dropout=False):\n        super(GlobalMaxPoolHead, self).__init__()\n        self.features_size = feature_maps[-1] // reduction\n        self.multisample_dropout = multisample_dropout \n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(feature_maps[-1], self.features_size, kernel_size=1, bias=False),\n            nn.BatchNorm2d(self.features_size),\n            nn.ReLU(inplace=True))\n        self.maxpool = GlobalMaxPool2d()\n        self.dropout = nn.Dropout(dropout)\n        self.logits = nn.Linear(self.features_size, num_classes)\n\n    def forward(self, feature_maps):\n        features = feature_maps[-1]\n        features = self.bottleneck(features)\n        features = self.maxpool(features)\n        features = features.view(features.size(0), features.size(1))\n        if self.multisample_dropout:\n            logits = torch.mean(\n                torch.stack(\n                    [self.logits(self.dropout(features)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            features = self.dropout(features)\n            logits = self.logits(features)\n\n        return {\n            'features': features,\n            'logits': logits,\n        }\n\nclass GlobalMaxPoolHeadV2(nn.Module):\n    def __init__(self, feature_maps, num_classes: int, dropout=0.,multisample_dropout=False):\n        super(GlobalMaxPoolHeadV2, self).__init__()\n        self.features_size = feature_maps[-1]\n        self.multisample_dropout = multisample_dropout \n        self.maxpool = GlobalMaxPool2d()\n        self.dropout = nn.Dropout(dropout)\n        self.logits = nn.Linear(self.features_size, num_classes)\n\n    def forward(self, feature_maps):\n        features = self.maxpool(feature_maps[-1])\n        features = features.view(features.size(0), features.size(1))\n        if self.multisample_dropout:\n            logits = torch.mean(\n                torch.stack(\n                    [self.logits(self.dropout(features)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            features = self.dropout(features)\n            logits = self.logits(features)\n        \n        return {\n            'features': features,\n            'logits': logits\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scheduler Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def step(optimizer, step_size=80, gamma=0.1, **_):\n  return lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\ndef one_cycle(optimizer,  num_epochs, **_):\n    return lr_scheduler.OneCycleLR(optimizer,\n                                   lr_range=(lr, 1e-6, 1e-5),\n                                   num_steps=num_epochs,\n                                   warmup_fraction=0.05, \n                                   decay_fraction=0.1)\n\ndef multi_step(optimizer, milestones=[500, 5000], gamma=0.1, **_):\n  if isinstance(milestones, str):\n    milestones = eval(milestones)\n  return lr_scheduler.MultiStepLR(optimizer, milestones=milestones,\n                                  gamma=gamma)\n\n\ndef exponential(optimizer, gamma=0.995, **_):\n  return lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n\n\ndef none(optimizer, **_):\n  return lr_scheduler.StepLR(optimizer, step_size=10000000)\n\n\ndef reduce_lr_on_plateau(optimizer, **params):\n  return lr_scheduler.ReduceLROnPlateau(optimizer, \n                                        mode='max',\n                                        factor=0.8,\n                                        patience=2,\n                                        verbose=False, \n                                        threshold=0.0001,\n                                        threshold_mode='rel',\n                                        cooldown=0, \n                                        min_lr=1e-8,\n                                        eps=1e-3)\n\ndef cosine(optimizer, T_max=50, eta_min=0.00001, **_):\n  print('cosine annealing, T_max: {}, eta_min: {}, last_epoch: {}'.format(T_max, eta_min, last_epoch))\n  return lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass RocAucMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.y_true = np.array([0,1])\n        self.y_pred = np.array([0.5,0.5])\n        self.score = 0\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred)\n\n    @property\n    def avg(self):\n        return self.score\n\nclass APScoreMeter(RocAucMeter):\n    def __init__(self):\n        super(APScoreMeter, self).__init__()\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = sklearn.metrics.average_precision_score(self.y_true, self.y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(model_name, num_classes, pretrained=True, dropout=0.0, multisample_dropout=False, freeze_bn=False, **kwargs):\n    keys = model_name.split('_')\n    if len(keys) == 2:\n        encoder_name, head_name = keys\n        model = 'baseline'\n    else:\n        model, encoder_name, head_name = keys\n    ENCODERS = {\n        'inceptionv4':InceptionV4Encoder,\n        'inceptionresnetv2':InceptionResnetV2Encoder,\n        'seresnext50':SEResNext50_32x4dEncoder,\n        'seresnext101':SEResNext101_32x4dEncoder\n    }\n    HEADS = {\n        'gem':GeneralizedMeanPoolingHead,\n        'gap':GlobalAvgPoolHead,\n        'gwap':GlobalWeightedAvgPoolHead,\n        'max':GlobalMaxPoolHead,\n        'max2':GlobalMaxPoolHeadV2\n    }\n    MODELS = {\n        'baseline':EncoderHeadModel\n    }\n    PATHS = {\n        'inceptionv4': '../input/pretrained-model-weights-pytorch/inceptionv4-8e4777a0.pth',\n        'inceptionresnetv2': '../input/pretrained-model-weights-pytorch/inceptionresnetv2-520b38e4.pth',\n        'seresnext50': '../input/pretrained-model-weights-pytorch/se_resnext50_32x4d-a260b3a4.pth',\n        'seresnext101': '../input/pretrained-model-weights-pytorch/se_resnext101_32x4d-3b2fe3d8.pth'\n    }\n    pretrained_path = PATHS[encoder_name]\n    encoder = ENCODERS[encoder_name](pretrained=pretrained, path=pretrained_path)\n    head = HEADS[head_name](feature_maps=encoder.output_filters, \n                            num_classes=num_classes, \n                            dropout=dropout,\n                            multisample_dropout=multisample_dropout)\n    model = MODELS[model](encoder=encoder, head=head)\n    if freeze_bn:\n        for m in model.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.weight.requires_grad = False\n                m.bias.requires_grad = False\n    return model\n\ndef get_loss(loss_name, **loss_params):\n    LOSSES = {\n        'focal':FocalLoss,\n        'labelsmoothing':LabelSmoothing,\n        'crossentropy':SoftCrossEntropyLoss,\n        'smooth':SmoothLoss\n    }\n    criterion = LOSSES[loss_name](**loss_params)\n    return criterion\n\ndef get_optimizer(optimizer_name: str, optimizer_parameters, learning_rate: float, weight_decay=0.001, **kwargs):\n    OPTIMIZERS = {\n        'sgd':sgd,\n        'adam':adam,\n        'adamw':adamw,\n        'qhadamw':qhadamw,\n        'radam':radam,\n        'lamb':lamb,\n        'ranger':ranger\n    }\n    optimizer = OPTIMIZERS[optimizer_name](parameters=optimizer_parameters, \n                                           lr=learning_rate,\n                                           weight_decay=weight_decay,\n                                           **kwargs\n                                          )\n    return optimizer\n\ndef get_scheduler(scheduler_name, optimizer, **scheduler_params):\n    SCHEDULERS = {\n        'step':step,\n        'multi_step':multi_step,\n        'exponential':exponential,\n        'none':none,\n        'reduce_lr_on_plateau':reduce_lr_on_plateau,\n        'cosine':cosine,\n        'one_cycle':one_cycle\n    }\n    scheduler = SCHEDULERS[scheduler_name](optimizer, **scheduler_params)\n    return scheduler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configuration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config():\n    def __init__(self):\n        self.epochs=10\n        self.batch_size=16\n        \n        self.image_size=192\n        self.aug='light'\n        \n        self.verbose=True\n        self.log_step=1\n        self.num_workers=4\n        \n        self.model_name = 'baseline_seresnext50_max'\n        self.loss = 'labelsmoothing'\n        self.optimizer = 'ranger'\n        self.scheduler = 'reduce_lr_on_plateau'\n        \n        self.checkpoint_prefix = f'{self.model_name}_{self.image_size}_{self.aug}'\n        self.checkpoint_prefix += f'_{self.loss}'\n        self.checkpoint_prefix += f'_{self.optimizer}'\n        self.checkpoint_prefix += f'_{self.scheduler}'\n        self.checkpoint_prefix += f'_{self.epochs}'\n        self.checkpoint_prefix += f'_{self.batch_size}'\n        \n        self.log_dir=f'{self.checkpoint_prefix}/logs'\n        self.log_path = os.path.join(self.log_dir, 'log.txt')\n        \n        self.config_path = f'{self.checkpoint_prefix}/config'\n        self.config_fname = os.path.join(self.config_path, 'config.json')\n        \n        self.num_classes = len(get_class_names())\n        self.pretrained = 'imagenet'\n        self.dropout = 0.0\n        self.multisample_dropout = False\n        self.freeze_bn = False\n        \n        self.model_params = dict(\n            model_name=self.model_name,\n            num_classes=self.num_classes,\n            pretrained=self.pretrained,\n            dropout=self.dropout,\n            multisample_dropout=self.multisample_dropout,\n            freeze_bn=self.freeze_bn\n        )\n\n        self.smoothing=0.05\n        self.loss_params = dict(\n            smoothing=self.smoothing\n        )\n\n        self.lr=1e-4\n        self.weight_decay=0.0\n        self.alpha=0.5\n        self.k=6\n        self.N_sma_threshhold=5 \n        self.betas=(.95, 0.999)\n        self.eps=1e-3\n        self.optimizer_params = dict(\n            learning_rate=self.lr,\n            weight_decay=self.weight_decay,\n            alpha=self.alpha,\n            k=self.k,\n            N_sma_threshhold=self.N_sma_threshhold,\n            eps=self.eps,\n        )\n\n        self.mode='max',\n        self.factor=0.8,\n        self.patience=2,\n        self.verbose=True, \n        self.threshold=0.0001,\n        self.threshold_mode='abs',\n        self.cooldown=0, \n        self.min_lr=1e-8,\n        self.eps=1e-3\n        self.scheduler_params = dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_encoder(name):\n    return 'encoder' in name\n\nclass Runner:\n    def __init__(self, config, device, fold=0):\n        self.config = config\n        self.device = device\n        self.fold = fold\n        self.best_loss = 10**6\n        self.model = get_model(**self.config.model_params)\n        self.model.to(self.device)\n        if torch.cuda.device_count() > 1:\n            self.model = nn.DataParallel(self.model)\n        \n        grouped_optimizer_parameters = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        self.optimizer_grouped_parameters = [\n            {\n                'params':[param for name, param in grouped_optimizer_parameters if is_encoder(name) and not any(nd in name for nd in no_decay)],\n                'lr':1e-4,\n                'weight_decay':0.001\n            },\n            {\n                'params':[param for name, param in grouped_optimizer_parameters if is_encoder(name) and any(nd in name for nd in no_decay)],\n                'lr':1e-4\n            },\n            {\n                'params':[param for name, param in grouped_optimizer_parameters if not is_encoder(name)],\n                'lr':1e-4*50\n            }\n        ]\n        self.optimizer = get_optimizer(self.config.optimizer, self.optimizer_grouped_parameters, **self.config.optimizer_params)\n        self.criterion = get_loss(self.config.loss, **self.config.loss_params)\n        self.scheduler = get_scheduler(self.config.scheduler, self.optimizer, **self.config.scheduler_params)\n        \n        if not os.path.exists(self.config.checkpoint_prefix):\n            os.mkdir(self.config.checkpoint_prefix)\n            os.mkdir(self.config.log_dir)\n            os.mkdir(self.config.config_path)\n        \n        with open(self.config.config_fname, 'w') as f:\n            train_config = self.config.__dict__\n            f.write(json.dumps(train_config, indent=4))\n        \n        print(f'Runner ready.! \\nDevice is {self.device}. \\nTotal parameters {count_parameters(self.model)}')\n    \n    def fit_one_epoch(self, train_loader, epoch):\n        self.model.train()\n        total_score = APScoreMeter()\n        roc_score = RocAucMeter()\n        total_loss = AverageMeter()\n        start_time = time.time()\n        for step, data in enumerate(train_loader):\n            images = data[\"images\"].to(self.device, dtype=torch.float32)\n            images = images.permute(0,3,1,2)\n            targets = data['targets'].to(self.device, dtype=torch.float32)\n            batch_size, _, _, _ = images.shape\n            \n            self.optimizer.zero_grad()\n            logits = self.model(images)\n            loss = self.criterion(logits['logits'], targets)\n            loss.backward()\n            self.optimizer.step()\n            loss = loss.detach().item()\n            \n            roc_score.update(targets, logits['logits'])\n            total_score.update(targets,logits['logits'])\n            total_loss.update(loss, batch_size)\n            \n            if self.config.verbose:\n                if step%self.config.log_step==0:\n                    print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tAP Score:{:0.4f}\\tRoC AUC: {:0.4f}\\tTotal Time:{:.2f}'.format(\n                        total_loss.avg,\n                        total_score.avg,\n                        roc_score.avg,\n                        time.time()-start_time,\n                        epoch=epoch,\n                        trained_samples=step*batch_size+len(images),\n                        total_samples=len(train_loader.dataset)\n                    ),end='\\r')\n        return total_score, total_loss, roc_score\n    \n    def validation(self, valid_loader, epoch):\n        self.model.eval()\n        total_score = APScoreMeter()\n        roc_score = RocAucMeter()\n        total_loss = AverageMeter()\n        start_time = time.time()\n        for step, data in enumerate(valid_loader):\n            images = data[\"images\"].to(self.device, dtype=torch.float32)\n            images = images.permute(0,3,1,2)\n            targets = data['targets'].to(self.device, dtype=torch.float32)\n            batch_size, _, _, _ = images.shape\n\n            with torch.no_grad():\n                logits = self.model(images)\n                loss = self.criterion(logits['logits'], targets)\n                loss = loss.detach().item()\n\n                roc_score.update(targets, logits['logits'])\n                total_score.update(targets,logits['logits'])\n                total_loss.update(loss, batch_size)\n\n                if self.config.verbose:\n                    if step%self.config.log_step==0:\n                        print('Validation Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tAP Score:{:0.4f}\\tRoC AUC: {:0.4f}\\tTotal Time:{:.2f}'.format(\n                            total_loss.avg,\n                            total_score.avg,\n                            roc_score.avg,\n                            time.time()-start_time,\n                            epoch=epoch,\n                            trained_samples=step*batch_size+len(images),\n                            total_samples=len(valid_loader.dataset)\n                        ),end='\\r')\n        return total_score, total_loss, roc_score\n    \n    def fit(self, train_loader, valid_loader):\n        for n_epoch in range(self.config.epochs):\n            if self.config.verbose:\n                learning_rate_encoder, learning_rate_head = self.optimizer.param_groups[0]['lr'], self.optimizer.param_groups[-1]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                print(f'Timestamp:{timestamp}\\nLR Encoder:{learning_rate_encoder}\\tLR Head:{learning_rate_head}')\n\n            start_time = time.time()\n            total_score, total_loss, roc_score = self.fit_one_epoch(train_loader, n_epoch)\n            self.log('[RESULT Train] Epoch: {epoch}\\tLoss: {:0.4f}\\tAP Score:{:0.4f}\\tRoC AUC: {:0.4f}\\tTotal Time:{:.2f}'.format(\n                total_loss.avg,\n                total_score.avg,\n                roc_score.avg,\n                time.time()-start_time,\n                epoch=n_epoch\n            ))\n\n            if n_epoch%3==0:\n                start_time = time.time()\n                total_score_val, total_loss_val, roc_score_val = self.validation(valid_loader, n_epoch)\n                self.log('[RESULT Validation] Epoch: {epoch}\\tLoss: {:0.4f}\\tAP Score:{:0.4f}\\tRoC AUC: {:0.4f}\\tTotal Time:{:.2f}'.format(\n                    total_loss_val.avg,\n                    total_score_val.avg,\n                    roc_score_val.avg,\n                    time.time()-start_time,\n                    epoch=n_epoch\n                ))  \n\n            if self.config.scheduler is not None:\n                self.scheduler.step(metrics=roc_score.avg)\n\n            if total_loss.avg < self.best_loss:\n                self.best_loss = total_loss.avg\n                self.model.eval()\n                self.save(f'{self.config.checkpoint_prefix}/model_{str(self.fold).zfill(2)}fold_{str(n_epoch).zfill(2)}epoch.bin')\n                for path in sorted(glob(f'{self.config.checkpoint_prefix}/model_{str(self.fold).zfill(2)}fold_*epoch.bin'))[:-3]:\n                    os.remove(path)\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n        }, path)\n    \n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.config.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.data.sampler import BalanceClassSampler\nfrom torch.utils.data.sampler import SequentialSampler\n\ndef run_training(fold):\n    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n    DIR = '../input/jpeg-melanoma-192x192/train/'\n    config = Config()\n    \n    train_dataset = MelanomaTrainDataset(dataframe = train[train.kfold!=fold],\n                                         dataset_dir=DIR,\n                                         classes=len(get_class_names()),\n                                         fold_idx=fold,\n                                         do_one_hot=True,\n                                         transforms=get_train_transform([config.image_size,config.image_size],config.aug),\n                                         meta_features=True\n                                        )\n    \n    validation_dataset = MelanomaTrainDataset(dataframe = train[train.kfold==fold],\n                                         dataset_dir=DIR,\n                                         classes=len(get_class_names()),\n                                         fold_idx=fold,\n                                         do_one_hot=True,\n                                         transforms=get_train_transform([config.image_size,config.image_size],config.aug),\n                                         meta_features=True\n                                        )\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset,\n                                               sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n                                               #sampler=SequentialSampler(train_dataset),\n                                               batch_size=config.batch_size,\n                                               pin_memory=False,\n                                               drop_last=True,\n                                               num_workers=config.num_workers,\n                                              )\n    \n    val_loader = torch.utils.data.DataLoader(validation_dataset, \n                                             batch_size=config.batch_size,\n                                             num_workers=config.num_workers,\n                                             shuffle=False,\n                                             sampler=SequentialSampler(validation_dataset),\n                                             pin_memory=False,\n                                            )\n\n    runner = Runner(config=config, device=device, fold=fold)\n    runner.fit(train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}