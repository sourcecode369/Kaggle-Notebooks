{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ac1c5d8-a575-4562-b79d-71328adf01a5",
    "_uuid": "7e00a1af-ce5e-41c8-bc67-b7a18bd5db75"
   },
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook trains from the XLM-Roberta large model which was finetuned with masked language modelling on the jigsaw test dataset [Link](https://www.kaggle.com/riblidezso/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm).\n",
    "\n",
    "This notebook also implements a few improvements compared to a previous starter notebook that I shared\n",
    "\n",
    "* 1, It trains on translated data\n",
    "* 2, It uses different learning rate for the head layer and the transformer\n",
    "* 3, It restores the model weights after training to the checkpoint which had the highest validation score\n",
    "\n",
    "Suggestions/improvements are appreciated!\n",
    "\n",
    "---\n",
    "\n",
    "### References:\n",
    "\n",
    "- The shared XLM-Roberta large model, finetuned on the Jigsaw multilingual test data with masked language modelling Notebook [link]() / Dataset [link](https://www.kaggle.com/riblidezso/jigsaw-mlm-finetuned-xlm-r-large)\n",
    "- My previous starter notebook [link](https://www.kaggle.com/riblidezso/tpu-custom-tensoflow2-training-loop)\n",
    "- This notebook uses the translated versions of the training dataset too, big thanks to Michael Kazachok! [link](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api)\n",
    "- This notebook uses different learning rate for the transformer and the head, I got the ideas from the writeup of the winning team of the Google QUEST Q&A Labeling competition  [link](https://www.kaggle.com/c/google-quest-challenge/discussion/129840), I have seen it described to be useful elsewhere too.\n",
    "- This notebook heavily relies on the great [notebook]((https://www.kaggle.com/xhlulu//jigsaw-tpu-xlm-roberta) by, Xhulu: [@xhulu](https://www.kaggle.com/xhulu/) \n",
    "- The tensorflow distrubuted training tutorial: [Link](https://www.tensorflow.org/tutorials/distribute/custom_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c4ad26dd-8d2a-43e9-a3ea-0a4342a16e52",
    "_uuid": "07d7bc95-0377-4db3-aa05-2bd6cf989e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192 \n",
    "DROPOUT = 0.5 # use aggressive dropout\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = 2000\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFRobertaModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dda6b430-ea9c-4772-8bd1-0938cfd6e63b",
    "_uuid": "f0c9b5d5-e529-441d-8f37-7f64a1daccb9"
   },
   "source": [
    "## Connect to TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "86a303dd-5d1d-4d3f-bf28-4a02b6816e9b",
    "_uuid": "5a5fd45e-8f97-44d8-95f6-29cc8563dc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e748f90-73ca-4a3e-a3a5-aef269ce89c4",
    "_uuid": "2111bb0e-44ed-4db7-aba8-d2ff3507c454"
   },
   "source": [
    " ## Load text data into memory\n",
    " \n",
    " - Traning data is englih + all translations. The reason to use english too is that people use english in their foreign language comments all the time.\n",
    " - Not using the full dataset, downsampling negatives to 50-50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1cba2ff3-145e-4187-a6a4-60d28adf5141",
    "_uuid": "80b620f4-85cd-4436-b868-acdcea7bf2b6"
   },
   "outputs": [],
   "source": [
    "def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "                      columns=['comment_text', 'toxic']):\n",
    "    train_6langs=[]\n",
    "    for i in range(len(langs)):\n",
    "        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n",
    "        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n",
    "\n",
    "    return train_6langs\n",
    "\n",
    "def downsample(df):\n",
    "    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "    ds_df= pd.concat([\n",
    "        df.query('toxic==1'),\n",
    "        df.query('toxic==0').sample(sum(df.toxic))\n",
    "    ])\n",
    "    \n",
    "    return ds_df\n",
    "    \n",
    "\n",
    "train_df = pd.concat(load_jigsaw_trans()) \n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d070fa12-8165-41c2-9828-9690c8efa826",
    "_uuid": "5e6d306f-f2ca-4452-9ace-661e6df8bae8"
   },
   "source": [
    "## Tokenize  it with the models own tokenizer\n",
    "\n",
    "- Note it takes some time ( approx 5 minutes)\n",
    "- Note, we need to reshape the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "900f2d01-b87c-42e7-8cc2-8dcba95ffa54",
    "_uuid": "7d5605a3-df77-4574-9899-89529d0a6e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 24s, sys: 1.78 s, total: 5min 26s\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0725dd72-142e-4139-8e12-103302d53bc9",
    "_uuid": "13500d69-d19f-4ca0-8958-d4d8af5610e8"
   },
   "source": [
    "## Create distributed tensorflow datasets\n",
    "\n",
    "- Note, validation dataset does not contain labels, we keep track of it ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "757b2c93-9c07-441d-ba2e-47b8607b3795",
    "_uuid": "8b748024-314d-4256-b67b-0bc71ecac19d"
   },
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bcfe8f7d-cb29-414b-9858-b36dc57a9af1",
    "_uuid": "7ae0b584-8ec8-4e10-928b-6c05ea2ea515"
   },
   "source": [
    "## Build model from pretrained transformer\n",
    "\n",
    "\n",
    "Let's use a different learning rate for the head and the transformer like the winning team of the Google QUEST Q&A Labeling competition  [link](https://www.kaggle.com/c/google-quest-challenge/discussion/129840). \n",
    "\n",
    "The reasoning is the following, the transformer is trained for super long time and has a very good multilingual representaton, which we only want to change a little, while the head needs to be trained from scratch.\n",
    "\n",
    "We define 2 separate optimizers for the transofmer and the head layer. This is a simple way to use different learning rate for the transformer and the head. The caffe style \"lr_multiplier\" option would be more elegant but that is not available in keras.\n",
    "\n",
    "We add the name 'custom' to the head layer, so that we can find it later and use a different learning rate with this layer\n",
    "\n",
    "- Note: Downloading the model takes some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "3ba0b8c0-3e4c-4afe-8766-844c6b6a8c0f",
    "_uuid": "9f6161c2-a256-4285-9a0e-f23700f4b195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "custom_head (Dense)          (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 31.6 s, sys: 26.6 s, total: 58.2 s\n",
      "Wall time: 59.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e02e579-4010-4a52-b4ba-f7157b5c9f42",
    "_uuid": "9ce1d72f-d16c-4cde-9edc-d63926623155"
   },
   "source": [
    "### Define stuff for the custom training loop\n",
    "\n",
    "We will need:\n",
    "- 1, losses, and  optionally a training AUC metric here: these need to be defined in the scope of the distributed strategy. \n",
    "- 2, A full training loop\n",
    "- 3, A distributed train step called in the training loop, which uses a single replica train step\n",
    "- 4, A prediction loop with dstibute \n",
    "\n",
    "\n",
    "At the end of training we restore the parameters which had the best validation score.\n",
    "\n",
    "\n",
    "For the different learning rate we need to apply gradients in two steps, check the train_step function for details.\n",
    "\n",
    "\n",
    "\n",
    "- Note, we are using exact AUC, for the valdationdata, and approximate AUC for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "20b20e6a-16c4-475f-92ef-2a7809d21621",
    "_uuid": "50c766ac-68bd-4884-ba54-657f5d385786"
   },
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "\n",
    "    return compute_loss, train_accuracy_metric\n",
    "\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=2000, validate_every=200):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "    ### Training lopp ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor) \n",
    "        step+=1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Print train metrics ###  \n",
    "            train_metric = train_accuracy_metric.result().numpy()\n",
    "            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n",
    "            \n",
    "            ### Test loop with exact AUC ###\n",
    "            if val_dist_dataset:\n",
    "                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n",
    "                print(\"Step %d,   val AUC: %.5f\" %  (step,val_metric))   \n",
    "                \n",
    "                # save weights if it is the best yet\n",
    "                history.append(val_metric)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = model.get_weights()\n",
    "\n",
    "            ### Reset (train) metrics ###\n",
    "            train_accuracy_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "    \n",
    "    ### Restore best weighths ###\n",
    "    model.set_weights(best_weights)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.experimental_run_v2(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "    \n",
    "    ### get transformer and head separate vars\n",
    "    # get rid of pooler head with None gradients\n",
    "    transformer_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                       if (('pooler' not in v.name)  and \n",
    "                                           ('custom' not in v.name))]\n",
    "    head_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                if 'custom'  in v.name]\n",
    "\n",
    "    # calculate the 2 gradients ( note persistent, and del)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n",
    "    gradients_head = tape.gradient(loss, head_trainable_variables)\n",
    "    del tape\n",
    "        \n",
    "    ### make the 2 gradients steps\n",
    "    optimizer_transformer.apply_gradients(zip(gradients_transformer, \n",
    "                                              transformer_trainable_variables))\n",
    "    optimizer_head.apply_gradients(zip(gradients_head, \n",
    "                                       head_trainable_variables))\n",
    "\n",
    "    train_accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    ### stack replicas and batches\n",
    "    predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs  # note datasets used in prediction do not have labels\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "compute_loss, train_accuracy_metric = define_losses_and_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f7958337-2407-4734-98a0-51c89f4f9dc6",
    "_uuid": "e2997f2d-ae14-4bd3-b109-82e6700abb33"
   },
   "source": [
    "## Finally train it on english comments\n",
    "\n",
    "\n",
    "- Note it takes some time\n",
    "- Don't mind the warning: \"Converting sparse IndexedSlices to a dense Tensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d3d0e97f-d7d4-4d3f-8c6e-925669565248",
    "_uuid": "b30a95a7-ae7e-40e8-846b-9d31bfe997ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.87843\n",
      "Step 200,   val AUC: 0.94492\n",
      "Step 400, train AUC: 0.96208\n",
      "Step 400,   val AUC: 0.94369\n",
      "Step 600, train AUC: 0.96788\n",
      "Step 600,   val AUC: 0.94477\n",
      "Step 800, train AUC: 0.97115\n",
      "Step 800,   val AUC: 0.94488\n",
      "Step 1000, train AUC: 0.97312\n",
      "Step 1000,   val AUC: 0.94648\n",
      "Step 1200, train AUC: 0.97232\n",
      "Step 1200,   val AUC: 0.94415\n",
      "Step 1400, train AUC: 0.97294\n",
      "Step 1400,   val AUC: 0.94720\n",
      "Step 1600, train AUC: 0.97507\n",
      "Step 1600,   val AUC: 0.94563\n",
      "Step 1800, train AUC: 0.97659\n",
      "Step 1800,   val AUC: 0.94600\n",
      "Step 2000, train AUC: 0.97696\n",
      "Step 2000,   val AUC: 0.94604\n",
      "CPU times: user 3min 29s, sys: 52.2 s, total: 4min 21s\n",
      "Wall time: 20min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a280adb2-4463-4ad4-8f4a-22bc43292d82",
    "_uuid": "803c2d73-e750-4189-aa53-db248fc25ea7"
   },
   "source": [
    "## Finetune it on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "1bfbb20f-409d-487f-bdf0-fca60927a868",
    "_uuid": "d5b8523a-69d6-41f0-971b-8a2bfcb9e908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.92982\n",
      "Step 10,   val AUC: 0.94024\n",
      "Step 20, train AUC: 0.92825\n",
      "Step 20,   val AUC: 0.94277\n",
      "Step 30, train AUC: 0.93556\n",
      "Step 30,   val AUC: 0.94405\n",
      "Step 40, train AUC: 0.94400\n",
      "Step 40,   val AUC: 0.94574\n",
      "Step 50, train AUC: 0.94940\n",
      "Step 50,   val AUC: 0.94669\n",
      "Step 60, train AUC: 0.94342\n",
      "Step 60,   val AUC: 0.94765\n",
      "Step 70, train AUC: 0.95696\n",
      "Step 70,   val AUC: 0.94746\n",
      "Step 80, train AUC: 0.97071\n",
      "Step 80,   val AUC: 0.94820\n",
      "Step 90, train AUC: 0.94957\n",
      "Step 90,   val AUC: 0.94722\n",
      "Step 100, train AUC: 0.94638\n",
      "Step 100,   val AUC: 0.94718\n",
      "Step 110, train AUC: 0.96697\n",
      "Step 110,   val AUC: 0.94787\n",
      "Step 120, train AUC: 0.96910\n",
      "Step 120,   val AUC: 0.94755\n",
      "Step 130, train AUC: 0.96364\n",
      "Step 130,   val AUC: 0.94863\n",
      "Step 140, train AUC: 0.97313\n",
      "Step 140,   val AUC: 0.95042\n",
      "Step 150, train AUC: 0.96709\n",
      "Step 150,   val AUC: 0.95093\n",
      "Step 160, train AUC: 0.96929\n",
      "Step 160,   val AUC: 0.95133\n",
      "Step 170, train AUC: 0.96161\n",
      "Step 170,   val AUC: 0.95132\n",
      "Step 180, train AUC: 0.97587\n",
      "Step 180,   val AUC: 0.95082\n",
      "Step 190, train AUC: 0.97410\n",
      "Step 190,   val AUC: 0.95072\n",
      "Step 200, train AUC: 0.97057\n",
      "Step 200,   val AUC: 0.95087\n",
      "CPU times: user 1min 6s, sys: 47.1 s, total: 1min 53s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# decrease LR for second stage in the head\n",
    "optimizer_head.learning_rate.assign(1e-4)\n",
    "\n",
    "# split validation data into train test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size = 0.1)\n",
    "\n",
    "# make a datasets\n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, training=True)\n",
    "val_dist_dataset = create_dist_dataset(X_val, y_val)\n",
    "\n",
    "# train again\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      total_steps = TOTAL_STEPS_STAGE2, \n",
    "      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e8be7da4-a25f-482c-88fa-c9b87a8a5f1e",
    "_uuid": "62874aa9-54a8-495b-a2a9-ac726c6dbecf"
   },
   "source": [
    "## Make predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "96a7be93-0dce-4d00-92c4-d4c4cb2238f8",
    "_uuid": "5cd548e4-5756-4d92-b72c-f71cb72b395d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 5.41 s, total: 29.9 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
