{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  3569  100  3569    0     0  46350      0 --:--:-- --:--:-- --:--:-- 46350\r\n",
      "Updating TPU and VM. This may take around 2 minutes.\r\n",
      "Updating TPU runtime to nightly ...\r\n",
      "Found existing installation: torch 1.4.0\r\n",
      "Uninstalling torch-1.4.0:\r\n",
      "  Successfully uninstalled torch-1.4.0\r\n",
      "Found existing installation: torchvision 0.5.0\r\n",
      "Uninstalling torchvision-0.5.0:\r\n",
      "  Successfully uninstalled torchvision-0.5.0\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/83.3 MiB.                                     \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/113.4 MiB.                                    \r\n",
      "Done updating TPU runtime: <Response [200]>\r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/2.5 MiB.                                      \r\n",
      "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch==nightly) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==nightly) (1.18.1)\r\n",
      "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: catalyst 20.2.4 requires torchvision>=0.2.1, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: torch\r\n",
      "Successfully installed torch-1.5.0a0+618c621\r\n",
      "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.6+c59c4d1\r\n",
      "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly) (1.5.0a0+618c621)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly) (1.18.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly) (1.14.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision==nightly) (5.4.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch->torchvision==nightly) (0.18.2)\r\n",
      "Installing collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.6.0a0+793b647\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following additional packages will be installed:\r\n",
      "  libopenblas-base\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libomp5 libopenblas-base libopenblas-dev\r\n",
      "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\r\n",
      "Need to get 7831 kB of archives.\r\n",
      "After this operation, 92.2 MB of additional disk space will be used.\r\n",
      "Get:1 http://deb.debian.org/debian stretch/main amd64 libopenblas-base amd64 0.2.19-3 [3793 kB]\r\n",
      "Get:2 http://deb.debian.org/debian stretch/main amd64 libopenblas-dev amd64 0.2.19-3 [3809 kB]\r\n",
      "Get:3 http://deb.debian.org/debian stretch/main amd64 libomp5 amd64 3.9.1-1 [228 kB]\r\n",
      "Fetched 7831 kB in 0s (22.9 MB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libopenblas-base.\r\n",
      "(Reading database ... 74146 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libopenblas-base_0.2.19-3_amd64.deb ...\r\n",
      "Unpacking libopenblas-base (0.2.19-3) ...\r\n",
      "Selecting previously unselected package libopenblas-dev.\r\n",
      "Preparing to unpack .../libopenblas-dev_0.2.19-3_amd64.deb ...\r\n",
      "Unpacking libopenblas-dev (0.2.19-3) ...\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "Preparing to unpack .../libomp5_3.9.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (3.9.1-1) ...\r\n",
      "Setting up libomp5:amd64 (3.9.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\r\n",
      "Setting up libopenblas-base (0.2.19-3) ...\r\n",
      "update-alternatives: using /usr/lib/openblas-base/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode\r\n",
      "update-alternatives: using /usr/lib/openblas-base/liblapack.so.3 to provide /usr/lib/liblapack.so.3 (liblapack.so.3) in auto mode\r\n",
      "Setting up libopenblas-dev (0.2.19-3) ...\r\n",
      "update-alternatives: using /usr/lib/openblas-base/libblas.so to provide /usr/lib/libblas.so (libblas.so) in auto mode\r\n",
      "update-alternatives: using /usr/lib/openblas-base/liblapack.so to provide /usr/lib/liblapack.so (liblapack.so) in auto mode\r\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\r\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "16a3df23-8416-46b5-8661-c52345005b6d",
    "_uuid": "993abe0b-2561-4abc-a6a2-b83d8dd4c846"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, namedtuple\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import joblib\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "import sys\n",
    "from sklearn import metrics, model_selection\n",
    "\n",
    "import warnings\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self, bert_path):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert_path = bert_path\n",
    "        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768 * 2, 1)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            ids,\n",
    "            mask,\n",
    "            token_type_ids\n",
    "    ):\n",
    "        o1, o2 = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "        apool = torch.mean(o1, 1)\n",
    "        mpool, _ = torch.max(o1, 1)\n",
    "        cat = torch.cat((apool, mpool), 1)\n",
    "\n",
    "        bo = self.bert_drop(cat)\n",
    "        p2 = self.out(bo)\n",
    "        return p2\n",
    "\n",
    "\n",
    "class BERTDatasetTraining:\n",
    "    def __init__(self, comment_text, targets, tokenizer, max_length):\n",
    "        self.comment_text = comment_text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        comment_text = str(self.comment_text[item])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        padding_length = self.max_length - len(ids)\n",
    "        \n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = BERTBaseUncased(bert_path=\"../input/bert-base-multilingual-uncased/\")\n",
    "df_train1 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n",
    "df_train2 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\n",
    "df_train_full = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\n",
    "df_train = df_train_full.sample(frac=1).reset_index(drop=True).head(200000)\n",
    "\n",
    "df_valid = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv', \n",
    "                       usecols=[\"comment_text\", \"toxic\"])\n",
    "\n",
    "df_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run():\n",
    "    def loss_fn(outputs, targets):\n",
    "        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "        model.train()\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            if bi % 10 == 0:\n",
    "                xm.master_print(f'bi={bi}, loss={loss}')\n",
    "\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "    def eval_loop_fn(data_loader, model, device):\n",
    "        model.eval()\n",
    "        fin_targets = []\n",
    "        fin_outputs = []\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            ids = d[\"ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            targets_np = targets.cpu().detach().numpy().tolist()\n",
    "            outputs_np = outputs.cpu().detach().numpy().tolist()\n",
    "            fin_targets.extend(targets_np)\n",
    "            fin_outputs.extend(outputs_np)    \n",
    "\n",
    "        return fin_outputs, fin_targets\n",
    "\n",
    "    \n",
    "    MAX_LEN = 192\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    EPOCHS = 2\n",
    "\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-multilingual-uncased/\", do_lower_case=True)\n",
    "\n",
    "    train_targets = df_train.toxic.values\n",
    "    valid_targets = df_valid.toxic.values\n",
    "\n",
    "    train_dataset = BERTDatasetTraining(\n",
    "        comment_text=df_train.comment_text.values,\n",
    "        targets=train_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          train_dataset,\n",
    "          num_replicas=xm.xrt_world_size(),\n",
    "          rank=xm.get_ordinal(),\n",
    "          shuffle=True)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDatasetTraining(\n",
    "        comment_text=df_valid.comment_text.values,\n",
    "        targets=valid_targets,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "          valid_dataset,\n",
    "          num_replicas=xm.xrt_world_size(),\n",
    "          rank=xm.get_ordinal(),\n",
    "          shuffle=False)\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=16,\n",
    "        sampler=valid_sampler,\n",
    "        drop_last=False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    model = mx.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n",
    "    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
    "    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        para_loader = pl.ParallelLoader(train_data_loader, [device])\n",
    "        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n",
    "\n",
    "        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n",
    "        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n",
    "        xm.save(model.state_dict(), \"model.bin\")\n",
    "        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n",
    "        xm.master_print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_steps = 812, world_size=8\n",
      "bi=0, loss=0.5784894227981567\n",
      "bi=10, loss=0.4176854193210602\n",
      "bi=20, loss=0.38402101397514343\n",
      "bi=30, loss=0.4190573990345001\n",
      "bi=40, loss=0.3812633156776428\n",
      "bi=50, loss=0.2844417691230774\n",
      "bi=60, loss=0.23404008150100708\n",
      "bi=70, loss=0.33819833397865295\n",
      "bi=80, loss=0.23843970894813538\n",
      "bi=90, loss=0.2481943964958191\n",
      "bi=100, loss=0.2160593569278717\n",
      "bi=110, loss=0.23247478902339935\n",
      "bi=120, loss=0.2055036425590515\n",
      "bi=130, loss=0.2173389494419098\n",
      "bi=140, loss=0.31595656275749207\n",
      "bi=150, loss=0.2666240334510803\n",
      "bi=160, loss=0.24332033097743988\n",
      "bi=170, loss=0.27124038338661194\n",
      "bi=180, loss=0.16089509427547455\n",
      "bi=190, loss=0.2558012902736664\n",
      "bi=200, loss=0.26145485043525696\n",
      "bi=210, loss=0.18103882670402527\n",
      "bi=220, loss=0.1921006441116333\n",
      "bi=230, loss=0.32901889085769653\n",
      "bi=240, loss=0.13699162006378174\n",
      "bi=250, loss=0.19692277908325195\n",
      "bi=260, loss=0.234874427318573\n",
      "bi=270, loss=0.2780483067035675\n",
      "bi=280, loss=0.1741013526916504\n",
      "bi=290, loss=0.21872104704380035\n",
      "bi=300, loss=0.1856013685464859\n",
      "bi=310, loss=0.2563312351703644\n",
      "bi=320, loss=0.1725718230009079\n",
      "bi=330, loss=0.21737347543239594\n",
      "bi=340, loss=0.18803219497203827\n",
      "bi=350, loss=0.23823565244674683\n",
      "bi=360, loss=0.21824167668819427\n",
      "bi=370, loss=0.25201523303985596\n",
      "bi=380, loss=0.3371922969818115\n",
      "bi=390, loss=0.19841696321964264\n",
      "bi=400, loss=0.2080656886100769\n",
      "AUC = 0.9724169262720664\n",
      "bi=0, loss=0.19439908862113953\n",
      "bi=10, loss=0.28269872069358826\n",
      "bi=20, loss=0.24980229139328003\n",
      "bi=30, loss=0.3083013892173767\n",
      "bi=40, loss=0.2840643525123596\n",
      "bi=50, loss=0.21023596823215485\n",
      "bi=60, loss=0.21433904767036438\n",
      "bi=70, loss=0.275532990694046\n",
      "bi=80, loss=0.20042496919631958\n",
      "bi=90, loss=0.20823226869106293\n",
      "bi=100, loss=0.20056062936782837\n",
      "bi=110, loss=0.21218045055866241\n",
      "bi=120, loss=0.18630708754062653\n",
      "bi=130, loss=0.2146265208721161\n",
      "bi=140, loss=0.2648042142391205\n",
      "bi=150, loss=0.25419744849205017\n",
      "bi=160, loss=0.23074109852313995\n",
      "bi=170, loss=0.24986857175827026\n",
      "bi=180, loss=0.16116531193256378\n",
      "bi=190, loss=0.227876678109169\n",
      "bi=200, loss=0.24354597926139832\n",
      "bi=210, loss=0.1732374131679535\n",
      "bi=220, loss=0.1900346577167511\n",
      "bi=230, loss=0.31212174892425537\n",
      "bi=240, loss=0.12716947495937347\n",
      "bi=250, loss=0.18117749691009521\n",
      "bi=260, loss=0.22139880061149597\n",
      "bi=270, loss=0.2644723653793335\n",
      "bi=280, loss=0.16128449141979218\n",
      "bi=290, loss=0.19033662974834442\n",
      "bi=300, loss=0.17478525638580322\n",
      "bi=310, loss=0.2510298192501068\n",
      "bi=320, loss=0.1700611412525177\n",
      "bi=330, loss=0.21052896976470947\n",
      "bi=340, loss=0.14821308851242065\n",
      "bi=350, loss=0.23010945320129395\n",
      "bi=360, loss=0.19994759559631348\n",
      "bi=370, loss=0.24652990698814392\n",
      "bi=380, loss=0.2987526059150696\n",
      "bi=390, loss=0.19913643598556519\n",
      "bi=400, loss=0.20830576121807098\n",
      "AUC = 0.9857622663551402\n"
     ]
    }
   ],
   "source": [
    "# Start training processes\n",
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    a = _run()\n",
    "\n",
    "FLAGS={}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
